title,authors,abstract,pub_date,sections_list
"Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics",Yuhan Zhang; Edward Gibson; Forrest Davis,"Language models (LMs) have been argued to overlap substantially with human beings in grammaticality judgment tasks. But when humans systematically make errors in language processing, should we expect LMs to behave like cognitive models of language and mimic human behavior? We answer this question by investigating LMs' more subtle judgments associated with ""language illusions"" -sentences that are vague in meaning, implausible, or ungrammatical but receive unexpectedly high acceptability judgments by humans. We looked at three illusions: the comparative illusion (e.g. ""More people have been to Russia than I have""), the depth-charge illusion (e.g. ""No head injury is too trivial to be ignored""), and the negative polarity item (NPI) illusion (e.g. ""The hunter who no villager believed to be trustworthy will ever shoot a bear""). We found that probabilities represented by LMs were more likely to align with human judgments of being ""tricked"" by the NPI illusion which examines a structural dependency, compared to the comparative and the depth-charge illusions which require sophisticated semantic understanding. No single LM or metric yielded results that are entirely consistent with human behavior. Ultimately, we show that LMs are limited both in their construal as cognitive models of human language processing and in their capacity to recognize nuanced but critical information in complicated language materials.",,"['Introduction', 'Related work', ""LMs' linguistic abilities"", 'Language illusions', 'Methods', 'Models and Measures', 'Evaluation procedure', 'Comparative illusion', 'Acceptability differentiation', 'Illusion effect', 'Sensitivity to manipulations', 'Depth-charge illusion', 'Acceptability differentiation', 'Illusion effect', 'Sensitivity to manipulations', 'NPI illusion', 'Acceptability differentiation', 'Illusion effect', 'Sensitivity to variations', 'Discussion', 'Illusion effect', 'Human-like behaviors & Potential processing mechanisms', ""Language models' performance in general"", 'Perplexity & Surprisal', 'Limitations', 'Conclusion', 'Acknowledgements', 'Illusion type', 'Relative No', 'Relative Never']"
A Minimal Approach for Natural Language Action Space in Text-based Games,Kelvin Dongwon;  Ryu; Meng Fang; Gholamreza Haffari; Shirui Pan; Ehsan Shareghi,"Text-based games (TGs) are language-based interactive environments for reinforcement learning. While language models (LMs) and knowledge graphs (KGs) are commonly used for handling large action space in TGs, it is unclear whether these techniques are necessary or overused. In this paper, we revisit the challenge of exploring the action space in TGs and propose ϵ-admissible exploration, a minimal approach of utilizing admissible actions, for training phase. Additionally, we present a textbased actor-critic (TAC) agent that produces textual commands for game, solely from game observations, without requiring any KG or LM. Our method, on average across 10 games from Jericho, outperforms strong baselines and stateof-the-art agents that use LM and KG. Our approach highlights that a much lighter model design, with a fresh perspective on utilizing the information within the environments, suffices for an effective exploration of exponentially large action spaces. 1  ",,"['Introduction', 'Basic Definitions', 'Related Work on TG Agents in RL', 'Experiments', 'Main Results', 'Ablation', 'Qualitative Analysis', 'Discussion', 'Conclusion', 'Appendices', 'A Hyperparameters', 'B Parameter Size for ZORK1', 'C Training Time', 'D Details of Actor and Critic Components', 'F Qualitative Analysis', 'G Full Experimental Results', 'H Stronger Supervised Signals for ZORK1', 'I Adaptive Score-based ϵ', 'J Limitations', 'K Ethical Considerations']"
ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind,Xiaomeng Ma; Lingyu Gao; Qihui Xu,"Theory of Mind (ToM), the capacity to comprehend the mental states of distinct individuals, is essential for numerous practical applications. With the development of large language models (LLMs), there is a heated debate about whether they are able to perform ToM tasks. Previous studies have used different tasks and prompts to test the ToM on LLMs and the results are inconsistent: some studies asserted that these models are capable of exhibiting ToM, while others suggested the opposite. In this study, we present TOMCHALLENGES, a dataset for comprehensively evaluating the Theory of Mind based on the Sally-Anne and Smarties tests with a diverse set of tasks. In addition, we also propose an auto-grader to streamline the answer evaluation process. We tested three models: davinci, turbo, and gpt-4. Our evaluation results and error analyses show that LLMs have inconsistent behaviors across prompts and tasks. Performing the ToM tasks robustly remains a challenge for the LLMs. In addition, our paper wants to raise awareness in evaluating the ToM in LLMs and we want to invite more discussion on how to design the prompts and tasks for ToM tasks that can better assess the LLMs' ability. 1",,"['Introduction', 'Validity issues of current neural ToM tests', 'Related Work', 'TOMCHALLENGES and Tasks', 'Dataset Construction', 'Smarties Test', 'Task Formulation', 'Experimental Setup', 'Answer Evaluation and Auto-grader', 'Results and Analyses', 'Accuracy by Question and Task', 'Accuracy by Narratives', 'Error Analysis', 'Conclusions', 'Reasoning:']"
The Zipfian Challenge: Learning the statistical fingerprint of natural languages,Christian Bentz,"Human languages are often claimed to fundamentally differ from other communication systems. But what is it exactly that unites them as a separate category? This article proposes to approach this problem -here termed the Zipfian Challenge -as a standard classification task. A corpus with textual material from diverse writing systems and languages, as well as other symbolic and non-symbolic systems, is provided. These are subsequently used to train and test binary classification algorithms, assigning labels ""writing"" and ""non-writing"" to character strings of the test sets. The performance is generally high, reaching 98% accuracy for the best algorithms. Human languages emerge to have a statistical fingerprint: large unit inventories, high entropy, and few repetitions of adjacent units. This fingerprint can be used to tease them apart from other symbolic and non-symbolic systems.",,"['Introduction', 'Data', 'Writing', 'TeDDi sample', 'Non-writing', 'Methods', 'Preprocessing', 'Sampling', 'Features', 'Type-token ratio (TTR)', 'Unigram character entropy (H)', 'Entropy rate (h)', 'Repetition rate (R)', 'Training and test sets', 'Logistic regression', 'Support Vector Machines', 'Multilayer Perceptrons (MLP)', 'Results', 'Discussion', 'Why do algorithms perform differently?', 'Why do longer strings yield better results', 'Which is the best feature?', 'How are the results influenced by subcorpora?', 'Conclusions', 'Acknowledgements', 'Appendices']"
On the Effects of Structural Modeling for Neural Semantic Parsing,Xiang Zhang; Shizhu He; Kang Liu; Jun Zhao; Mohammad Kaiser; Clemens Bavarian; Philippe Winter; Felipe Petroski Tillet; Dave Such; Matthias Cum- Mings; Fotios Plappert; Eliza- Beth Chantzis; Ariel Barnes; William Hebgen Herbert-Voss; Alex Guss; Alex Nichol; Nikolas Paino; Jie Tezak; Igor Tang; Suchir Babuschkin; Shantanu Balaji; William Jain; Christopher Saunders; Andrew N Hesse; Jan Carr; Josh Leike; Vedant Achiam; Evan Misra; Alec Morikawa; Matthew Radford; Miles Knight; Mira Brundage; Katie Murati; Peter Mayer; Bob Welinder; Dario Mcgrew; Sam Amodei; Ilya Mccandlish; Wojciech Sutskever;  2021 Zaremba; David Chiang; Jacob Andreas; Daniel Bauer; Karl Moritz; Bevan Jones; De Gruyter; Mouton Kevin Clark; Minh-Thang Luong; Quoc V Le; Christopher D 2020 Manning;  Electra,"Semantic parsing aims to map natural language sentences to predefined formal languages, such as logic forms and programming languages, as the semantic annotation. From the theoretic views of linguistic and programming language, structures play an important role in both languages, which had motivated semantic parsers since the task was proposed in the beginning. But in the neural era, semantic parsers treating both natural and formal language as sequences, such as Seq2Seq and LLMs, have got more attentions. On the other side, lots of neural progress have been made for grammar induction, which only focuses on natural languages. Although closely related in the sense of structural modeling, these techniques hadn't been jointly analyzed on the semantic parsing testbeds. To gain the better understanding on structures for semantic parsing, we design a taxonomy of structural modeling methods, and evaluate some representative techniques on semantic parsing, including both compositional and i.i.d. generalizations. In addition to the previous opinion that structures will help in general, we find that (1) structures must be designed for the specific dataset and generalization level, and ( 2) what really matters is not the structure choice of either source or target side, but the choice combination of both sides. Based on the finding, we further propose a metric that can evaluate the structure choice, which we believe can boost the automation of grammar designs for specific datasets and domains.",,"['Introduction', 'Datasets', 'Problem Formalization', 'Selected Structural Models', 'S Model', 'Absent', 'Evaluation Method', 'Results Analysis', 'Lateral Structural Modeling', 'Combinations of Source and Target', 'Latent Source Structures', 'Differences between Accuracies', 'Discussions', 'Metric for Structural Evaluation', 'Related Works', 'Conclusion', 'Limitations', 'Ethics Statement', 'A Structure Modeling', 'A.1 Encoders', 'A.2 Decoders', 'D Accuracies for Model Combinations', 'E EBNF Grammar for SQL', 'Acknowledgements', '', 'B Experiment Hyperparameters', 'C Few-shot Parsing with LLMs', 'F EBNF Grammar for COGS', 'G EBNF Grammar for Lispress']"
Humans and language models diverge when predicting repeating text,Aditya R Vaidya; Javier Turek; Alexander G Huth; U T Austin,"Language models that are trained on the nextword prediction task have been shown to accurately model human behavior in word prediction and reading speed. In contrast with these findings, we present a scenario in which the performance of humans and LMs diverges. We collected a dataset of human next-word predictions for five stimuli that are formed by repeating spans of text. Human and GPT-2 LM predictions are strongly aligned in the first presentation of a text span, but their performance quickly diverges when memory (or in-context learning) begins to play a role. We traced the cause of this divergence to specific attention heads in a middle layer. Adding a power-law recency bias to these attention heads yielded a model that performs much more similarly to humans. We hope that this scenario will spur future work in bringing LMs closer to human behavior. 1  ",,"['Introduction', 'Related works', 'Human behavioral study', 'Setup for humans', 'Setup for language models', 'Behavioral study results', 'Patterns in model attention', 'Attention optimization', 'Optimization results', 'Conclusions', 'A Stimuli', 'B Additional GPT-2 experiments', 'B.1 Results']"
Investigating the Nature of Disagreements on Mid-Scale Ratings: A Case Study on the Abstractness-Concreteness Continuum,Urban Knupleš; Diego Frassinelli; Sabine Schulte Im Walde,"Humans tend to strongly agree on ratings on a scale for extreme cases (e.g., a CAT is judged as very concrete), but judgements on mid-scale words exhibit more disagreement. Yet, collected rating norms are heavily exploited across disciplines. Our study focuses on concreteness ratings and (i) implements correlations and supervised classification to identify salient multimodal characteristics of mid-scale words, and (ii) applies a hard clustering to identify patterns of systematic disagreement across raters. Our results suggest to either fine-tune or filter midscale target words before utilising them.",,"['Motivation', 'Related Work', 'Concreteness Targets and Ratings', 'Target Words: Characteristics', 'Characteristics and Resources', 'Word Classes and Resource Coverage', 'Holistic Perspective', 'Mid-Scale Peculiarities', 'Mid-Scale Disagreement Patterns', 'Discussion & Conclusion', 'Limitations', 'A Dominance of Perception across Targets', 'D Mid-Scale Definitions, Ranges and Classifications across Word Classes', 'Acknowledgements', 'Ethics Statement']"
ArchBERT: Bi-Modal Understanding of Neural Architectures and Natural Languages,Mohammad Akbari; Saeed Ranjbar Alvar; Behnam Kamranian; Amin Banitalebi-Dehkordi; Yong Zhang,"Building multi-modal language models has been a trend in the recent years, where additional modalities such as image, video, speech, etc. are jointly learned along with natural languages (i.e., textual information). Despite the success of these multi-modal language models with different modalities, there is no existing solution for neural network architectures and natural languages. Providing neural architectural information as a new modality allows us to provide fast architecture-2-text and text-2-architecture retrieval/generation services on the cloud with a single inference. Such solution is valuable in terms of helping beginner and intermediate ML users to come up with better neural architectures or AutoML approaches with a simple text query. In this paper, we propose ArchBERT, a bi-modal model for joint learning and understanding of neural architectures and natural languages, which opens up new avenues for research in this area. We also introduce a pre-training strategy named Masked Architecture Modeling (MAM) for a more generalized joint learning. Moreover, we introduce and publicly release two new bi-modal datasets for training and validating our methods. The ArchBERT's performance is verified through a set of numerical experiments on different downstream tasks such as architecture-oriented reasoning, question answering, and captioning (summarization). Datasets, codes, and demos are available as supplementary materials 1 .",,"['Introduction', 'Related Works', 'Proposed Method: ArchBERT', 'Masked Architecture Modeling (MAM)', 'Architectural Question Answering (AQA)', 'Language Decoder', 'Datasets', 'TVHF', 'AutoNet', 'AutoNet-AQA', 'Experimental Results', 'Uni-Modal Baselines', 'Architectural Reasoning (AR)', 'As reported in', 'Architecture Clone Detection (ACD)', 'Architectural Question Answering (AQA)', 'Architecture Captioning (AC)', 'Architecture Search (AS)', 'Qualitative Results', 'Ablation Study', 'Embeddings Visualization', 'Conclusion', 'A.3 Embeddings Visualization', 'A.4 Data Generation', 'Algorithm 1 TVHF dataset generator', 'Input:', 'A.5 Distribution Plots for TVHF and AutoNet', 'A.6 Sample Data from TVHF and AutoNet', 'A.7 Dataset Quality Analysis', 'A.7.1 Reliability and Completeness', 'A.7.2 Label/Feature Noise', 'A.7.3 Feature Representation', 'A Appendix', ""A.2 ArchBERT's Performance on OOD Data""]"
"A Comparative Study on Textual Saliency of Styles from Eye Tracking, Annotations, and Language Models",Karin De Langis; Dongyeop Kang,"There is growing interest in incorporating eyetracking data and other implicit measures of human language processing into natural language processing (NLP) pipelines. The data from human language processing contain unique insight into human linguistic understanding that could be exploited by language models. However, many unanswered questions remain about the nature of this data and how it can best be utilized in downstream NLP tasks. In this paper, we present eyeStyliency, an eye-tracking dataset for human processing of stylistic text (e.g., politeness). We develop a variety of methods to derive style saliency scores over text using the collected eye dataset. We further investigate how this saliency data compares to both human annotation methods and model-based interpretability metrics. We find that while eyetracking data is unique, it also intersects with both human annotations and model-based importance scores, providing a possible bridge between human-and machine-based perspectives. We propose utilizing this type of data to evaluate the cognitive plausibility of models that interpret style. Our eye-tracking data and processing code are publicly available. 1  ",,"['Introduction', 'Related Work', 'eyeStyliency: A Dataset of Eye Movement for Textual Saliency', 'Data Setups', 'Eye-Tracking Measures', 'Experimental Procedure', 'Congruent Setup', 'Incongruent Setup Context', 'Stimuli', 'Eye-based saliency', 'Pre-processing Eye Tracking Data', 'Calculating Saliency Scores', 'Comparison with Other Saliency Metrics', 'Qualitative Results', '""Eye-in-the-loop"" few-shot learning', 'Key Findings and Discussion', 'Limitations', 'Acknowledgements', 'A Appendix', 'A.1 Experimental Materials', 'A.2 Mixed Effect Modeling', 'A.3 Additional Saliency Comparisons', 'A.3.1 Saliency Scores', 'A.4 Few-Shot Learning Experiment Details and Results']"
PROPRES: Investigating the Projectivity of Presupposition with Various Triggers and Environments,Daiki Asami; Saku Sugawara,"What makes a presupposition of an utteranceinformation taken for granted by its speakerdifferent from other pragmatic inferences such as an entailment is projectivity (e.g., the negative sentence the boy did not stop shedding tears presupposes the boy had shed tears before). The projectivity may vary depending on the combination of presupposition triggers and environments. However, prior natural language understanding studies fail to take it into account as they either use no human baseline or include only negation as an entailment-canceling environment to evaluate models' performance. The current study attempts to reconcile these issues. We introduce a new dataset, projectivity of presupposition (PROPRES), which includes 12k premise-hypothesis pairs crossing six triggers involving some lexical variety with five environments. Our human evaluation reveals that humans exhibit variable projectivity in some cases. However, the model evaluation shows that the best-performed model, DeBERTa, does not fully capture it. Our findings suggest that probing studies on pragmatic inferences should take extra care of the human judgment variability and the combination of linguistic items.",,"['Introduction', 'Background', 'Presupposition in Linguistics', 'Presupposition in NLI', 'Setup', 'Results and Discussion', 'Experiment 2: PROPRES', 'Data Generation', 'Setup', 'Model Evaluation', 'Results and Discussion', 'Conclusion', 'A Limitations', 'B Templates', 'C Crowdsourcing Human Evaluation', 'D Triggers and Environments in IMPPRES', 'E Results without Exclusion', 'Acknowledgments']"
