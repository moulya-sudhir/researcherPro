title,authors,abstract,pub_date,sections_list,processed
One Cannot Stand for Everyone! Leveraging Multiple User Simulators to train Task-oriented Dialogue Systems,Yajiao Liu; Xin Jiang; Yichun Yin; Yasheng Wang; Fei Mi; Qun Liu; Xiang Wan; Benyou Wang,"User simulators are agents designed to imitate human users; recent advances have found that Task-oriented Dialogue (ToD) systems optimized toward a user simulator could better satisfy the need of human users. However, this might result in a sub-optimal ToD system if it is tailored to only one ad hoc user simulator, since human users can behave differently. In this paper, we propose a framework called MUST 1 to optimize ToD systems via leveraging Multiple User SimulaTors. The main challenges of implementing the MUST are 1) how to adaptively determine which user simulator to interact with the ToD system at each optimization step, since the ToD system might be over-fitted to some specific user simulators, and simultaneously underfitted to some others; 2) how to avoid catastrophic forgetting of the adaption for a simulator that is not selected for several consecutive optimization steps. To tackle these challenges, we formulate MUST as a Multi-armed bandits (MAB) problem and provide a method called MUST adaptive that balances i) the boosting adaption for adaptive interactions between different user simulators and the ToD system and ii) the uniform adaption to avoid the catastrophic forgetting issue. With both automatic evaluations and human evaluations, our experimental results on MultiWOZ show that the dialogue system trained by MUST achieves a better performance than those trained by a single user simulator. It also has a better generalization ability when testing with unseen user simulators.",,"['Introduction', 'Background', 'MUST: a Framework to Leverage', 'Some Preliminary Proposals for MUST', 'MUST as a MAB Problem', 'Formulating MUST as a MAB Problem', 'Training with MUST adaptive', 'Experiments', 'Experimental Setup', 'Implementations', 'Two new User Simulators', 'Dialogue Systems', 'Experimental Results', 'Analysis and Discussions', '(b)-2(e).', 'Conclusion', 'Limitation', 'Ethics Statement', 'A Multi-armed bandit problem', 'B More details about training dialogue systems B.1 The architectures of user simulators and dialogue systems', 'B.2 The implementations of the dialogue systems', 'B.3 The details of running policy gradient algorithm', 'B.4 The parameters of training Sys-MUST adaptive', 'B.5 Human Evaluation on dialogue systems', 'C Implement MUST with the MUST CRL strategy', 'D Sensitivity on different subsets of user simulators', 'E Ablation study for the modified UCB1 algorithm E.1 Necessity of the exploration term', 'E.2 Ablation study on the designed distribution', 'F Implementing MUST with more user simulators', 'G Modeling User Simulator with GPT', 'G.1 The architecture of U-GPT', 'G.2 Evaluations on U-GPT', 'G.3 Training details of user simulators', 'System Agent User Simulator', 'G.4 Experiments', 'Acknowledgements', '', 'C Did you run computational experiments?', 'See appendix']","user simulator agent designed imitate human user ; recent advance found task-oriented dialogue ( tod ) system optimized toward user simulator could better satisfy need human user . however , might result sub-optimal tod system tailored one ad hoc user simulator , since human user behave differently . paper , propose framework called must 1 optimize tod system via leveraging multiple user simulator . main challenge implementing must 1 ) adaptively determine user simulator interact tod system optimization step , since tod system might over-fitted specific user simulator , simultaneously underfitted others ; 2 ) avoid catastrophic forgetting adaption simulator selected several consecutive optimization step . tackle challenge , formulate must multi-armed bandit ( mab ) problem provide method called must adaptive balance ) boosting adaption adaptive interaction different user simulator tod system ii ) uniform adaption avoid catastrophic forgetting issue . automatic evaluation human evaluation , experimental result multiwoz show dialogue system trained must achieves better performance trained single user simulator . also better generalization ability testing unseen user simulator ."
SAFECONV: Explaining and Correcting Conversational Unsafe Behavior,Mian Zhang; Jin ⋄ Lifeng; Linfeng Song; ⋄ Haitao; Wenliang Chen; Dong Yu,"One of the main challenges open-domain endto-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior. In this work, we construct a new dataset called SAFECONV for the research of conversational safety: (1) Besides the utterancelevel safety labels, SAFECONV also provides unsafe spans in an utterance, information able to indicate which words contribute to the detected unsafe behavior; (2) SAFECONV provides safe alternative responses to continue the conversation when unsafe behavior detected, guiding the conversation to a gentle trajectory. By virtue of the comprehensive annotation of SAFECONV, we benchmark three powerful models for the mitigation of conversational unsafe behavior, including a checker to detect unsafe utterances, a tagger to extract unsafe spans, and a rewriter to convert an unsafe response to a safe version. Moreover, we explore the huge benefits brought by combining the models for explaining the emergence of unsafe behavior and detoxifying chatbots. Experiments show that the detected unsafe behavior could be well explained with unsafe spans and popular chatbots could be detoxified by a huge extent. The dataset is available at https://github.com/mianzhang/SafeConv. Warning: This paper contains cases that may be offensive or upsetting.",,"['Introduction', 'Dataset', 'Source', 'Related Work', 'Data Collection', 'Data Sources', 'Data Selection', 'Human Annotation', 'Base Models', 'Results', 'Explainable Safety Checking', 'Correct Conversational Unsafe Behavior via Contextual Rewriting', 'Conclusion', 'Ethics Considerations', 'Limitations', 'Acknowledgements', '3', 'Left blank.']","one main challenge open-domain endto-end dialogue system , chatbots , face prevalence unsafe behavior , toxic language harmful suggestion . however , existing dialogue datasets provide enough annotation explain correct unsafe behavior . work , construct new dataset called safeconv research conversational safety : ( 1 ) besides utterancelevel safety label , safeconv also provides unsafe span utterance , information able indicate word contribute detected unsafe behavior ; ( 2 ) safeconv provides safe alternative response continue conversation unsafe behavior detected , guiding conversation gentle trajectory . virtue comprehensive annotation safeconv , benchmark three powerful model mitigation conversational unsafe behavior , including checker detect unsafe utterance , tagger extract unsafe span , rewriter convert unsafe response safe version . moreover , explore huge benefit brought combining model explaining emergence unsafe behavior detoxifying chatbots . experiment show detected unsafe behavior could well explained unsafe span popular chatbots could detoxified huge extent . dataset available http : //github.com/mianzhang/safeconv . warning : paper contains case may offensive upsetting ."
"Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better",David Dale; Elena Voita; Loïc Barrault; Marta R Costa-Jussà; Meta Ai,"While the problem of hallucinations in neural machine translation has long been recognized, so far the progress on its alleviation is very little. Indeed, recently it turned out that without artificially encouraging models to hallucinate, previously existing methods fall short and even the standard sequence log-probability is more informative. It means that internal characteristics of the model can give much more information than we expect, and before using external models and measures, we first need to ask: how far can we go if we use nothing but the translation model itself ? We propose to use a method that evaluates the percentage of the source contribution to a generated translation. Intuitively, hallucinations are translations ""detached"" from the source, hence they can be identified by low source contribution. This method improves detection accuracy for the most severe hallucinations by a factor of 2 and is able to alleviate hallucinations at test time on par with the previous best approach that relies on external models. Next, if we move away from internal model characteristics and allow external tools, we show that using sentence similarity from cross-lingual embeddings further improves these results. We release the code of our experiments. 1",,"['Introduction', 'Background and Setting', 'Model', 'Hallucination Dataset', 'Reference-Based Oracles', 'Internal Measures', 'External models', 'Main results', 'Analysing Distributions of the Scores', 'Detected Pathology Types', 'Mitigating Hallucinations at Test Time', 'Evaluation methodology', 'Metrics.', 'Generation Strategies', 'The Impact of Generation Strategy', 'The Impact of Number of Hypotheses', 'Reranking Approaches', 'Automatic Evaluation', 'S. O.', 'Human evaluation', 'Conclusions', 'Limitations', 'Ethical statement', 'A Implementation and computing', 'B Mitigating Hallucinations at Test Time', 'C Manual Evaluation', '', 'C Did you run computational experiments?']","problem hallucination neural machine translation long recognized , far progress alleviation little . indeed , recently turned without artificially encouraging model hallucinate , previously existing method fall short even standard sequence log-probability informative . mean internal characteristic model give much information expect , using external model measure , first need ask : far go use nothing translation model ? propose use method evaluates percentage source contribution generated translation . intuitively , hallucination translation `` detached '' source , hence identified low source contribution . method improves detection accuracy severe hallucination factor 2 able alleviate hallucination test time par previous best approach relies external model . next , move away internal model characteristic allow external tool , show using sentence similarity cross-lingual embeddings improves result . release code experiment . 1"
Explainable Recommendation with Personalized Review Retrieval and Aspect Learning,Hao Cheng; Shuo Wang; Wensheng Lu; Wei Zhang; Mingyang Zhou; Kezhong Lu; Hao Liao,"Explainable recommendation is a technique that combines prediction and generation tasks to produce more persuasive results. Among these tasks, textual generation demands large amounts of data to achieve satisfactory accuracy. However, historical user reviews of items are often insufficient, making it challenging to ensure the precision of generated explanation text. To address this issue, we propose a novel model, ERRA (Explainable Recommendation by personalized Review retrieval and Aspect learning). With retrieval enhancement, ERRA can obtain additional information from the training sets. With this additional information, we can generate more accurate and informative explanations. Furthermore, to better capture users' preferences, we incorporate an aspect enhancement component into our model. By selecting the top-n aspects that users are most concerned about for different items, we can model user representation with more relevant details, making the explanation more persuasive. To verify the effectiveness of our model, extensive experiments on three datasets show that our model outperforms state-of-theart baselines (for example, 3.4% improvement in prediction and 15.8% improvement in explanation for TripAdvisor).",,"['Introduction', 'Explainable Recommendation with Generation', 'Retrieval Enhancement', 'Problem Statement', 'Input Data', 'Output Data', 'Methodology', 'Overview of Model', 'Retrieval Enhancement', 'Retrieval Encode', 'Retrieval Method', 'Aspect Enhancement', 'Joint Enhancement Transformers', 'Rating Prediction', 'Explanation Generation', 'Aspect Discriminator', 'Text Generation', 'Multi-Task Learning', 'Experiments', 'Datasets', 'Evaluation Metrics', 'Baseline Methods', 'Prediction', 'Explainability', 'Reproducibility', 'Explainability Study', 'Accuracy of Prediction', 'Ablation Analysis', 'Conclusion', 'Limitation', 'Acknowledgments', '']","explainable recommendation technique combine prediction generation task produce persuasive result . among task , textual generation demand large amount data achieve satisfactory accuracy . however , historical user review item often insufficient , making challenging ensure precision generated explanation text . address issue , propose novel model , erra ( explainable recommendation personalized review retrieval aspect learning ) . retrieval enhancement , erra obtain additional information training set . additional information , generate accurate informative explanation . furthermore , better capture user ' preference , incorporate aspect enhancement component model . selecting top-n aspect user concerned different item , model user representation relevant detail , making explanation persuasive . verify effectiveness model , extensive experiment three datasets show model outperforms state-of-theart baseline ( example , 3.4 % improvement prediction 15.8 % improvement explanation tripadvisor ) ."
Binary and Ternary Natural Language Generation,Zechun Liu; Barlas Oguz; Meta Ai; Aasish Pappu; Yangyang Shi; Raghuraman Krishnamoorthi,"Ternary and binary neural networks enable multiplication-free computation and promise multiple orders of magnitude efficiency gains over full-precision networks if implemented on specialized hardware. However, since both the parameter and the output space are highly discretized, such networks have proven very difficult to optimize. The difficulties are compounded for the class of transformer text generation models due to the sensitivity of the attention operation to quantization and the noise-compounding effects of autoregressive decoding in the high-cardinality output space. We approach the problem with a mix of statistics-based quantization for the weights and elastic quantization of the activations and demonstrate the first ternary and binary transformer models on the downstream tasks of summarization and machine translation. Our ternary BART base achieves an R1 score of 41 on the CNN/DailyMail benchmark, which is merely 3.9 points behind the full model while being 16x more efficient. Our binary model, while less accurate, achieves a highly nontrivial score of 35.6. For machine translation, we achieved BLEU scores of 21.7 and 17.6 on the WMT16 En-Ro benchmark, compared with a full precision mBART model score of 26.8. We also compare our approach in the 8-bit activation setting, where our ternary and even binary weight models can match or outperform the best existing 8-bit weight models in the literature. Our code and models are available at: https://github.com/facebookresearch/ Ternary_Binary_Transformer.",,"['Introduction', 'Method', 'Preliminary', 'Ternarization', 'Binarization', 'Stats-based max-entropy isometric weight quantization', 'Learning-based activation quantization', 'Experiments', 'Experimental settings', 'Summarization', 'Machine translation', 'Ablations', 'Sequence length analysis', 'Weights Activations', 'Visualization', 'Conclusion', 'Limitations', 'Ethics Statement']","ternary binary neural network enable multiplication-free computation promise multiple order magnitude efficiency gain full-precision network implemented specialized hardware . however , since parameter output space highly discretized , network proven difficult optimize . difficulty compounded class transformer text generation model due sensitivity attention operation quantization noise-compounding effect autoregressive decoding high-cardinality output space . approach problem mix statistics-based quantization weight elastic quantization activation demonstrate first ternary binary transformer model downstream task summarization machine translation . ternary bart base achieves r1 score 41 cnn/dailymail benchmark , merely 3.9 point behind full model 16x efficient . binary model , le accurate , achieves highly nontrivial score 35.6. machine translation , achieved bleu score 21.7 17.6 wmt16 en-ro benchmark , compared full precision mbart model score 26.8. also compare approach 8-bit activation setting , ternary even binary weight model match outperform best existing 8-bit weight model literature . code model available : http : //github.com/facebookresearch/ ternary_binary_transformer ."
Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking,Björn Bebensee; Haejun Lee,"In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and Mul-tiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30× larger D3ST-XXL model by 5.0 points.",,"['Introduction', 'Approach', 'Task Formulation', 'Joint Encoding with Linear Attention', 'Intent Classification', 'Span Pointer Module', 'Pre-Training via Recurrent Span Selection', 'Experimental Setup', 'Benchmark Datasets', 'Evaluation Metrics', 'Implementation Details', 'Evaluation', 'Baselines', '""[ACTION] Offer [SLOT] location [VALUE]', 'Main Results', 'Robustness', 'Generalization to unseen domains', 'Ablation Study', 'Related Work', 'Conclusion', 'Limitations', 'Ethics Statement', ' [INTENT] ', 'C Did you run computational experiments?']","schema-guided dialogue state tracking model estimate current state conversation using natural language description service schema generalization unseen service . prior generative approach decode slot value sequentially generalize well variation schema , discriminative approach separately encode history schema fail account inter-slot intent-slot dependency . introduce splat , novel architecture achieves better generalization efficiency prior approach constraining output limited prediction space . time , model allows rich attention among description history keeping computation cost constrained incorporating linear-time attention . demonstrate effectiveness model schema-guided dialogue ( sgd ) mul-tiwoz datasets . approach significantly improves upon existing model achieving 85.3 jga sgd dataset . , show increased robustness sgd-x benchmark : model outperforms 30× larger d3st-xxl model 5.0 point ."
EM Pre-training for Multi-party Dialogue Response Generation,Yiyang Li; Hai Zhao,"Dialogue response generation requires an agent to generate a response according to the current dialogue history, in terms of which twoparty dialogues have been well studied, but leaving a great gap for multi-party dialogues at the same time. Different from two-party dialogues where each response is a direct reply to its previous utterance, the addressee of a response utterance should be specified before it is generated in the multi-party scenario. Thanks to the huge amount of two-party conversational data, various pre-trained language models for two-party dialogue response generation have been proposed. However, due to the lack of annotated addressee labels in multi-party dialogue datasets, it is hard to use them to pre-train a response generation model for multi-party dialogues. To tackle this obstacle, we propose an Expectation-Maximization (EM) approach that iteratively performs the expectation steps to generate addressee labels, and the maximization steps to optimize a response generation model. Theoretical analyses and extensive experiments have justified the feasibility and effectiveness of our proposed method. The official implementation of this paper is available at https://github.com/EricLee8/MPDRG.",,"['Introduction', 'Pre-training for Response Generation', 'Human Response:', 'Multi-party Dialog Response Generation', 'Methodology', 'Task Formulation', 'Addressee Modeling', 'Latent Variable Prediction', 'Expectation-Maximization Process', 'Proof of Feasibility', 'Experiments', 'Datasets and Experimental Setups', 'Baseline Models and Evaluation Metrics', 'Automatic Evaluation Results', 'Human Evaluation Results', 'Analysis', 'Ablation Study', 'Response Generation vs. Addressee Prediction', 'Case Studies', 'Response Parser: A Byproduct for Free', 'Limitations', '', 'C Did you run computational experiments?']","dialogue response generation requires agent generate response according current dialogue history , term twoparty dialogue well studied , leaving great gap multi-party dialogue time . different two-party dialogue response direct reply previous utterance , addressee response utterance specified generated multi-party scenario . thanks huge amount two-party conversational data , various pre-trained language model two-party dialogue response generation proposed . however , due lack annotated addressee label multi-party dialogue datasets , hard use pre-train response generation model multi-party dialogue . tackle obstacle , propose expectation-maximization ( em ) approach iteratively performs expectation step generate addressee label , maximization step optimize response generation model . theoretical analysis extensive experiment justified feasibility effectiveness proposed method . official implementation paper available http : //github.com/ericlee8/mpdrg ."
ACLM: A Selective-Denoising based Generative Data Augmentation Approach for Low-Resource Complex NER,Sreyan Ghosh; Utkarsh Tyagi; Manan Suri; ♣ Sonal,"Complex Named Entity Recognition (NER) is the task of detecting linguistically complex named entities in low-context text. In this paper, we present ACLM (Attention-map aware keyword selection for Conditional Language Model fine-tuning), a novel data augmentation approach, based on conditional generation, to address the data scarcity problem in lowresource complex NER. ACLM alleviates the context-entity mismatch issue, a problem existing NER data augmentation techniques suffer from and often generates incoherent augmentations by placing complex named entities in the wrong context. ACLM builds on BART and is optimized on a novel text reconstruction or denoising task -we use selective masking (aided by attention maps) to retain the named entities and certain keywords in the input sentence that provide contextually relevant additional knowledge or hints about the named entities. Compared with other data augmentation strategies, ACLM can generate more diverse and coherent augmentations preserving the true word sense of complex entities in the sentence. We demonstrate the effectiveness of ACLM both qualitatively and quantitatively on monolingual, crosslingual, and multilingual complex NER across various low-resource settings. ACLM outperforms all our neural baselines by a significant margin (1%-36%). In addition, we demonstrate the application of ACLM to other domains that suffer from data scarcity (e.g., biomedical). In practice, ACLM generates more effective and factual augmentations for these domains than prior methods.",,"['Introduction', 'Background and Related Work', 'Methodology', 'Keyword Selection', 'Generated Samples:', 'Template Creation', 'Fine-tuning ACLM', 'Data Generation', 'Post-Processing', 'Experiments and Results', 'Dataset', 'Algorithm 1 ACLM: Our proposed augmentation framework', 'Baselines', 'Experimental Results', 'Original', 'LwTR', 'MELM', 'ACLM w/o mixner', 'Conclusion', 'Limitations', 'A Hyperparameter Tuning', 'C Templates and Attention Maps', 'D Qualitative Analysis of Augmentations D.1 Augmentation Examples', 'Original', 'LwTR', 'MELM', 'ACLM w/o mixner', 'ACLM w/ mixner', 'Original', 'LwTR', 'MELM', 'ACLM w/o mixner', 'ACLM w/ mixner', '']","complex named entity recognition ( ner ) task detecting linguistically complex named entity low-context text . paper , present aclm ( attention-map aware keyword selection conditional language model fine-tuning ) , novel data augmentation approach , based conditional generation , address data scarcity problem lowresource complex ner . aclm alleviates context-entity mismatch issue , problem existing ner data augmentation technique suffer often generates incoherent augmentation placing complex named entity wrong context . aclm build bart optimized novel text reconstruction denoising task -we use selective masking ( aided attention map ) retain named entity certain keywords input sentence provide contextually relevant additional knowledge hint named entity . compared data augmentation strategy , aclm generate diverse coherent augmentation preserving true word sense complex entity sentence . demonstrate effectiveness aclm qualitatively quantitatively monolingual , crosslingual , multilingual complex ner across various low-resource setting . aclm outperforms neural baseline significant margin ( 1 % -36 % ) . addition , demonstrate application aclm domain suffer data scarcity ( e.g. , biomedical ) . practice , aclm generates effective factual augmentation domain prior method ."
Natural Language to Code Generation in Interactive Data Science Notebooks,Pengcheng Yin; Wen-Ding Li; Kefan Xiao; Abhishek Rao; Yeming Wen; Kensen Shi; Joshua Howland; Paige Bailey; Michele Catasta; Henryk Michalewski; Alex Polozov; Charles Sutton; Jacob Andreas; Johannes Bufe; David Burkett; Charles C Chen; Joshua Clausman; Jean Crawford; Kate Crim; Jordan Deloach; Leah Dorner; Jason Eisner; Hao Fang; Alan Guo; David Leo; Wright Hall; Kristin Delia Hayes; Kellie Hill; Diana Ho; Wendy Iwaszuk; Smriti Jha; Dan Klein; Theo Lanman; Percy Liang; C H Lin; Ilya Lintsbakh; Andy Mcgovern; Aleksandr Nisnevich; Adam Pauls; Dmitrij Petters; Brent Read; Dan Roth; Subhro Roy; Jesse Rusak; Beth Ann Short; Div Slomin; B Snyder; Stephon Striplin; Yu Su; Zachary Tellman; Sam Thomson; A A Vorobev; Izabela Witoszko; Jason Wolfe; A G Wray; Yuchen Zhang; Jacob Austin; Augustus Odena; Maxwell Nye; Maarten Bosma; David Dohan; Ellen Jiang; Carrie Cai; Michael Terry; Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Bei Chen; Fengji Zhang; A Nguyen; Daoguang Zan; Zeqi Lin; Jian-Guang Lou; Weizhu Chen; Mark Chen; Jerry Tworek; Heewoo Jun; Qiming Yuan; Henrique Ponde; Harrison Edwards; Yura Burda; Nicholas Joseph; Greg Brockman; Alex Ray; Raul Puri; Gretchen Krueger; Michael Petrov; Heidy Khlaaf; Pamela Mishkin; Xinyun Chen; Petros Maniatis; Rishabh Singh; Hanjun Dai; Max Lin; Denny 2022 Zhou;  Spreadsheetcoder; Qinyu Cheng; Linyang Li; Guofeng Quan; Feng Gao; Xiaofeng Mou; Xipeng 2022a Qiu; Zhoujun Cheng; Tianbao Xie; Peng Shi; Chengzu Li; Rahul Nadkarni; Yushi Hu; Caiming 2022 Xiong; Dragomir R Radev; Mari Ostendorf; Luke Zettlemoyer; Matthias Feurer; Aaron Klein; Katharina Eggensperger; Daniel Fried; Armen Aghajanyan; Jessy Lin; Sida I Wang; Eric Wallace; Freda Shi; Ruiqi Zhong; Yujian Gan; Qiuping Huang; Matthew Purver; John R Woodward; Jinxia Xie; Peng- Sheng Huang; Luyu Gao; Aman Madaan; Shuyan Zhou; Uri Alon; Ehsan Hosseini-Asl; Bryan Mccann; Chien-Sheng Wu; Semih Yavuz; Richard 2020 Socher; Junjie Huang; Chenglong Wang; Jipeng Zhang; Cong Yan; Haotian Cui; Jeevana Priya Inala; Colin Clement; Nan Duan; Jianfeng Gao; Sean Kandel; Andreas Paepcke; Joseph Hellerstein; Jeffrey Heer 2011 Wrangler; Shubhra ( Santu; ) Karmaker; Micah J Hassan; Lei Smith; Chengxiang Xu; Kalyan Veeramachaneni Zhai; Thomas Kluyver; Benjamin Ragan-Kelley; Fer- Nando Pérez; Brian Granger; Matthias Bussonnier; Jonathan Frederic; Kyle Kelley; Jessica Hamrick; Jason Grout; Sylvain Corlay; Paul Ivanov; Damián Avila; Safia Abdalla; Yuhang Lai; Chengxi Li; Yiming Wang; Tianyi Zhang; Scott Wen; Daniel Yih; Sida Fried; Tao Wang;  Yu; Chia-Hsuan Lee; Oleksandr Polozov; Matthew Richardson;  Kaggledbqa; Zi Lin; Jeremiah Liu; Jingbo Shang;  Neural; Reginald Long; Panupong Pasupat; Arpit Narechania; Arjun Srinivasan; John T Stasko;  Nl4dv; Alfredo Nazabal; Christopher K I Williams; Gio- Vanni Colavizza; Camila Rangel Smith; Erik Nijkamp; Bo Pang; Hiroaki Hayashi; Lifu Tu; Huan Wang; Yingbo Zhou; Silvio Savarese; Anders Andreassen; Guy Gur-Ari; David Bieber; Aitor Lewkowycz; David Luan; David A Patterson; Joseph Gonzalez; Quoc V Le; Chen Liang; Lluís-Miquel Munguía; Daniel Rothchild; David R So; Jianlin Su; Yu Lu; Shengfeng Pan; Bo Wen; Yunfeng Liu;  Roformer; April Yi Wang; Dakuo Wang; Jaimie Drozdal; Michael Muller; Soya Park; Justin D Weisz; Xuye Liu; Lingfei Wu; Casey Dugan;  Documen; Yu Feng; Rastislav Bodik; Josh Andres; Erick Oduor;  Autods; Vera Liao; Yunfeng Zhang; Udayan Khurana; Horst Samulowitz; Lisa Amini; Jason Wei; Xuezhi Wang; Dale Schuurmans; Ed Chi; Zhengkai Wu; Vu Le; Ashish Tiwari; Sumit Gulwani; Arjun Radhakrishna; Ivan Radicek; Gustavo Soares; Chen Henry Wu; Torsten Scholak; Michihiro Yasunaga; Chien-Sheng Wu; Ming Zhong; Vic- Tor Zhong; Bailin Wang; Chengzu Li; Connor Boyle; Ansong Ni; Ziyu Yao; Lingpeng Kong; Rui Zhang; Noah A Smith; Yunyi Yang; Yunhao Li; Xiaojun 2020 Quan; Tao Yu; Yang Er; Suyi Li; Eric Xue; Victoria Xi; Yi Chern Lin; Tianze Tan; Zihan Shi; Youxuan Li; Michihiro Jiang; Sun- Grok Yasunaga; Tao Shim; Alexander R Chen; Zifan Fabbri; Luyao Li; Yuwen Chen; Shreya Zhang; Vincent Dixit; Caiming Zhang; Richard Xiong; Walter S Socher; Dragomir R Lasecki;  Radev;  Cosql; Yi Chern Tan; Victoria Lin; Irene Z Li; Tao Chen; Emily Ji; Shreya Dixit; David Proctor; Sungrok Shim; Jonathan Kraft; Vin- Cent Zhang; R 2019b Radev,"Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1,078 code generation problems using the pandas data analysis framework in data science notebooks. AR-CADE features multiple rounds of NL-to-code problems from the same notebook. It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. To establish a strong baseline on this challenging task, we develop PACH-INCO, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanations, showing the potential to improve the diversity and explainability of model predictions. ARCADE is publicly available at https://github.com/ google-research/arcade-nl2code/.",,"['Introduction', 'Problem Statement', 'ARCADE: A Benchmark of pandas', 'Creating Notebooks for Novel ML Datasets', 'Dataset Analysis', 'Evaluation by Fuzzy Output Matching', 'PACHINCO: Adapting Code LMs to Computational Notebooks', 'LM Prompting Strategies', 'Main Results', 'Few-shot Prompting Results', 'Step-by-Step Prompting Diversifies Solutions', 'Case Study: How Useful is Predicted', 'Related Work', 'Conclusion', 'Limitations', 'Supplementary Materials A Details of Dataset Construction', 'A.1 Mining Examples from Existing Notebooks', 'A.2 Creating Notebooks with Examples from Scratch', 'A.3 Annotation Process and Quality Assurance', 'B Outline of ARCADE Annotation Guideline', 'For each code snippet identified in', 'What Tasks to Create', 'Complexity of Tasks', 'C Descriptions of Existing Data Science Code Generation Dataset', 'D Details of Fine-tuning PACHINCO', 'E Inference Setup', 'F CODEGEN Scaling Curve on ARCADE', 'G Break-down Analysis of pass@k on ARCADE', 'H Additional Few-shot Prompting Results', 'I Further Analysis of Solution Diversity', 'J Error Analysis J.1 Summary of Error Types', 'J.2 Case Study', 'Case Study for Executable but Incorrect Predictions', 'K Data Card for the Training Data of PACHINCO', 'Motivation', 'Composition', 'Is any information missing from individual instances?', 'Collection Process', 'Preprocessing, cleaning, and labeling', 'Uses', 'Distribution', 'Acknowledgements', '']","computational notebook , jupyter notebook , interactive computing environment ubiquitous among data scientist perform data wrangling analytic task . measure performance ai pair programmer automatically synthesize program task given natural language ( nl ) intent user , build arcade , benchmark 1,078 code generation problem using panda data analysis framework data science notebook . ar-cade feature multiple round nl-to-code problem notebook . requires model understand rich multi-modal context , existing notebook cell execution state well previous turn interaction . establish strong baseline challenging task , develop pach-inco , 62b code language model ( lm ) python computational notebook , significantly outperforms public code lm . finally , explore few-shot prompting strategy elicit better code step-by-step decomposition nl explanation , showing potential improve diversity explainability model prediction . arcade publicly available http : //github.com/ google-research/arcade-nl2code/ ."
Program Chairs' Report on Peer Review at ACL 2023,Anna Rogers; Marzena Karpinska ~jordan Boyd-Graber; Naoaki Okazaki,"We present a summary of the efforts to improve conference peer review that were implemented at ACL'23. This includes work with the goal of improving review quality, clearer workflow and decision support for the area chairs, as well as our efforts to improve paper-reviewer matching for various kinds of nonmainstream NLP work, and improve the overall incentives for all participants of the peer review process. We present analysis of the factors affecting peer review, identify the most problematic issues that the authors complained about, and provide suggestions for the future chairs. We hope that publishing such reports would (a) improve transparency in decision-making, (b) help the people new to the field to understand how the *ACL conferences work, (c) provide useful data for the future chairs and workshop organizers, and also academic work on peer review, and (d) provide useful context for the final program, as a source of information for meta-research on the structure and trajectory of the field of NLP.",,"['Introduction', 'Tracks and Acceptance Statistics', 'Resubmissions', ""Authors and Reviewers at ACL'23"", 'Efforts towards improving review quality', 'Reviewer training', 'ACL paper-reviewer matching: Area-Contribution-Language', 'Review issue flagging', 'Reviewer discussion', 'Improving decision support for the chairs', 'Updated SAC and AC guidelines', 'Support for checking assignments', 'Paper-reviewer match rationales', 'Soundness/Excitement scores', 'What Factors Contribute to ACL Peer Review Outcome?', 'Review Scores: Overall Distribution', 'The Impact of Other Submission Properties', 'Variable Importance', 'Short/long papers', 'Types of contribution', 'How Much do ACL Reviewers Agree?', '26', 'Analysing Reviews and Review Scores', 'Do the Area-Contribution-Language matches impact reviewer scores?', 'Do the Area-Contribution-Language matches impact the reviewer activity?', 'Do reviewer confidence scores reflect their experience?', 'Do the reviewer scores correlate with length of the reviews?', 'What factors are associated with review issues?', 'Do we have bad actors?', 'Can the reviewers tell who the authors are?', 'Ethics review', 'Best paper selection', 'Improving Incentives for Reviewers: Reviewer Awards', 'Improving Incentives for Chairs: Peer Review Reports', 'Recommendations', '', '']","present summary effort improve conference peer review implemented acl'23 . includes work goal improving review quality , clearer workflow decision support area chair , well effort improve paper-reviewer matching various kind nonmainstream nlp work , improve overall incentive participant peer review process . present analysis factor affecting peer review , identify problematic issue author complained , provide suggestion future chair . hope publishing report would ( ) improve transparency decision-making , ( b ) help people new field understand * acl conference work , ( c ) provide useful data future chair workshop organizer , also academic work peer review , ( ) provide useful context final program , source information meta-research structure trajectory field nlp ."
"Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics",Yuhan Zhang; Edward Gibson; Forrest Davis,"Language models (LMs) have been argued to overlap substantially with human beings in grammaticality judgment tasks. But when humans systematically make errors in language processing, should we expect LMs to behave like cognitive models of language and mimic human behavior? We answer this question by investigating LMs' more subtle judgments associated with ""language illusions"" -sentences that are vague in meaning, implausible, or ungrammatical but receive unexpectedly high acceptability judgments by humans. We looked at three illusions: the comparative illusion (e.g. ""More people have been to Russia than I have""), the depth-charge illusion (e.g. ""No head injury is too trivial to be ignored""), and the negative polarity item (NPI) illusion (e.g. ""The hunter who no villager believed to be trustworthy will ever shoot a bear""). We found that probabilities represented by LMs were more likely to align with human judgments of being ""tricked"" by the NPI illusion which examines a structural dependency, compared to the comparative and the depth-charge illusions which require sophisticated semantic understanding. No single LM or metric yielded results that are entirely consistent with human behavior. Ultimately, we show that LMs are limited both in their construal as cognitive models of human language processing and in their capacity to recognize nuanced but critical information in complicated language materials.",,"['Introduction', 'Related work', ""LMs' linguistic abilities"", 'Language illusions', 'Methods', 'Models and Measures', 'Evaluation procedure', 'Comparative illusion', 'Acceptability differentiation', 'Illusion effect', 'Sensitivity to manipulations', 'Depth-charge illusion', 'Acceptability differentiation', 'Illusion effect', 'Sensitivity to manipulations', 'NPI illusion', 'Acceptability differentiation', 'Illusion effect', 'Sensitivity to variations', 'Discussion', 'Illusion effect', 'Human-like behaviors & Potential processing mechanisms', ""Language models' performance in general"", 'Perplexity & Surprisal', 'Limitations', 'Conclusion', 'Acknowledgements', 'Illusion type', 'Relative No', 'Relative Never']","language model ( lm ) argued overlap substantially human being grammaticality judgment task . human systematically make error language processing , expect lm behave like cognitive model language mimic human behavior ? answer question investigating lm ' subtle judgment associated `` language illusion '' -sentences vague meaning , implausible , ungrammatical receive unexpectedly high acceptability judgment human . looked three illusion : comparative illusion ( e.g . `` people russia '' ) , depth-charge illusion ( e.g . `` head injury trivial ignored '' ) , negative polarity item ( npi ) illusion ( e.g . `` hunter villager believed trustworthy ever shoot bear '' ) . found probability represented lm likely align human judgment `` tricked '' npi illusion examines structural dependency , compared comparative depth-charge illusion require sophisticated semantic understanding . single lm metric yielded result entirely consistent human behavior . ultimately , show lm limited construal cognitive model human language processing capacity recognize nuanced critical information complicated language material ."
A Minimal Approach for Natural Language Action Space in Text-based Games,Kelvin Dongwon;  Ryu; Meng Fang; Gholamreza Haffari; Shirui Pan; Ehsan Shareghi,"Text-based games (TGs) are language-based interactive environments for reinforcement learning. While language models (LMs) and knowledge graphs (KGs) are commonly used for handling large action space in TGs, it is unclear whether these techniques are necessary or overused. In this paper, we revisit the challenge of exploring the action space in TGs and propose ϵ-admissible exploration, a minimal approach of utilizing admissible actions, for training phase. Additionally, we present a textbased actor-critic (TAC) agent that produces textual commands for game, solely from game observations, without requiring any KG or LM. Our method, on average across 10 games from Jericho, outperforms strong baselines and stateof-the-art agents that use LM and KG. Our approach highlights that a much lighter model design, with a fresh perspective on utilizing the information within the environments, suffices for an effective exploration of exponentially large action spaces. 1  ",,"['Introduction', 'Basic Definitions', 'Related Work on TG Agents in RL', 'Experiments', 'Main Results', 'Ablation', 'Qualitative Analysis', 'Discussion', 'Conclusion', 'Appendices', 'A Hyperparameters', 'B Parameter Size for ZORK1', 'C Training Time', 'D Details of Actor and Critic Components', 'F Qualitative Analysis', 'G Full Experimental Results', 'H Stronger Supervised Signals for ZORK1', 'I Adaptive Score-based ϵ', 'J Limitations', 'K Ethical Considerations']","text-based game ( tgs ) language-based interactive environment reinforcement learning . language model ( lm ) knowledge graph ( kg ) commonly used handling large action space tgs , unclear whether technique necessary overused . paper , revisit challenge exploring action space tgs propose ϵ-admissible exploration , minimal approach utilizing admissible action , training phase . additionally , present textbased actor-critic ( tac ) agent produce textual command game , solely game observation , without requiring kg lm . method , average across 10 game jericho , outperforms strong baseline stateof-the-art agent use lm kg . approach highlight much lighter model design , fresh perspective utilizing information within environment , suffices effective exploration exponentially large action space . 1"
ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind,Xiaomeng Ma; Lingyu Gao; Qihui Xu,"Theory of Mind (ToM), the capacity to comprehend the mental states of distinct individuals, is essential for numerous practical applications. With the development of large language models (LLMs), there is a heated debate about whether they are able to perform ToM tasks. Previous studies have used different tasks and prompts to test the ToM on LLMs and the results are inconsistent: some studies asserted that these models are capable of exhibiting ToM, while others suggested the opposite. In this study, we present TOMCHALLENGES, a dataset for comprehensively evaluating the Theory of Mind based on the Sally-Anne and Smarties tests with a diverse set of tasks. In addition, we also propose an auto-grader to streamline the answer evaluation process. We tested three models: davinci, turbo, and gpt-4. Our evaluation results and error analyses show that LLMs have inconsistent behaviors across prompts and tasks. Performing the ToM tasks robustly remains a challenge for the LLMs. In addition, our paper wants to raise awareness in evaluating the ToM in LLMs and we want to invite more discussion on how to design the prompts and tasks for ToM tasks that can better assess the LLMs' ability. 1",,"['Introduction', 'Validity issues of current neural ToM tests', 'Related Work', 'TOMCHALLENGES and Tasks', 'Dataset Construction', 'Smarties Test', 'Task Formulation', 'Experimental Setup', 'Answer Evaluation and Auto-grader', 'Results and Analyses', 'Accuracy by Question and Task', 'Accuracy by Narratives', 'Error Analysis', 'Conclusions', 'Reasoning:']","theory mind ( tom ) , capacity comprehend mental state distinct individual , essential numerous practical application . development large language model ( llm ) , heated debate whether able perform tom task . previous study used different task prompt test tom llm result inconsistent : study asserted model capable exhibiting tom , others suggested opposite . study , present tomchallenges , dataset comprehensively evaluating theory mind based sally-anne smarties test diverse set task . addition , also propose auto-grader streamline answer evaluation process . tested three model : davinci , turbo , gpt-4 . evaluation result error analysis show llm inconsistent behavior across prompt task . performing tom task robustly remains challenge llm . addition , paper want raise awareness evaluating tom llm want invite discussion design prompt task tom task better ass llm ' ability . 1"
The Zipfian Challenge: Learning the statistical fingerprint of natural languages,Christian Bentz,"Human languages are often claimed to fundamentally differ from other communication systems. But what is it exactly that unites them as a separate category? This article proposes to approach this problem -here termed the Zipfian Challenge -as a standard classification task. A corpus with textual material from diverse writing systems and languages, as well as other symbolic and non-symbolic systems, is provided. These are subsequently used to train and test binary classification algorithms, assigning labels ""writing"" and ""non-writing"" to character strings of the test sets. The performance is generally high, reaching 98% accuracy for the best algorithms. Human languages emerge to have a statistical fingerprint: large unit inventories, high entropy, and few repetitions of adjacent units. This fingerprint can be used to tease them apart from other symbolic and non-symbolic systems.",,"['Introduction', 'Data', 'Writing', 'TeDDi sample', 'Non-writing', 'Methods', 'Preprocessing', 'Sampling', 'Features', 'Type-token ratio (TTR)', 'Unigram character entropy (H)', 'Entropy rate (h)', 'Repetition rate (R)', 'Training and test sets', 'Logistic regression', 'Support Vector Machines', 'Multilayer Perceptrons (MLP)', 'Results', 'Discussion', 'Why do algorithms perform differently?', 'Why do longer strings yield better results', 'Which is the best feature?', 'How are the results influenced by subcorpora?', 'Conclusions', 'Acknowledgements', 'Appendices']","human language often claimed fundamentally differ communication system . exactly unites separate category ? article proposes approach problem -here termed zipfian challenge -as standard classification task . corpus textual material diverse writing system language , well symbolic non-symbolic system , provided . subsequently used train test binary classification algorithm , assigning label `` writing '' `` non-writing '' character string test set . performance generally high , reaching 98 % accuracy best algorithm . human language emerge statistical fingerprint : large unit inventory , high entropy , repetition adjacent unit . fingerprint used tease apart symbolic non-symbolic system ."
On the Effects of Structural Modeling for Neural Semantic Parsing,Xiang Zhang; Shizhu He; Kang Liu; Jun Zhao; Mohammad Kaiser; Clemens Bavarian; Philippe Winter; Felipe Petroski Tillet; Dave Such; Matthias Cum- Mings; Fotios Plappert; Eliza- Beth Chantzis; Ariel Barnes; William Hebgen Herbert-Voss; Alex Guss; Alex Nichol; Nikolas Paino; Jie Tezak; Igor Tang; Suchir Babuschkin; Shantanu Balaji; William Jain; Christopher Saunders; Andrew N Hesse; Jan Carr; Josh Leike; Vedant Achiam; Evan Misra; Alec Morikawa; Matthew Radford; Miles Knight; Mira Brundage; Katie Murati; Peter Mayer; Bob Welinder; Dario Mcgrew; Sam Amodei; Ilya Mccandlish; Wojciech Sutskever;  2021 Zaremba; David Chiang; Jacob Andreas; Daniel Bauer; Karl Moritz; Bevan Jones; De Gruyter; Mouton Kevin Clark; Minh-Thang Luong; Quoc V Le; Christopher D 2020 Manning;  Electra,"Semantic parsing aims to map natural language sentences to predefined formal languages, such as logic forms and programming languages, as the semantic annotation. From the theoretic views of linguistic and programming language, structures play an important role in both languages, which had motivated semantic parsers since the task was proposed in the beginning. But in the neural era, semantic parsers treating both natural and formal language as sequences, such as Seq2Seq and LLMs, have got more attentions. On the other side, lots of neural progress have been made for grammar induction, which only focuses on natural languages. Although closely related in the sense of structural modeling, these techniques hadn't been jointly analyzed on the semantic parsing testbeds. To gain the better understanding on structures for semantic parsing, we design a taxonomy of structural modeling methods, and evaluate some representative techniques on semantic parsing, including both compositional and i.i.d. generalizations. In addition to the previous opinion that structures will help in general, we find that (1) structures must be designed for the specific dataset and generalization level, and ( 2) what really matters is not the structure choice of either source or target side, but the choice combination of both sides. Based on the finding, we further propose a metric that can evaluate the structure choice, which we believe can boost the automation of grammar designs for specific datasets and domains.",,"['Introduction', 'Datasets', 'Problem Formalization', 'Selected Structural Models', 'S Model', 'Absent', 'Evaluation Method', 'Results Analysis', 'Lateral Structural Modeling', 'Combinations of Source and Target', 'Latent Source Structures', 'Differences between Accuracies', 'Discussions', 'Metric for Structural Evaluation', 'Related Works', 'Conclusion', 'Limitations', 'Ethics Statement', 'A Structure Modeling', 'A.1 Encoders', 'A.2 Decoders', 'D Accuracies for Model Combinations', 'E EBNF Grammar for SQL', 'Acknowledgements', '', 'B Experiment Hyperparameters', 'C Few-shot Parsing with LLMs', 'F EBNF Grammar for COGS', 'G EBNF Grammar for Lispress']","semantic parsing aim map natural language sentence predefined formal language , logic form programming language , semantic annotation . theoretic view linguistic programming language , structure play important role language , motivated semantic parser since task proposed beginning . neural era , semantic parser treating natural formal language sequence , seq2seq llm , got attention . side , lot neural progress made grammar induction , focus natural language . although closely related sense structural modeling , technique n't jointly analyzed semantic parsing testbeds . gain better understanding structure semantic parsing , design taxonomy structural modeling method , evaluate representative technique semantic parsing , including compositional i.i.d . generalization . addition previous opinion structure help general , find ( 1 ) structure must designed specific dataset generalization level , ( 2 ) really matter structure choice either source target side , choice combination side . based finding , propose metric evaluate structure choice , believe boost automation grammar design specific datasets domain ."
Humans and language models diverge when predicting repeating text,Aditya R Vaidya; Javier Turek; Alexander G Huth; U T Austin,"Language models that are trained on the nextword prediction task have been shown to accurately model human behavior in word prediction and reading speed. In contrast with these findings, we present a scenario in which the performance of humans and LMs diverges. We collected a dataset of human next-word predictions for five stimuli that are formed by repeating spans of text. Human and GPT-2 LM predictions are strongly aligned in the first presentation of a text span, but their performance quickly diverges when memory (or in-context learning) begins to play a role. We traced the cause of this divergence to specific attention heads in a middle layer. Adding a power-law recency bias to these attention heads yielded a model that performs much more similarly to humans. We hope that this scenario will spur future work in bringing LMs closer to human behavior. 1  ",,"['Introduction', 'Related works', 'Human behavioral study', 'Setup for humans', 'Setup for language models', 'Behavioral study results', 'Patterns in model attention', 'Attention optimization', 'Optimization results', 'Conclusions', 'A Stimuli', 'B Additional GPT-2 experiments', 'B.1 Results']","language model trained nextword prediction task shown accurately model human behavior word prediction reading speed . contrast finding , present scenario performance human lm diverges . collected dataset human next-word prediction five stimulus formed repeating span text . human gpt-2 lm prediction strongly aligned first presentation text span , performance quickly diverges memory ( in-context learning ) begin play role . traced cause divergence specific attention head middle layer . adding power-law recency bias attention head yielded model performs much similarly human . hope scenario spur future work bringing lm closer human behavior . 1"
Investigating the Nature of Disagreements on Mid-Scale Ratings: A Case Study on the Abstractness-Concreteness Continuum,Urban Knupleš; Diego Frassinelli; Sabine Schulte Im Walde,"Humans tend to strongly agree on ratings on a scale for extreme cases (e.g., a CAT is judged as very concrete), but judgements on mid-scale words exhibit more disagreement. Yet, collected rating norms are heavily exploited across disciplines. Our study focuses on concreteness ratings and (i) implements correlations and supervised classification to identify salient multimodal characteristics of mid-scale words, and (ii) applies a hard clustering to identify patterns of systematic disagreement across raters. Our results suggest to either fine-tune or filter midscale target words before utilising them.",,"['Motivation', 'Related Work', 'Concreteness Targets and Ratings', 'Target Words: Characteristics', 'Characteristics and Resources', 'Word Classes and Resource Coverage', 'Holistic Perspective', 'Mid-Scale Peculiarities', 'Mid-Scale Disagreement Patterns', 'Discussion & Conclusion', 'Limitations', 'A Dominance of Perception across Targets', 'D Mid-Scale Definitions, Ranges and Classifications across Word Classes', 'Acknowledgements', 'Ethics Statement']","human tend strongly agree rating scale extreme case ( e.g. , cat judged concrete ) , judgement mid-scale word exhibit disagreement . yet , collected rating norm heavily exploited across discipline . study focus concreteness rating ( ) implement correlation supervised classification identify salient multimodal characteristic mid-scale word , ( ii ) applies hard clustering identify pattern systematic disagreement across raters . result suggest either fine-tune filter midscale target word utilising ."
ArchBERT: Bi-Modal Understanding of Neural Architectures and Natural Languages,Mohammad Akbari; Saeed Ranjbar Alvar; Behnam Kamranian; Amin Banitalebi-Dehkordi; Yong Zhang,"Building multi-modal language models has been a trend in the recent years, where additional modalities such as image, video, speech, etc. are jointly learned along with natural languages (i.e., textual information). Despite the success of these multi-modal language models with different modalities, there is no existing solution for neural network architectures and natural languages. Providing neural architectural information as a new modality allows us to provide fast architecture-2-text and text-2-architecture retrieval/generation services on the cloud with a single inference. Such solution is valuable in terms of helping beginner and intermediate ML users to come up with better neural architectures or AutoML approaches with a simple text query. In this paper, we propose ArchBERT, a bi-modal model for joint learning and understanding of neural architectures and natural languages, which opens up new avenues for research in this area. We also introduce a pre-training strategy named Masked Architecture Modeling (MAM) for a more generalized joint learning. Moreover, we introduce and publicly release two new bi-modal datasets for training and validating our methods. The ArchBERT's performance is verified through a set of numerical experiments on different downstream tasks such as architecture-oriented reasoning, question answering, and captioning (summarization). Datasets, codes, and demos are available as supplementary materials 1 .",,"['Introduction', 'Related Works', 'Proposed Method: ArchBERT', 'Masked Architecture Modeling (MAM)', 'Architectural Question Answering (AQA)', 'Language Decoder', 'Datasets', 'TVHF', 'AutoNet', 'AutoNet-AQA', 'Experimental Results', 'Uni-Modal Baselines', 'Architectural Reasoning (AR)', 'As reported in', 'Architecture Clone Detection (ACD)', 'Architectural Question Answering (AQA)', 'Architecture Captioning (AC)', 'Architecture Search (AS)', 'Qualitative Results', 'Ablation Study', 'Embeddings Visualization', 'Conclusion', 'A.3 Embeddings Visualization', 'A.4 Data Generation', 'Algorithm 1 TVHF dataset generator', 'Input:', 'A.5 Distribution Plots for TVHF and AutoNet', 'A.6 Sample Data from TVHF and AutoNet', 'A.7 Dataset Quality Analysis', 'A.7.1 Reliability and Completeness', 'A.7.2 Label/Feature Noise', 'A.7.3 Feature Representation', 'A Appendix', ""A.2 ArchBERT's Performance on OOD Data""]","building multi-modal language model trend recent year , additional modality image , video , speech , etc . jointly learned along natural language ( i.e. , textual information ) . despite success multi-modal language model different modality , existing solution neural network architecture natural language . providing neural architectural information new modality allows u provide fast architecture-2-text text-2-architecture retrieval/generation service cloud single inference . solution valuable term helping beginner intermediate ml user come better neural architecture automl approach simple text query . paper , propose archbert , bi-modal model joint learning understanding neural architecture natural language , open new avenue research area . also introduce pre-training strategy named masked architecture modeling ( mam ) generalized joint learning . moreover , introduce publicly release two new bi-modal datasets training validating method . archbert 's performance verified set numerical experiment different downstream task architecture-oriented reasoning , question answering , captioning ( summarization ) . datasets , code , demo available supplementary material 1 ."
"A Comparative Study on Textual Saliency of Styles from Eye Tracking, Annotations, and Language Models",Karin De Langis; Dongyeop Kang,"There is growing interest in incorporating eyetracking data and other implicit measures of human language processing into natural language processing (NLP) pipelines. The data from human language processing contain unique insight into human linguistic understanding that could be exploited by language models. However, many unanswered questions remain about the nature of this data and how it can best be utilized in downstream NLP tasks. In this paper, we present eyeStyliency, an eye-tracking dataset for human processing of stylistic text (e.g., politeness). We develop a variety of methods to derive style saliency scores over text using the collected eye dataset. We further investigate how this saliency data compares to both human annotation methods and model-based interpretability metrics. We find that while eyetracking data is unique, it also intersects with both human annotations and model-based importance scores, providing a possible bridge between human-and machine-based perspectives. We propose utilizing this type of data to evaluate the cognitive plausibility of models that interpret style. Our eye-tracking data and processing code are publicly available. 1  ",,"['Introduction', 'Related Work', 'eyeStyliency: A Dataset of Eye Movement for Textual Saliency', 'Data Setups', 'Eye-Tracking Measures', 'Experimental Procedure', 'Congruent Setup', 'Incongruent Setup Context', 'Stimuli', 'Eye-based saliency', 'Pre-processing Eye Tracking Data', 'Calculating Saliency Scores', 'Comparison with Other Saliency Metrics', 'Qualitative Results', '""Eye-in-the-loop"" few-shot learning', 'Key Findings and Discussion', 'Limitations', 'Acknowledgements', 'A Appendix', 'A.1 Experimental Materials', 'A.2 Mixed Effect Modeling', 'A.3 Additional Saliency Comparisons', 'A.3.1 Saliency Scores', 'A.4 Few-Shot Learning Experiment Details and Results']","growing interest incorporating eyetracking data implicit measure human language processing natural language processing ( nlp ) pipeline . data human language processing contain unique insight human linguistic understanding could exploited language model . however , many unanswered question remain nature data best utilized downstream nlp task . paper , present eyestyliency , eye-tracking dataset human processing stylistic text ( e.g. , politeness ) . develop variety method derive style saliency score text using collected eye dataset . investigate saliency data compare human annotation method model-based interpretability metric . find eyetracking data unique , also intersects human annotation model-based importance score , providing possible bridge human-and machine-based perspective . propose utilizing type data evaluate cognitive plausibility model interpret style . eye-tracking data processing code publicly available . 1"
PROPRES: Investigating the Projectivity of Presupposition with Various Triggers and Environments,Daiki Asami; Saku Sugawara,"What makes a presupposition of an utteranceinformation taken for granted by its speakerdifferent from other pragmatic inferences such as an entailment is projectivity (e.g., the negative sentence the boy did not stop shedding tears presupposes the boy had shed tears before). The projectivity may vary depending on the combination of presupposition triggers and environments. However, prior natural language understanding studies fail to take it into account as they either use no human baseline or include only negation as an entailment-canceling environment to evaluate models' performance. The current study attempts to reconcile these issues. We introduce a new dataset, projectivity of presupposition (PROPRES), which includes 12k premise-hypothesis pairs crossing six triggers involving some lexical variety with five environments. Our human evaluation reveals that humans exhibit variable projectivity in some cases. However, the model evaluation shows that the best-performed model, DeBERTa, does not fully capture it. Our findings suggest that probing studies on pragmatic inferences should take extra care of the human judgment variability and the combination of linguistic items.",,"['Introduction', 'Background', 'Presupposition in Linguistics', 'Presupposition in NLI', 'Setup', 'Results and Discussion', 'Experiment 2: PROPRES', 'Data Generation', 'Setup', 'Model Evaluation', 'Results and Discussion', 'Conclusion', 'A Limitations', 'B Templates', 'C Crowdsourcing Human Evaluation', 'D Triggers and Environments in IMPPRES', 'E Results without Exclusion', 'Acknowledgments']","make presupposition utteranceinformation taken granted speakerdifferent pragmatic inference entailment projectivity ( e.g. , negative sentence boy stop shedding tear presupposes boy shed tear ) . projectivity may vary depending combination presupposition trigger environment . however , prior natural language understanding study fail take account either use human baseline include negation entailment-canceling environment evaluate model ' performance . current study attempt reconcile issue . introduce new dataset , projectivity presupposition ( propres ) , includes 12k premise-hypothesis pair crossing six trigger involving lexical variety five environment . human evaluation reveals human exhibit variable projectivity case . however , model evaluation show best-performed model , deberta , fully capture . finding suggest probing study pragmatic inference take extra care human judgment variability combination linguistic item ."
IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions,Zhebin Zhang; Xinyu Zhang; Yuanhang Ren; Saijiang Shi; Meng Han; Yongkang Wu; Ruofei Lai; Zhao Cao,"Retrieval-Augmented Generation (RAG), by incorporating external knowledge with parametric memory of language models, has become the state-of-the-art architecture for opendomain QA tasks. However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions. In this paper, we propose an Induction-Augmented Generation (IAG) framework that utilizes inductive knowledge along with the retrieved documents for implicit reasoning. We leverage large language models (LLMs) for deriving such knowledge via a novel prompting method based on inductive reasoning patterns. On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student, respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for answer prediction, while IAG-Student gets rid of dependencies on GPT service at inference time by incorporating a student inductor model. The inductor is firstly trained via knowledge distillation and further optimized by back-propagating the generator feedback via differentiable beam scores. Experimental results show that IAG outperforms RAG baselines as well as ChatGPT on two Open-Domain QA tasks. Notably, our best models have won the first place in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA (since Jan 8, 2023).",,"['Introduction', 'Related Work', 'Overview', 'Knowledge Elicitation via Inductive Prompting', 'IAG Implementations', 'IAG-GPT', 'IAG-Student', 'Experimental Setup', 'Datasets', 'Models', 'Results', 'Main Results', 'Prompting Methods', 'Optimization of Inductor', 'Distillation Strategies', 'TAILBACK', 'Knowledge Fusion Mechanism', 'Knowledge Fusion v.s. Self-Consistency', 'Number of Knowledge Statements', 'Conclusion', 'Limitations', 'A Prompting Template', 'B Additional Experimental Results', 'B.1 Comparison between Information Retrieval and knowledge Induction', 'B.2 Comparison among Prompting Methods']","retrieval-augmented generation ( rag ) , incorporating external knowledge parametric memory language model , become state-of-the-art architecture opendomain qa task . however , common knowledge base inherently constrained limited coverage noisy information , making retrieval-based approach inadequate answer implicit reasoning question . paper , propose induction-augmented generation ( iag ) framework utilizes inductive knowledge along retrieved document implicit reasoning . leverage large language model ( llm ) deriving knowledge via novel prompting method based inductive reasoning pattern . top , implement two version iag named iag-gpt iag-student , respectively . iag-gpt directly utilizes knowledge generated gpt-3 answer prediction , iag-student get rid dependency gpt service inference time incorporating student inductor model . inductor firstly trained via knowledge distillation optimized back-propagating generator feedback via differentiable beam score . experimental result show iag outperforms rag baseline well chatgpt two open-domain qa task . notably , best model first place official leaderboards csqa2.0 ( since nov 1 , 2022 ) strategyqa ( since jan 8 , 2023 ) ."
Evaluating and Modeling Attribution for Cross-Lingual Question Answering,Benjamin Muller; John Wieting; Jonathan H Clark; Tom Kwiatkowski; Sebastian Ruder; Livio Baldini Soares; Roee Aharoni; Jonathan Herzig; Xinyi Wang; Inria Paris; Google Deepmind; Google Research,"Trustworthy answer content is abundant in many high-resource languages and is instantly accessible through question answering systems-yet this content can be hard to access for those that do not speak these languages. The leap forward in cross-lingual modeling quality offered by generative language models offers much promise, yet their raw generations often fall short in factuality. To improve trustworthiness in these systems, a promising direction is to attribute the answer to a retrieved source, possibly in a content-rich language different from the query. Our work is the first to study attribution for cross-lingual question answering. First, we introduce the XOR-AttriQA dataset to assess the attribution level of a state-of-theart cross-lingual question answering (QA) system in 5 languages. To our surprise, we find that a substantial portion of the answers is not attributable to any retrieved passages (up to 47% of answers exactly matching a gold reference) despite the system being able to attend directly to the retrieved text. Second, to address this poor attribution level, we experiment with a wide range of attribution detection techniques. We find that Natural Language Inference models and PaLM 2 fine-tuned on a very small amount of attribution data can accurately detect attribution. With these models, we improve the attribution level of a cross-lingual QA system. Overall, we show that current academic generative cross-lingual QA systems have substantial shortcomings in attribution and we build tooling to mitigate these issues. 1 1 The XOR-AttriQA dataset is available at https: //github.com/google-research/google-research/ tree/master/xor_attriqa. XOR-AttriQA includes approximately 10,000 annotated examples to foster research in the modeling and evaluation of attribution in cross-lingual settings. † Correspondence to {jwieting,jhclark}@google.com. ♠ Work done as an intern at Google Research.",,"['Introduction', 'Attribution of Generative Language Models', 'Attributed Question Answering', '2.3', 'Cross-Lingual QA Attribution Evaluation', 'The XOR-AttriQA Dataset', ""Raters' Demographic and Cultural Background"", 'Attribution Evaluation of CORA', 'Lack of Attribution of XORQA Predictions', ""Analysis of CORA's Attribution Level"", 'Attribution Detection for XORQA', 'Attribution Detection Models', 'STRING-MATCH', 'Results', 'NLI Model for Reranking', 'Discussion and Future Directions', 'Conclusion', 'Limitations', 'Contributions', 'General', 'Acknowledgements', '', 'A.1 Codebase', 'A.2 XOR-TyDiQA', 'A.3 Languages Distribution of MDPR', 'B Data Collection', 'B.2 AIS Score', 'C Examples of Attribution without Exact-Match', 'Query:']","trustworthy answer content abundant many high-resource language instantly accessible question answering systems-yet content hard access speak language . leap forward cross-lingual modeling quality offered generative language model offer much promise , yet raw generation often fall short factuality . improve trustworthiness system , promising direction attribute answer retrieved source , possibly content-rich language different query . work first study attribution cross-lingual question answering . first , introduce xor-attriqa dataset ass attribution level state-of-theart cross-lingual question answering ( qa ) system 5 language . surprise , find substantial portion answer attributable retrieved passage ( 47 % answer exactly matching gold reference ) despite system able attend directly retrieved text . second , address poor attribution level , experiment wide range attribution detection technique . find natural language inference model palm 2 fine-tuned small amount attribution data accurately detect attribution . model , improve attribution level cross-lingual qa system . overall , show current academic generative cross-lingual qa system substantial shortcoming attribution build tooling mitigate issue . 1 1 xor-attriqa dataset available http : //github.com/google-research/google-research/ tree/master/xor_attriqa . xor-attriqa includes approximately 10,000 annotated example foster research modeling evaluation attribution cross-lingual setting . † correspondence { jwieting , jhclark } @ google.com . ♠ work done intern google research ."
Absolute Position Embedding Learns Sinusoid-like Waves for Attention Based on Relative Position,Yuji Yamamoto; Takuya Matsuzaki,"Attention weight is a clue to interpret how a Transformer-based model makes an inference. In some attention heads, the attention focuses on the neighbors of each token. This allows the output vector of each token to depend on the surrounding tokens and contributes to make the inference context-dependent. We analyze the mechanism behind the concentration of attention on nearby tokens. We show that the phenomenon emerges as follows: (1) learned position embedding has sinusoid-like components, (2) such components are transmitted to the query and the key in the selfattention, (3) the attention head shifts the phases of the sinusoid-like components so that the attention concentrates on nearby tokens at specific relative positions. In other words, a certain type of Transformer-based model acquires the sinusoidal positional encoding to some extent on its own through Masked Language Modeling.",,"['Introduction', 'Background', 'Multi-Head Self-Attention', 'Position Embedding', 'Relative Position Dependence of Attention', 'Attention to Nearby Tokens', 'Learned Representation of Positions', 'APE Includes Sinusoid-like Waves', 'Dimensionality of Positional Representation', 'Positional Representation in', 'Rethinking About Query and Key', 'Spectral Analysis of Query and Key', 'Attention Based on Relative Position is due to the Phase Shift', 'Phase Shift Width is the Same even if', 'Attention to the Adjacent Tokens', 'Remark on the Learning Process of Position Embeddings', 'Related Works', 'Conclusion', 'Limitations', 'A Cumulative Principal Component', 'B Amplitude Spectra of Various Models', 'C Comparing Different Architectures', 'Figures for GPT-', 'Position embedding The position embeddings', 'E.2 Results', 'D Theorems About the Phase Shift', 'E How the Relative Position Dependence of Attention Emerges']","attention weight clue interpret transformer-based model make inference . attention head , attention focus neighbor token . allows output vector token depend surrounding token contributes make inference context-dependent . analyze mechanism behind concentration attention nearby token . show phenomenon emerges follows : ( 1 ) learned position embedding sinusoid-like component , ( 2 ) component transmitted query key selfattention , ( 3 ) attention head shift phase sinusoid-like component attention concentrate nearby token specific relative position . word , certain type transformer-based model acquires sinusoidal positional encoding extent masked language modeling ."
Chinese Lexical Substitution: Dataset and Method,Jipeng Qiang; Kang Liu; Ying Li; Yun Li; Yi Zhu; Yunhao Yuan; Xiaocheng Hu; Xiaoye Ouyang,"Existing lexical substitution (LS) benchmarks were collected by asking human annotators to think of substitutes from memory, resulting in benchmarks with limited coverage and relatively small scales. To overcome this problem, we propose a novel annotation method to construct an LS dataset based on human and machine collaboration. Based on our annotation method, we construct the first Chinese LS dataset CHNLS which consists of 33,695 instances and 144,708 substitutes, covering three text genres (News, Novel, and Wikipedia). Specifically, we first combine four unsupervised LS methods as an ensemble method to generate the candidate substitutes, and then let human annotators judge these candidates or add new ones. This collaborative process combines the diversity of machine-generated substitutes with the expertise of human annotators. Experimental results that the ensemble method outperforms other LS methods. To our best knowledge, this is the first study for the Chinese LS task.",,"['Introduction', 'Related Work', 'Creating CHNLS', 'Data Preparation', 'Machine-generated Substitution', 'Manual Annotation', 'Experiments', 'Experimental Setup', 'Evaluation Results', 'Qualitative evaluation', 'Conclusions', 'Limitations', 'Ethics Statement', 'A.1 Selection of target words', 'A.2 Annotation Website', 'A.3 Annotation Manual', 'A.4 The Work of Annotators', 'B More Examples', 'Acknowledgement']","existing lexical substitution ( l ) benchmark collected asking human annotator think substitute memory , resulting benchmark limited coverage relatively small scale . overcome problem , propose novel annotation method construct l dataset based human machine collaboration . based annotation method , construct first chinese l dataset chnls consists 33,695 instance 144,708 substitute , covering three text genre ( news , novel , wikipedia ) . specifically , first combine four unsupervised l method ensemble method generate candidate substitute , let human annotator judge candidate add new one . collaborative process combine diversity machine-generated substitute expertise human annotator . experimental result ensemble method outperforms l method . best knowledge , first study chinese l task ."
Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting,Chenkai Sun; Jinning Li; Yi R Fung; Hou Pong Chan; Tarek Abdelzaher; Chengxiang Zhai; Heng Ji,"Automatic response forecasting for news media plays a crucial role in enabling content producers to efficiently predict the impact of news releases and prevent unexpected negative outcomes such as social conflict and moral injury. To effectively forecast responses, it is essential to develop measures that leverage the social dynamics and contextual information surrounding individuals, especially in cases where explicit profiles or historical actions of the users are limited (referred to as lurkers). As shown in a previous study, 97% of all tweets are produced by only the most active 25% of users. However, existing approaches have limited exploration of how to best process and utilize these important features. To address this gap, we propose a novel framework, named SOCIALSENSE, that leverages a large language model to induce a belief-centered graph on top of an existent social network, along with graph-based propagation to capture social dynamics. We hypothesize that the induced graph that bridges the gap between distant users who share similar beliefs allows the model to effectively capture the response patterns. Our method surpasses existing state-of-the-art in experimental evaluations for both zero-shot and supervised settings, demonstrating its effectiveness in response forecasting. Moreover, the analysis reveals the framework's capability to effectively handle unseen user and lurker scenarios, further highlighting its robustness and practical applicability.",,"['Introduction', 'Task Formulation', 'SOCIALSENSE', 'Attributes Operator', 'LLM-based Propagation', 'Network Construction', 'Unmasking Latent Persona with Large Language Model', 'Belief-Augmented Social Network', 'Information Propagation', 'Zero-Shot Prediction by Simulating Propagation with Social Prompts', 'Experiment', 'Data Construction', 'Experimental Setup', 'Results Discussion', 'Ablation Study', 'Zero-Shot Evaluation', 'Evaluation on Lurker and Unseen User Scenarios', 'Related Work', 'Limitations', 'Ethics Statements', 'A.2 Analysis of Belief Data', 'A.3 Prompts Templates', 'Acknowledgement', 'A Appendix', 'A.1 Implementation Details']","automatic response forecasting news medium play crucial role enabling content producer efficiently predict impact news release prevent unexpected negative outcome social conflict moral injury . effectively forecast response , essential develop measure leverage social dynamic contextual information surrounding individual , especially case explicit profile historical action user limited ( referred lurker ) . shown previous study , 97 % tweet produced active 25 % user . however , existing approach limited exploration best process utilize important feature . address gap , propose novel framework , named socialsense , leverage large language model induce belief-centered graph top existent social network , along graph-based propagation capture social dynamic . hypothesize induced graph bridge gap distant user share similar belief allows model effectively capture response pattern . method surpasses existing state-of-the-art experimental evaluation zero-shot supervised setting , demonstrating effectiveness response forecasting . moreover , analysis reveals framework 's capability effectively handle unseen user lurker scenario , highlighting robustness practical applicability ."
Fine-grained Conversational Decoding via Isotropic and Proximal Search,Yuxuan Yao; Han Wu; Qiling Xu; Linqi Song,"General-purpose text decoding approaches are usually adopted for dialogue response generation. Although the quality of the generated responses can be improved with dialogue-specific encoding methods, conversational decoding methods are still under-explored. Inspired by Wu et al. ( 2023) that a good dialogue feature space should follow the rules of locality and isotropy, we present a fine-grained conversational decoding method, termed isotropic and proximal search (IPS). Our method is designed to generate the semantic-concentrated response, while still maintaining informativeness and discrimination against the context. Experiments show that our approach outperforms existing decoding strategies in the dialogue field across both automatic and human evaluation metrics. More in-depth analyses further confirm the effectiveness of our approach.",,"['Introduction', 'Methodology', 'Preliminary', 'Isotropic and Proximal Search', 'Experiments', 'Results and Discussion', 'Conclusion', 'Ackonwledgements', 'Limitations', 'Ethics Statement', 'A.1.2 Informativeness', 'A.1.3 Coherence', 'A.1.4 Semantic Coverage', 'A.2 More Details of the Task', 'A.2.1 Evaluation of G-EVAL Score', 'A.2.2 More Experimental Results', 'A.3 Surface-level Analysis', 'A.3.1 Score Distribution According to the Length of the Previous Context', 'A.3.2 Utterance Length Analysis', 'A.4.1 Instances Illustration', 'A.5 Cosine Similarity Heatmap', 'A.6 Examples of Generated Texts', '', 'A Appendix', 'A.1 Human Evaluation Instructions', 'A.1.1 Fluency']","general-purpose text decoding approach usually adopted dialogue response generation . although quality generated response improved dialogue-specific encoding method , conversational decoding method still under-explored . inspired wu et al . ( 2023 ) good dialogue feature space follow rule locality isotropy , present fine-grained conversational decoding method , termed isotropic proximal search ( ip ) . method designed generate semantic-concentrated response , still maintaining informativeness discrimination context . experiment show approach outperforms existing decoding strategy dialogue field across automatic human evaluation metric . in-depth analysis confirm effectiveness approach ."
Holistic Inter-Annotator Agreement and Corpus Coherence Estimation in a Large-scale Multilingual Annotation Campaign,Nicolas Stefanovitch; Jakub Piskorski,"In this paper we report on the complexity of persuasion technique annotation in the context of a large multilingual annotation campaign involving 6 languages and approximately 40 annotators. We highlight the techniques that appear to be difficult for humans to annotate and elaborate on our findings on the causes of this phenomenon. We introduce Holistic IAA, a new word embedding-based annotator agreement metric and we report on various experiments using this metric and its correlation with the traditional Inter Annotator Agreement (IAA) metrics. However, given somewhat limited and loose interaction between annotators, i.e., only a few annotators annotate the same document subsets, we try to devise a way to assess the coherence of the entire dataset and strive to find a good proxy for IAA between annotators tasked to annotate different documents and in different languages, for which classical IAA metrics can not be applied.",,"['Introduction', 'Related Work', 'Persuasion Technique Annotation', 'Taxonomy', 'Annotation Process', 'Annotation Coherence & Complexity', 'Traditional IAA', 'Confusion matrix', ""Techniques' Annotation Complexity"", 'Disagreement sources', 'Holistic IAA', 'Validation: Methodology', 'Validation: Results', 'Validation: Error Analysis', 'Impact of the second curation step', 'Multilingual Dataset Coherence Estimation', 'Conclusions', 'Limitations', 'Ethics Statement', 'A Persuasion Techniques', 'B Dataset Statistics', 'C Annotation guidelines excerpt', 'ATTACK ON REPUTATION', 'JUSTIFICATION', 'DISTRACTION', 'SIMPLIFICATION', 'CALL', 'False Dilemma or No Choice:', '(b) Dictatorship', 'Consequential Oversimplification:', 'Slogans:', 'Obfuscation, Intentional Vagueness, Confusion:', 'D Confusion Matrix based on Holistic IAA', 'E Identifying the top and low groups of annotators', 'Acknowledgements', 'F Parameter search']","paper report complexity persuasion technique annotation context large multilingual annotation campaign involving 6 language approximately 40 annotator . highlight technique appear difficult human annotate elaborate finding cause phenomenon . introduce holistic iaa , new word embedding-based annotator agreement metric report various experiment using metric correlation traditional inter annotator agreement ( iaa ) metric . however , given somewhat limited loose interaction annotator , i.e. , annotator annotate document subset , try devise way ass coherence entire dataset strive find good proxy iaa annotator tasked annotate different document different language , classical iaa metric applied ."
PHD: Pixel-Based Language Modeling of Historical Documents,Nadav Borenstein; Phillip Rust; Desmond Elliott; Isabelle Augenstein,"The digitisation of historical documents has provided historians with unprecedented research opportunities. Yet, the conventional approach to analysing historical documents involves converting them from images to text using OCR, a process that overlooks the potential benefits of treating them as images and introduces high levels of noise. To bridge this gap, we take advantage of recent advancements in pixel-based language models trained to reconstruct masked patches of pixels instead of predicting token distributions. Due to the scarcity of real historical scans, we propose a novel method for generating synthetic scans to resemble real historical documents. We then pre-train our model, PHD, on a combination of synthetic scans and real historical newspapers from the 1700-1900 period. Through our experiments, we demonstrate that PHD exhibits high proficiency in reconstructing masked image patches and provide evidence of our model's noteworthy language understanding capabilities. Notably, we successfully apply our model to a historical QA task, highlighting its utility in this domain.",,"['Introduction', 'Background', 'NLP for Historical Texts', 'Pixel-based Models for NLU', 'Model', 'Training a Pixel-Based Historical LM', 'Artificially Generated Pretraining Data', 'Real Historical Scans', 'Pretraining Procedure', 'Pretraining Results', 'Training for Downstream NLU Tasks', 'Language Understanding', 'Historical Question Answering', 'Training Procedure', 'Results', 'Conclusion', 'Limitations', 'B Historical GLUE Baselines', 'C Additional Material', 'Acknowledgements', 'A Reproducibility', 'A.1 Training', 'Runaways Slaves in Britain', 'A.2 Dataset Generation']","digitisation historical document provided historian unprecedented research opportunity . yet , conventional approach analysing historical document involves converting image text using ocr , process overlook potential benefit treating image introduces high level noise . bridge gap , take advantage recent advancement pixel-based language model trained reconstruct masked patch pixel instead predicting token distribution . due scarcity real historical scan , propose novel method generating synthetic scan resemble real historical document . pre-train model , phd , combination synthetic scan real historical newspaper 1700-1900 period . experiment , demonstrate phd exhibit high proficiency reconstructing masked image patch provide evidence model 's noteworthy language understanding capability . notably , successfully apply model historical qa task , highlighting utility domain ."
Primacy Effect of ChatGPT,Yiwei Wang; Yujun Cai; Muhao Chen; Yuxuan Liang; Bryan Hooi; Angeles ‡ Meta,"Instruction-tuned large language models (LLMs), such as ChatGPT, have led to promising zero-shot performance in discriminative natural language understanding (NLU) tasks. This involves querying the LLM using a prompt containing the question, and the candidate labels to choose from. The question-answering capabilities of ChatGPT arise from its pre-training on large amounts of human-written text, as well as its subsequent fine-tuning on human preferences, which motivates us to ask: Does ChatGPT also inherit humans' cognitive biases? In this paper, we study the primacy effect of ChatGPT: the tendency of selecting the labels at earlier positions as the answer. We have two main findings: i) ChatGPT's decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer. We hope that our experiments and analyses provide additional insights into building more reliable ChatGPT-based solutions. We release the source code at https: //github.com/wangywUST/PrimacyEffectGPT.",,"['Introduction', 'Primacy Effect of ChatGPT', 'Prompts for ChatGPT', 'Analysis with Label Shuffling', 'Prediction Comparison on an Instance', 'Statistics of Predicted Indices', 'Experiments', 'Experiment Setup', 'Consistency under Label Shuffling', 'Primacy Effect of ChatGPT', 'Evaluation on Fairness', 'Related Work', 'Conclusion', 'Limitation', 'Acknowledgement']","instruction-tuned large language model ( llm ) , chatgpt , led promising zero-shot performance discriminative natural language understanding ( nlu ) task . involves querying llm using prompt containing question , candidate label choose . question-answering capability chatgpt arise pre-training large amount human-written text , well subsequent fine-tuning human preference , motivates u ask : chatgpt also inherit human ' cognitive bias ? paper , study primacy effect chatgpt : tendency selecting label earlier position answer . two main finding : ) chatgpt 's decision sensitive order label prompt ; ii ) chatgpt clearly higher chance select label earlier position answer . hope experiment analysis provide additional insight building reliable chatgpt-based solution . release source code http : //github.com/wangywust/primacyeffectgpt ."
Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension,Akira Kawabata; Saku Sugawara; Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel Ziegler; Jeffrey Wu; Clemens Winter; Chris Hesse; Mark Chen; Eric Sigler; Ma- Teusz Litwin; Scott Gray; Benjamin Chess; Jack Clark; Christopher Berner; Sam Mccandlish; Alec Radford; Ilya Sutskever; Dario 2020 Amodei; Dallas Card; Peter Henderson; Urvashi Khandelwal; Robin Jia; Kyle Mahowald; Dan 2020 Jurafsky; Wei-Lin Chiang; Zhuohan Li; Zi Lin; Ying Sheng; Zhanghao Wu; Hao Zhang; Lianmin Zheng; Siyuan Zhuang; Yonghao Zhuang; Joseph E Gonzalez; Ion Stoica; Eric P 2023 Xing;  Vicuna; Hyung Won; Le Hou; Shayne Longpre; Barret Zoph; Yi Tay; William Fedus; Yunxuan Li; Xuezhi Wang; Mostafa Dehghani; Siddhartha Brahma; Al- Bert Webson; Shane Shixiang; Zhuyun Gu; Mirac Dai; Xinyun Suzgun; Aakanksha Chen; Alex Chowdh- Ery; Marie Castro-Ros; Kevin Pellat; Dasha Robinson; Sharan Valter; Gaurav Narang; Adams Mishra; Vincent Yu; Yanping Zhao; Andrew Huang; Hongkun Dai; Slav Yu; Ed H Petrov; Jeff Chi; Ja- Cob Dean; Adam Devlin; Denny Roberts; Quoc V Zhou;  Le; Bhavana Dalvi; Peter Jansen; Oyvind Tafjord; Zhengnan Xie; Hannah Smith; Leighanna Pipatanangkura; Matt Gardner; Yoav Artzi; Victoria Basmov; Jonathan Berant; Ben Bogin; Sihao Chen; Pradeep Dasigi; Dheeru Dua; Yanai Elazar; Ananth Gottumukkala; Nitish Gupta; Hannaneh Hajishirzi; Gabriel Ilharco; Daniel Khashabi; Kevin Lin; Jiangming Liu; Nel- Son F Liu; Phoebe Mulcaire; Qiang Ning; Sameer Singh; Noah A Smith; Sanjay Subramanian; Reut Tsarfaty; Eric Wallace; Ally Zhang; Ben Zhou; Long Ouyang; Xu Jiang; Diogo Almeida; Carroll Wainwright; Pamela Mishkin; Chong Zhang; Katarina Slama; Alex Ray; John Schulman; Jacob Hilton; Fraser Kelton; Luke Miller; Maddie Simens; Peter Welinder; Paul F Christiano; Jan Leike; Ryan 2022 Lowe; Danilo Neves Ribeiro; Shen Wang; Xiaofei Ma; Swarnadeep Saha; Peter Hase; Nazneen Rajani; Prateek Yadav; Lisa Bauer; Mohit Bansal;  Explagraphs; Freda Shi; Xinyun Chen; Kanishka Misra; Nathan Scales; David Dohan; Ed H Chi; Nathanael Schärli; Denny Zhou;  Large; Vinh Q Tran; Xavier Garcia; Jason Wei; Won Chung; Dara Bahri; Tal Schuster; Steven Zheng; Neil Houlsby; Donald Metzler;  Ul2; Hugo Touvron; Thibaut Lavril; Gautier Izacard; Xavier Martinet; Marie-Anne Lachaux; Timothée Lacroix; Baptiste Rozière; Naman Goyal; Eric Hambro; Faisal Azhar; Aurelien Rodriguez; Armand Joulin; Louis Martin; Kevin Stone; Peter Al- Bert; Amjad Almahairi; Yasmine Babaei; Nikolay Bashlykov; Soumya Batra; Prajjwal Bhargava; Shruti Bhosale; Dan Bikel; Lukas Blecher; Cristian Canton Ferrer; Moya Chen; Guillem Cucurull; David Esiobu; Jude Fernandes; Jeremy Fu; Wenyin Fu; Brian Fuller; Cynthia Gao; Vedanuj Goswami; An- Thony Hartshorn; Saghar Hosseini; Rui Hou; Hakan Inan; Marcin Kardas; Viktor Kerkez; Madian Khabsa; Isabel Kloumann; Artem Korenev; Singh Koura; Jenya Lee; Di- Ana Liskovich; Yinghai Lu; Yuning Mao; Xavier Mar- Tinet; Todor Mihaylov; Pushkar Mishra; Igor Moly- Bog; Yixin Nie; Andrew Poulton; Jeremy Reizen- Stein; Rashi Rungta; Kalyan Saladi; Alan Schelten; Ruan Silva; Eric Michael Smith; Ranjan Subrama- Nian; Ellen Tan; Binh Tang; Ross Tay- Lor; Adina Williams; Jian Xiang Kuan; Puxin Xu; Zheng Yan; Iliyan Zarov; Yuchen Zhang; Angela Fan; Melanie Kambadur; Sharan Narang; Aurelien Ro- Driguez; Robert Stojnic; Sergey Edunov,"To precisely evaluate a language model's capability for logical reading comprehension, we present a dataset for testing the understanding of the rationale behind critical reasoning. For questions taken from an existing multiplechoice logical reading comprehension dataset, we crowdsource rationale texts that explain why we should select or eliminate answer options, resulting in 3,003 multiple-choice subquestions that are associated with 943 main questions. Experiments on our dataset show that recent large language models (e.g., InstructGPT) struggle to answer the subquestions even if they are able to answer the main questions correctly. We find that the models perform particularly poorly in answering subquestions written for the incorrect options of the main questions, implying that the models have a limited capability for explaining why incorrect alternatives should be eliminated. These results suggest that our dataset encourages further investigation into the critical reasoning ability of language models while focusing on the elimination process of relevant alternatives. Mana Ashida and Saku Sugawara. 2022. Possible stories: Evaluating situated commonsense reasoning under multiple possible scenarios. In",,"['Introduction', 'Passage', 'Main Question', 'Sub Question (on the rationale of eliminating Option A)', '🤖 🧔', 'Related Works', 'Option D', 'RationaleD', 'RULE Data Collection', 'Design Choices', 'Rationale Collection', 'Collecting Rationales', 'Rationale Writing', 'Rationale Validation', 'Subquestion Construction', 'Human Validation', 'Dataset Statistics', 'Baseline Performance on RULE', 'Models and Settings', 'Few-and', 'Results', 'Analysis', 'Conclusion', 'Ethical Consideration', 'Limitations', 'B Question Generation Prompt', 'C Crowdsourcing Details', 'D Chain-of-Thought Prompt', 'E Test Split Setting', 'F MainQ-wise SubQ Results of InstructGPT', 'G Complementary Few-Shot and Zero-Shot Results', 'H Main Results of the Subquestions for the Correctly-Answered Main Questions', 'I Relationship between Question and Option Length and Model Performance', 'J Rationale Alignment Task', 'K Reasoning Type Annotation', 'L ReClor Reasoning Types and Subquestion Accuracy', 'M Context-Ablation Analysis', 'N Similarity of Rationale with MainQ Option', 'Rationale1:', 'Question:', 'Rationale0:', 'Test Instance', 'Question:', 'Rationale2:', 'Direct External', 'Indirect Contextual', 'Indirect External', 'MainQ', 'Selective SubQ', 'Eliminative SubQ', 'Question Type Option Rationale', 'Eliminative', 'Answer:', 'Test Instance', 'Solve reading comprehension questions!', 'Question', 'Acknowledgments', 'Evaluate explanation for reading comprehension Instructions', 'Question']","precisely evaluate language model 's capability logical reading comprehension , present dataset testing understanding rationale behind critical reasoning . question taken existing multiplechoice logical reading comprehension dataset , crowdsource rationale text explain select eliminate answer option , resulting 3,003 multiple-choice subquestions associated 943 main question . experiment dataset show recent large language model ( e.g. , instructgpt ) struggle answer subquestions even able answer main question correctly . find model perform particularly poorly answering subquestions written incorrect option main question , implying model limited capability explaining incorrect alternative eliminated . result suggest dataset encourages investigation critical reasoning ability language model focusing elimination process relevant alternative . mana ashida saku sugawara . 2022. possible story : evaluating situated commonsense reasoning multiple possible scenario ."
