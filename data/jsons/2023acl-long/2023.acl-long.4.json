{
    "title": "Explainable Recommendation with Personalized Review Retrieval and Aspect Learning",
    "authors": "Hao Cheng; Shuo Wang; Wensheng Lu; Wei Zhang; Mingyang Zhou; Kezhong Lu; Hao Liao",
    "pub_date": "",
    "abstract": "Explainable recommendation is a technique that combines prediction and generation tasks to produce more persuasive results. Among these tasks, textual generation demands large amounts of data to achieve satisfactory accuracy. However, historical user reviews of items are often insufficient, making it challenging to ensure the precision of generated explanation text. To address this issue, we propose a novel model, ERRA (Explainable Recommendation by personalized Review retrieval and Aspect learning). With retrieval enhancement, ERRA can obtain additional information from the training sets. With this additional information, we can generate more accurate and informative explanations. Furthermore, to better capture users' preferences, we incorporate an aspect enhancement component into our model. By selecting the top-n aspects that users are most concerned about for different items, we can model user representation with more relevant details, making the explanation more persuasive. To verify the effectiveness of our model, extensive experiments on three datasets show that our model outperforms state-of-theart baselines (for example, 3.4% improvement in prediction and 15.8% improvement in explanation for TripAdvisor).",
    "sections": [
        {
            "heading": "Introduction",
            "text": [
                "Recent years have witnessed a growing interest in the development of explainable recommendation models (Ai et al., 2018;Chen et al., 2021). In general, there are three different kinds of frameworks for explainable recommendation models, which are post-hoc (Peake and Wang, 2018), embedded (Chen et al., 2018) and multi-task learning methods (Chen et al., 2019b). Post-hoc methods generate explanations for a pre-trained model after the fact, leading to limited diversity in explanations. * Corresponding author Embedded methods, on the other hand, demonstrate efficacy in acquiring general features from samples and mapping data to a high-dimensional vector space. However, since embedded methods rely on historical interactions or features to learn representations, they may struggle to provide accurate recommendations for users or items with insufficient data.",
                "In addition to the two frameworks mentioned above, there has been a utilization of multi-task learning frameworks in explainable recommendation systems, where the latent representation shared between user and item embeddings is employed (Chen et al., 2019b;Ai et al., 2018). These frameworks often employ the Transformer (Vaswani et al., 2017;Li et al., 2021b), a powerful text encoder and decoder structure widely used for textual processing tasks. While efficient for prediction tasks, they encounter challenges in generation tasks due to limited review content, leading to a significant decline in performance. Furthermore, these previous transformer-based frameworks do not incorporate personalized information and treat heterogeneous textual data indiscriminately. To address these issues, we make adaptations to the existing multi-task learning framework by incorporating two main components: retrieval enhancement, which alleviates the problem of data scarcity, and aspect enhancement, which facilitates the generation of specific and relevant explanations.",
                "Real-world datasets usually contain redundant reviews generated by similar users, making the selected reviews uninformative and meaningless, which is illustrated in Figure 1. To address this issue, a model-agnostic retrieval enhancement method has been employed to identify and select the most relevant reviews. Retrieval is typically implemented using established techniques, such as TF-IDF (Term Frequency-Inverse Document Frequency) or BM25 (Best Match 25) (Lewis et al., 2020), which efficiently match keywords with an Figure 1: A user's reviews of different items and selected reviews by different models. Specifically, (a) a CNN-based method, by which the review selected is too general, (b) a user-id attention-based query method (Papineni et al., 2002), by which the review selected is not specific, (c) a Co-attention based method (Chen et al., 2019b), by which the review selected contain some details, (d) our model: retrieval-based method generates informative and personalized reviews that are relevant to the hotel. inverted index and represent the question and context using high-dimensional sparse vectors. This approach facilitates the generation of sufficient specific text, thereby attaining enhanced textual quality for the user. Generally, Wikipedia is utilized as a retrieval corpus for the purpose of aiding statement verification (Karpukhin et al., 2020;Yamada et al., 2021). Here, we adopt a novel approach wherein the training set of each dataset is utilized as the retrieval corpus. By integrating this component into our framework, we are able to generate sentences with more specific and relevant details. Consequently, this enhancement facilitates the generation of explanations that are more accurate, comprehensive, and informative at a finer granularity.",
                "Moreover, users rarely share a common preference (Papineni et al., 2002). Therefore, aspects (Zhang et al., 2014), extracted from corresponding reviews, can be utilized to assist in the modeling of user representation. The incorporation of aspect enhancement has resulted in not only improved prediction accuracy, but also more personalized and user-specific text during the text generation process. By incorporating retrieval enhancement and aspect enhancement into our model, we adjust the transformer architecture to meet our needs, achieving better performance in both prediction and generation tasks.",
                "The main contributions of our framework are as follows:",
                "\u2022 In response to the problem of insufficient historical reviews for users and items in explainable recommendation systems, we propose a retrieval enhancement technique to supplement the available information with knowledge bases obtained from a corpus. To the best of our knowledge, this study represents the first application of retrievalenhanced techniques to review-based explainable recommendations.",
                "\u2022 We propose a novel approach wherein different aspects are selected for individual users when interacting with different items, and are subsequently utilized to facilitate the modeling of user representation, thereby leading to the generation of more personalized explanations.",
                "\u2022 Experimental results on real-world datasets demonstrate the effectiveness of our proposed approach, achieving superior performance compared to state-of-the-art baselines 2 .",
                "2 Related Work"
            ],
            "publication_ref": [
                "b0",
                "b2",
                "b26",
                "b5",
                "b5",
                "b0",
                "b32",
                "b21",
                "b16",
                "b25",
                "b5",
                "b34",
                "b25"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Explainable Recommendation with Generation",
            "text": [
                "Explainable recommendation systems (Zhang et al., 2020) have been extensively studied using two primary methodologies: machine learning and human-computer interaction. The former (Gedikli et al., 2014;Chen and Wang, 2017) investigates how humans perceive different styles of explanations, whereas the latter generates explanations through the application of explainable recommendation algorithms, which is more relevant to our research. Numerous approaches exist for explaining recommendations, including the use of definition templates (Li et al., 2021a), image visualization (Chen et al., 2019a), knowledge graphs (Xian et al., 2019), and rule justifications (Shi et al., 2020). Among these methods, natural language explanations (Chen et al., 2019b;Li et al., 2021b) are gaining popularity due to their user accessibility, advancements in natural language processing techniques, and the availability of vast amounts of text data on recommendation platforms. Several studies have employed Recurrent Neural Network (RNN) networks (Li et al., 2017), coupled with Long Short-Term Memory (LSTM) (Graves and Graves, 2012), for generating explanatory texts, while others have utilized co-attention and Gated",
                "Recurrent Unit (GRU) (Cho et al., 2014) in conjunction with Convolutional Attentional Memory Networks (CAML) (Chen et al., 2019b) for text generation. More recently, transformer-based networks have seen increased utilization for score prediction and interpretation generation. (Li et al., 2021b) 2.2 Pre-trained Models",
                "The pre-trained model has gained significant traction in the field of NLP recently. These models, such as (Devlin et al., 2019;Reimers and Gurevych, 2019) are trained on large-scale opendomain datasets utilizing self-supervised learning tasks, which enables them to encode common language knowledge. The ability to fine-tune these models with a small amount of labeled data has further increased their utility for NLP tasks (Qiu et al., 2020;Ren et al., 2021). For example, a pre-trained model is Sentence-BERT (Reimers and Gurevych, 2019), which utilizes a multi-layer bidirectional transformer encoder and incorporates Masked Language Model and Next Sentence Prediction to capture word and sentence-level representations. Another example is UniLM (Dong et al., 2019), which builds upon the architecture of BERT and has achieved outstanding performance in a variety of NLP tasks including unidirectional, bidirectional, and sequence-to-sequence prediction. Furthermore, research has demonstrated that pre-trained models possess the capability to capture hierarchysensitive and syntactic dependencies (Qiu et al., 2020), which is highly beneficial for downstream NLP tasks. The utilization of pre-trained models has proven to be a powerful approach in NLP field, with the potential to further improve performance on a wide range of tasks."
            ],
            "publication_ref": [
                "b9",
                "b3",
                "b19",
                "b4",
                "b33",
                "b31",
                "b5",
                "b21",
                "b3",
                "b12",
                "b6",
                "b5",
                "b21",
                "b7",
                "b28",
                "b27",
                "b29",
                "b28",
                "b8",
                "b27"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Retrieval Enhancement",
            "text": [
                "Retrieval-enhanced text generation has recently received increased attention due to its capacity to enhance model performance in a variety of natural language processing (NLP) tasks (Ren et al., 2021;Qiu et al., 2020). For instance, in open-domain question answering, retrieval-enhanced text generation models can generate the most up-to-date answers by incorporating the latest information during the generation process (Li and Gaussier, 2021;Li et al., 2020a). This is not possible for traditional text generation models, which store knowledge through large parameters, and the stored information is immutable. Retrieval-based methods also have an advantage in scalability, as they require fewer additional parameters compared to traditional text generation models (Ren et al., 2021). Moreover, by utilizing relevant information retrieved from external sources as the initial generation condition (Ren et al., 2021), retrieval-enhanced text generation can generate more diverse and accurate text compared to text generation without any external information."
            ],
            "publication_ref": [
                "b29",
                "b27",
                "b22",
                "b17",
                "b29",
                "b29"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Problem Statement",
            "text": [
                "Our task is to develop a model that can accurately predict ratings for a specific product and provide a reasonable explanation for the corresponding prediction. The model's input is composed of various elements, namely the user ID, item ID, aspects, reviews, and retrieval sentences, whereas the resulting output of the model encompasses both a prediction and its explanation. We offer a detailed description of our models' input and output data in this section."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Input Data",
            "text": [
                "\u2022 Heterogeneous information: The variables included in the framework encompass user ID u, item ID v, aspects A, retrieval sentences S and review R. Aspects A are captured in the form of a vector representing user's attention, denoted as (A u,1 , . . . , A u,n ), where A u,j represents the j-th aspect extracted from the reviews provided by user u. As an illustration, the review The screen of this phone is too small encompasses the aspect (screen, small). Regarding users, we extract the most important sentence S u,j from the set (S u,1 , ..., S u,n ). Similar operations are performed for items, where S v,j is employed. Ultimately, the user's review for the item R u,v is fed into the training process to enhance the ability to generate sentences."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Output Data",
            "text": [
                "\u2022 Prediction and explaination: Given a user u and an item v, we can obtain a rating prediction ru,v , representing user u's preference towards item v and a generated explanatory text L = (l 1 , l 2 , . . . , l T ), providing a rationale for the prediction outcome. In this context, l i denotes the i-th word within the explanation text, while T represents the maximum length of the generated text."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Methodology",
            "text": "",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Overview of Model",
            "text": [
                "Here we present a brief overview of ERRA model. As shown in Figure 2, our model mainly consists of three components, each corresponding to a subprocess of the information processing model:",
                "\u2022 Retrieval Enhancement aims to retrieve external knowledge from the training sets.",
                "\u2022 Aspect Enhancement aims to identify the most important aspects that users are concerned about in their reviews.",
                "\u2022 Joint Enhancement Transformers is responsible for the integration of the retrieved sentences and aspects with a transformer structure for simultaneously performing the prediction and explanation tasks.",
                "Next, we will provide an in-depth description of each component and how they are integrated into a unified framework."
            ],
            "publication_ref": [],
            "figure_ref": [
                "fig_0"
            ],
            "table_ref": []
        },
        {
            "heading": "Retrieval Enhancement",
            "text": [
                "A major challenge in generating rich and accurate explanations for users is the lack of sufficient review data. However, this problem can be alleviated via retrieval-enhanced technology, which introduces external semantic information."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Retrieval Encode",
            "text": [
                "The retrieval corpus is constructed using the training set. To obtain fine-grained information, lengthy reviews are divided into individual sentences with varied semantics. Using these sentences as searching unit allows the model to generate more fine-grained text. Sentence-BERT (Reimers and Gurevych, 2019) is utilized to encode each sentence in the corpus, which introduces no additional model parameters. We did not use other LLMs (Large Language Models) for retrieval encoding because it is optimized for dense retrieval and efficient for extensive experiments. Sentence-BERT is considerably faster than BERT-large or RoBERTa when encoding large-scale sentences and possesses an enhanced capacity for capturing semantic meaning, making it particularly well-suited for the retrieval task. The encoded corpus is saved as an embedding file, denoted as C. During the retrieval process, the most relevant information is directly searched from the saved vector C, which greatly improves the efficiency of retrieval."
            ],
            "publication_ref": [
                "b28"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Retrieval Method",
            "text": [
                "We adopt a searching model commonly used in the field of question answering (QA) and utilize cosine similarity for querying as a simple and efficient retrieval method. Here, we use the average of the review embedding U avg of each user as the query. This representation is in the same semantic space and also captures user preferential information to a certain extent. The average embedding U avg of all the reviews for a user is used as a query to retrieve the most similar n sentences (S u,1 , ..., S u,n ) in the previous corpus C. Our approach incorporates the Approximate Nearest Neighbor (ANN) search technique, with an instantiation based on the Faiss 3 library to improve retrieval speed through index creation. This optimization substantially decreases the total retrieval search duration. Then, in our implementation, we set n as 3 and stitch these sentences together to form a final sentence. Sentence-BERT is then used to encode this final sentence to obtain a vector S u,v , which represents the user for the item retrieval. Similarly, S v,u is used for items to retrieve users."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Aspect Enhancement",
            "text": [
                "Users' preferences are often reflected in their reviews. To better represent users, we need to select the most important aspects of their reviews. Specifically, we first extract aspects from each user and item review using extraction tools. The extracted aspects from user reviews represent the style of the users in their reviews, while the extracted aspects from item reviews represent the most important features of the item. We aim to identify the most important aspects that users are concerned about in their reviews. It is worth noting that users' interests may vary in different situations. For example, when choosing a hotel, a user may care more about the environment. Whereas, price is a key factor to consider when buying a mobile phone. To address this, we use the average vector A v i ,avg , v i \u2208 V , representing all aspects under the item reviews, as the query. This vector is encoded using Sentence-BERT. For each user, we construct a local corpus of their aspects collection (A u i ,1 , ..., A u i ,l ), u i \u2208 U and use cosine similarity as the measurement indicator. We search for the top-n aspects from the local corpus by A v i ,avg . These retrieved aspects represent the top-n aspects that the user is concerned about this item. "
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Joint Enhancement Transformers",
            "text": [
                "In our proposed model, we adopt the transformer structure in the prediction and explanation tasks.",
                "The transformer consists of multiple identical layers with each layer comprising two sub-layers: the multi-head self-attention and the position-wise feed feedback network. Previous research has made various modifications to the transformer architecture (Li et al., 2021b;Geng et al., 2022). Here we integrate the retrieved aspects with the encoding of multiple sentences in various ways. The retrieved sentences S U,j , S V,j are encoded uniformly as the input hidden vector s uv , s vu and are introduced into the first layer of the transformer.",
                "Below, we use one layer as an example to introduce our calculation steps.",
                "A i,h = softmax Q i,h K \u22a4 i,h \u221a d V i,h(1)",
                "Q i,h = S i\u22121 W Q i,h , K i,h = S i\u22121 W K i,h ,(2)",
                "V i,h = S i W V i,h(3)",
                "where S i\u22121 \u2208 R |S|\u00d7d is the i-th layer's output,",
                "W Q i,h , W K i,h , W V i,h \u2208 R d\u00d7 d",
                "H are projection matrices, d denotes the dimension of embeddings and is set to 384. |S| denotes the length of the input sequence.",
                "Subsequently, we incorporate aspect information into the model. As aspects are closely related to both users and items, we modify the internal mask structure of the model and combine the user's aspects and ID information through a selfattention mechanism. Not only does this strategy account for the uniqueness of the ID when modeling users, but also increase the personalization of the user's interactions with the item. Specifically, the same user may have different points of attention when interacting with different items. As illustrated in Figure 2, we make the information of the first four positions attend to each other, because the first two positions encode the unique user identifier, while the third and fourth positions encapsulate the personalized aspects of the user's preferences. The underlying rationale for selecting these positions is to facilitate the attention mechanism in capturing the interactions between users and products, ultimately enhancing the model's accuracy. At this point, our final input is as follows: For the two different information of ID and aspects, we use them jointly to represent the user and item. We use the self-attention mechanism to combine these two different semantic information, however, we found that it causes the final ID embedding matrix to be very close to the word embedding matrix, resulting in the loss of unique ID information and high duplication in generated sentences. To address this problem, we adopt the strategy from previous research (Geng et al., 2022) that only uses an ID to generate texts, and compares the generated text with the real text to compute the loss L c . To a certain extent, this method preserves the unique ID information in the process of combining aspects, thereby reducing the problem of repetitive sentences.",
                "[U id , V id , A u 1 , A u 2 , s uv , s vu ,",
                "L c = (u,v)\u2208T 1 |t len | |t len | t=1 \u2212 log H g ti v (4)",
                "where T denotes the training set. g ti denotes that only use the hidden vector of the position H v to generate the i-th word, i \u2208 1, 2..., t len ."
            ],
            "publication_ref": [
                "b21",
                "b10",
                "b10"
            ],
            "figure_ref": [
                "fig_0"
            ],
            "table_ref": []
        },
        {
            "heading": "Rating Prediction",
            "text": [
                "We utilized the two positions of the final layer (denoted as H v ) as the input. To combine the infor-mation of the ID and the hidden vector H v , we employed a multi-layer perceptron (MLP) to map the input into a scalar. The loss function used in this model is the Root Mean Square Error (RMSE) function.",
                "r u,v = ReLU ([H v , u id , v id ]W l,1 ) W l,2(5)",
                "L r = 1 |T | (u,v)\u2208T (r u,v \u2212 ru,v ) 2 (6)",
                "where where W 1 \u2208 R 3d\u00d7d , W 2 \u2208 R d\u00d71 are weight parameters, r u,v is the ground-truth rating."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Explanation Generation",
            "text": [
                "We adopt an auto-regressive methodology for word generation, whereby words are produced sequentially to form a coherent interpretation text. Specifically, we employ a greedy decoding strategy, wherein the model selects the word with the highest likelihood to sample at each time step. The model predicts the subsequent hidden vector based on the previously generated one, thereby ensuring the preservation of context throughout the entire generation process.",
                "e t = softmax (W v H L,t + b v )(7)",
                "where W v \u2208 R |V|\u00d7d and b v \u2208 R |V| are weight parameters. The vector e t represents the probability distribution over the vocabulary V."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Aspect Discriminator",
            "text": [
                "To increase the probability that the selected aspects appear in explanation generation. We use the previous method (Chen et al., 2019b) and adapt it to our task. We represent \u03c4 as the aspects that interest this user, \u03c4 \u2208 R |V| . If the generated word at time t is an aspect, then \u03c4 a is 1. Otherwise, it is 0. The loss function is as follows:",
                "L a = 1 |T | (u,v)\u2208T 1 |t len | |t len | t=1 (\u2212\u03c4 a log e t,a ) (8)"
            ],
            "publication_ref": [
                "b5"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Text Generation",
            "text": [
                "We propose a mask mechanism that allows for the efficient integration of ID, aspects, and retrieved sentence information into the hidden vector of the Beginning of Sentence (BOS) position. At each time step, the word hidden vector is transformed into a vocabulary probability through a matrix, and the word with the highest probability is selected via the Greedy algorithm. The generation process terminates when the predicted word is the End of Sentence (EOS) marker. To ensure that the generated text adheres to a specific length, we employ a padding and truncation strategy. When the number of generated words falls short of the set length, we fill in the remaining positions with a padding token (PAD). Conversely, when the number of generated words exceeds the set length, we truncate the later words. Here we use the Negative log-likelihood loss as a generated text L g . This loss function ensures the similarity between the generated words and the ground truth ones.",
                "L g = 1 |T | (u,v)\u2208T 1 |t len | |t len | t=1 \u2212 log e gt 6+t (9)",
                "where T denotes the training set. g t denotes the utilization of the 6+t position hidden vector to generate the t-th word, t \u2208 1, 2..., t len . 6 represents the initial first six positions vector information before the BOS, and t represents the current moment."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Multi-Task Learning",
            "text": [
                "We aggregate losses to form the final objective function of our multi-task learning framework. The objective function is defined as:",
                "L = plL r + \u03bb c L c + glL g + alL a + \u03bb l \u2225\u0398\u2225 2 2 (10)",
                "where L g represents the loss function of text generation and L c is the loss function for context prediction, with pl and gl as their weights, respectively. L a is the loss function for aspect discriminator and al is its weights. \u0398 contains all the neural parameters."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Experiments",
            "text": "",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Datasets",
            "text": [
                "We performed experiments on three datasets, namely Amazon (cell phones), Yelp (restaurants), and TripAdvisor (hotels) (Li et al., 2020b). We  (Zhang et al., 2014) to extract the aspects in each review and correspond it to the respective review."
            ],
            "publication_ref": [
                "b20"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Evaluation Metrics",
            "text": [
                "For rating prediction, in order to evaluate the recommendation performance, we employ two commonly used indicators: Root Mean Square Error (RMSE) and Mean Absolute Error (MAE), which measure the deviation between the predicted ratings r and the ground truth ratings r * . For generated text, we adopt a variety of indicators that consider the quality of the generated text from different levels. BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and BERTscore (Reimers and Gurevych, 2019) are commonly used metrics in natural language generation tasks. BLEU-N (N=1,4) mainly counts on the N-grams. R2-P, R2-R, R2-F, RL-P, RL-R and RL-F denote Precision, Recall and F1 of ROUGE-2 and ROUGE-L. BERT-S represents similarity scores using contextual embeddings to calculate. They are employed to objectively evaluate the similarity between the generated text and the targeted content."
            ],
            "publication_ref": [
                "b25",
                "b24",
                "b28"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Baseline Methods",
            "text": "",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Prediction",
            "text": [
                "The performance in terms of accuracy of rating prediction is compared with two types of methods: Machine Learning and Deep Learning:",
                "\u2022 Deep learning models: NARRE (Chen et al., 2018) is a popular type of neural network for textbased tasks. PETER (Li et al., 2021b) and NRT (Li et al., 2017) are deep learning models that use review text for prediction and explanation at the same time.",
                "\u2022 Factorization methods: PMF (Salakhutdinov and Mnih, 2007) is a matrix factorization method that uses latent factors to represent users and SVD++ (Koren, 2008) leverages a user's interacted items to enhance the latent factors."
            ],
            "publication_ref": [
                "b21",
                "b3",
                "b30",
                "b15"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Explainability",
            "text": [
                "To evaluate the performance of explainability, we compare against three explanation methods, namely CAML (Chen et al., 2019b) and ReXPlug (Hada et al., 2021) and NRT and PETER.",
                "\u2022 ReXPlug uses GPT-2 to generate texts and is capable of rating prediction.",
                "\u2022 CAML uses users' historical reviews to represent users and uses co-attention mechanisms to pick the most relevant reviews and concepts and combine these concepts to generate text.",
                "\u2022 NRT is an advanced deep learning method for explanation tasks. As a generative method, NRT mainly generates explanations based on predicted ratings and the distribution of words in tips.",
                "\u2022 PETER is a powerful model improved by a transformer. This model effectively integrates the ID in the transformer and combines this ID information as the starting vector to generate text."
            ],
            "publication_ref": [
                "b5",
                "b13"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Reproducibility",
            "text": [
                "We conduct experiments by randomly splitting the dataset into a training set (80%), validation set (10%), and test set (10%). The baselines are tuned by following the corresponding papers to ensure the best results. The embedded vector dimension is 384 and the value yielded superior performance after conducting a grid search within the range of [128,256,384,512,768,1024]. The maximum length of the generated sentence is set to 15-20. The weight of the rating prediction (pl) is set to 0.2, and the weight of the \u03bb c and al is set to either 0.8 or 0.05. For the explanation task, the parameter gl is adjusted to 1.0 and is initialized using the Xavier method (Glorot and Bengio, 2010). The models are optimized using the Adam optimizer with a learning rate of 10 \u22121 and L2 regularization of 10 \u22124 . When the model reaches the minimum loss in a certain epoch, the learning rate will be changed at that time and multiplied by 0.25. When the total loss of continuous three epochs has not decreased, the training process will terminate. More implementation details can be found on github 4 . "
            ],
            "publication_ref": [
                "b11"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Explainability Study",
            "text": [
                "Explainability results: Table 3 shows that our proposed ERRA method consistently outperforms the baselines in terms of BLEU and ROUGE on different datasets. For instance, take BLEU as an example, our method demonstrates the largest improvement on the TripAdvisor dataset. It is likely due to the smaller size of the dataset and the relatively short length of the reviews, which allows for additional information from the retrieved sentences and aspects to supplement the generated sentences, leading to an enhancement in their richness and accuracy. In contrast, the increase in BLEU on the Yelp dataset is relatively small. It is due to the large size of the Yelp dataset, which allows the model to be trained on a vast amount of data. The GPT (Brown et al., 2020) series also prove this case, large amounts of data can train the model well, resulting in our retrieval not having as obvious an improvement compared to other datasets. Similarly, when compared with NRT and PE-TER, our model consistently outperforms them in all metrics. Whether it is in terms of the fluency of the sentence, the richness of the text, or the consistency with the real label, our model has achieved excellent results.",
                "Case study: We take three cases generated from three datasets by NRT, PETER, and ERRA method as examples. Table 4 shows that ERRA model can predict keywords, which are both closer to the original text and match the consumers' opinions, generating better explanations compared to the baseline. While the baseline model always generates statements and explanations that are not specific and detailed enough, our model can generate personalized, targeted text, such as the battery doesn't last long in Case 2 and excellent! The food here is very delicious! in Case 3. This either is the same as or similar to the ground truth.",
                "Human evaluation: We also evaluate the model's usefulness in generated sentences via the fluency evaluation experiment, which is done by human judgment. We randomly selected 1000 samples and invited 10 annotators to assign scores. Five "
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": [
                "tab_3",
                "tab_5"
            ]
        },
        {
            "heading": "Accuracy of Prediction",
            "text": [
                "The evaluation result of prediction accuracy is shown in "
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Ablation Analysis",
            "text": [
                "In order to investigate the contribution of individual modules in our proposed model, we performed ablation studies by removing the retrieval enhancement and aspect module denoted as \"ERRA-R\" and \"ERRA-A\", From Figure 3(a), we can see that the retrieval module plays a crucial role in enhancing the performance of the explanation generation task. Specifically, for the Amazon and TripAdvisor datasets, the difference between \"ERRA-R\" and ERRA is the largest for explanation generation, while showing mediocrity in the prediction task. Additionally, we also evaluated the impact of  the aspect enhancement module on performance. Without this key module, discernible degradation can be observed in both the prediction and explanation tasks, which is shown in Figure 3(b). This can be attributed to the diverse attention points of individual users. The aspects can more accurately represent the user's preference, thus making the prediction more accurate and the generated text more personalized."
            ],
            "publication_ref": [],
            "figure_ref": [
                "fig_1",
                "fig_1"
            ],
            "table_ref": []
        },
        {
            "heading": "Conclusion",
            "text": [
                "In this paper, we propose a novel model, called ERRA, that integrates personalized aspect selection and retrieval enhancement for prediction and explanation tasks. To address the issue of incorrect embedding induced by data sparsity, we incorporate personalized aspect information and rich review knowledge corpus into our model. Experimental results demonstrate that our approach is highly effective compared with state-of-the-art baselines on both the accuracy of recommendations and the quality of corresponding explanations."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Limitation",
            "text": [
                "Despite the promising results obtained in our model, there are still several areas for improvement. Firstly, when dealing with a large corpus, the online retrieval function becomes challenging as it requires a significant amount of computational resources and time. Additionally, creating a vectorized corpus dynamically every time becomes difficult. Secondly, the process of collecting a large number of reviews from users raises privacy concerns. The collection of data, especially from private and non-public sources, may pose difficulties. "
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Acknowledgments",
            "text": [
                "The authors thank all the anonymous reviewers for their valuable comments and constructive feedback. The authors acknowledge financial support from the National Natural Science Foundation of China (Grant Nos. 62276171 and 62072311), Shenzhen Fundamental Research-General Project (Grant Nos. JCYJ20190808162601658, 20220811155803001, 20210324094402008 and 20200814105901001), CCF-Baidu Open Fund (Grant No. OF2022028), and Swiftlet Fund Fintech funding. Hao Liao is the corresponding author."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "",
            "text": [
                "B2. Did you discuss the license or terms for use and / or distribution of any artifacts? Not applicable. Left blank.",
                "B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Not applicable. Left blank.",
                "B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.",
                "B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Not applicable. Left blank.",
                "B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Not applicable. Left blank.",
                "C Did you run computational experiments? 5 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? 5",
                "The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        }
    ],
    "references": [
        {
            "ref_id": "b0",
            "title": "Learning heterogeneous knowledge base embeddings for explainable recommendation",
            "journal": "",
            "year": "2018",
            "authors": "Qingyao Ai; Vahid Azizi; Xu Chen; Yongfeng Zhang; ; Jared D Kaplan; Prafulla Dhariwal; Amanda "
        },
        {
            "ref_id": "b1",
            "title": "Neural attentional rating regression with review-level explanations",
            "journal": "",
            "year": "2018",
            "authors": "Chong Chen; Min Zhang; Yiqun Liu; Shaoping Ma"
        },
        {
            "ref_id": "b2",
            "title": "Generate natural language explanations for recommendation",
            "journal": "",
            "year": "2021",
            "authors": "Hanxiong Chen; Xu Chen; Shaoyun Shi; Yongfeng Zhang"
        },
        {
            "ref_id": "b3",
            "title": "Explaining recommendations based on feature sentiments in product reviews",
            "journal": "",
            "year": "2017",
            "authors": "Li Chen; Feng Wang"
        },
        {
            "ref_id": "b4",
            "title": "Personalized fashion recommendation with visual explanations based on multimodal attention network: Towards visually explainable recommendation",
            "journal": "",
            "year": "2019",
            "authors": "Xu Chen; Hanxiong Chen; Hongteng Xu; Yongfeng Zhang"
        },
        {
            "ref_id": "b5",
            "title": "Co-attentive multi-task learning for explainable recommendation",
            "journal": "",
            "year": "2019",
            "authors": "Zhongxia Chen; Xiting Wang; Xing Xie; Tong Wu; Guoqing Bu; Yining Wang; Enhong Chen"
        },
        {
            "ref_id": "b6",
            "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
            "journal": "",
            "year": "2014",
            "authors": "Kyunghyun Cho; Bart Van Merri\u00ebnboer; Caglar Gulcehre; Dzmitry Bahdanau; Fethi Bougares; Holger Schwenk; Yoshua Bengio"
        },
        {
            "ref_id": "b7",
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "journal": "",
            "year": "2019",
            "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"
        },
        {
            "ref_id": "b8",
            "title": "Unified language model pre-training for natural language understanding and generation",
            "journal": "",
            "year": "2019",
            "authors": "Li Dong; Nan Yang; Wenhui Wang; Furu Wei; Xiaodong Liu; Yu Wang; Jianfeng Gao; Ming Zhou; Hsiao-Wuen Hon"
        },
        {
            "ref_id": "b9",
            "title": "How should i explain? a comparison of different explanation types for recommender systems",
            "journal": "International Journal of Human-Computer Studies",
            "year": "2014",
            "authors": "Fatih Gedikli; Dietmar Jannach; Mouzhi Ge"
        },
        {
            "ref_id": "b10",
            "title": "Improving personalized explanation generation through visualization",
            "journal": "",
            "year": "2022",
            "authors": "Shijie Geng; Zuohui Fu; Yingqiang Ge; Lei Li; Gerard De Melo; Yongfeng Zhang"
        },
        {
            "ref_id": "b11",
            "title": "Understanding the difficulty of training deep feedforward neural networks",
            "journal": "",
            "year": "2010",
            "authors": "Xavier Glorot; Yoshua Bengio"
        },
        {
            "ref_id": "b12",
            "title": "Long short-term memory. Supervised sequence labelling with recurrent neural networks",
            "journal": "",
            "year": "2012",
            "authors": "Alex Graves; Alex Graves"
        },
        {
            "ref_id": "b13",
            "title": "Rexplug: Explainable recommendation using plug-and-play language model",
            "journal": "",
            "year": "2021",
            "authors": "V Deepesh;  Hada; M Vijaikumar; Shirish K Shevade"
        },
        {
            "ref_id": "b14",
            "title": "Dense passage retrieval for open-domain question answering",
            "journal": "",
            "year": "2020-11-16",
            "authors": "Vladimir Karpukhin; Barlas Oguz; Wen-Tau Yih"
        },
        {
            "ref_id": "b15",
            "title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model",
            "journal": "",
            "year": "2008",
            "authors": "Yehuda Koren"
        },
        {
            "ref_id": "b16",
            "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "journal": "",
            "year": "2020",
            "authors": "Patrick Lewis; Ethan Perez; Aleksandra Piktus; Fabio Petroni; Vladimir Karpukhin; Naman Goyal; Heinrich K\u00fcttler; Mike Wen-Tau Yih; Tim Rockt\u00e4schel"
        },
        {
            "ref_id": "b17",
            "title": "Parade: Passage representation aggregation for document reranking",
            "journal": "",
            "year": "2020",
            "authors": "Canjia Li; Andrew Yates; Sean Macavaney; Ben He; Yingfei Sun"
        },
        {
            "ref_id": "b18",
            "title": "Generating long and informative reviews with aspect-aware coarse-to-fine decoding",
            "journal": "",
            "year": "2019",
            "authors": "Junyi Li; Wayne Xin Zhao; Ji-Rong Wen; Yang Song"
        },
        {
            "ref_id": "b19",
            "title": "Caesar: context-aware explanation based on supervised attention for service recommendations",
            "journal": "Journal of Intelligent Information Systems",
            "year": "2021",
            "authors": "Lei Li; Li Chen; Ruihai Dong"
        },
        {
            "ref_id": "b20",
            "title": "Generate neural template explanations for recommendation",
            "journal": "",
            "year": "2020",
            "authors": "Lei Li; Yongfeng Zhang; Li Chen"
        },
        {
            "ref_id": "b21",
            "title": "Personalized transformer for explainable recommendation",
            "journal": "",
            "year": "2021",
            "authors": "Lei Li; Yongfeng Zhang; Li Chen"
        },
        {
            "ref_id": "b22",
            "title": "Keybld: Selecting key blocks with local pre-ranking for long document information retrieval",
            "journal": "",
            "year": "2021",
            "authors": "Minghan Li; Eric Gaussier"
        },
        {
            "ref_id": "b23",
            "title": "Neural rating regression with abstractive tips generation for recommendation",
            "journal": "",
            "year": "2017",
            "authors": "Piji Li; Zihao Wang; Zhaochun Ren; Lidong Bing; Wai Lam"
        },
        {
            "ref_id": "b24",
            "title": "Rouge: A package for automatic evaluation of summaries",
            "journal": "",
            "year": "2004",
            "authors": "Chin-Yew Lin"
        },
        {
            "ref_id": "b25",
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "journal": "",
            "year": "2002",
            "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"
        },
        {
            "ref_id": "b26",
            "title": "Explanation mining: Post hoc interpretability of latent factor models for recommendation systems",
            "journal": "",
            "year": "2018",
            "authors": "Georgina Peake; Jun Wang"
        },
        {
            "ref_id": "b27",
            "title": "Pre-trained models for natural language processing: A survey",
            "journal": "Science China Technological Sciences",
            "year": "2020",
            "authors": "Xipeng Qiu; Tianxiang Sun; Yige Xu; Yunfan Shao; Ning Dai; Xuanjing Huang"
        },
        {
            "ref_id": "b28",
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "journal": "",
            "year": "2019",
            "authors": "Nils Reimers; Iryna Gurevych"
        },
        {
            "ref_id": "b29",
            "title": "Rocketqav2: A joint training method for dense passage retrieval and passage re-ranking",
            "journal": "",
            "year": "2021",
            "authors": "Ruiyang Ren; Yingqi Qu; Jing Liu; Wayne Xin Zhao; Qiaoqiao She; Hua Wu; Haifeng Wang; Ji-Rong Wen"
        },
        {
            "ref_id": "b30",
            "title": "Probabilistic matrix factorization",
            "journal": "",
            "year": "2007",
            "authors": "Ruslan Salakhutdinov; Andriy Mnih"
        },
        {
            "ref_id": "b31",
            "title": "Neural logic reasoning",
            "journal": "",
            "year": "2020",
            "authors": "Shaoyun Shi; Hanxiong Chen; Weizhi Ma; Jiaxin Mao; Min Zhang; Yongfeng Zhang"
        },
        {
            "ref_id": "b32",
            "title": "Attention is all you need",
            "journal": "",
            "year": "2017",
            "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"
        },
        {
            "ref_id": "b33",
            "title": "Reinforcement knowledge graph reasoning for explainable recommendation",
            "journal": "",
            "year": "2019",
            "authors": "Yikun Xian; Zuohui Fu; S Muthukrishnan; Gerard De Melo; Yongfeng Zhang"
        },
        {
            "ref_id": "b34",
            "title": "Efficient passage retrieval with hashing for open-domain question answering",
            "journal": "",
            "year": "2021",
            "authors": "Ikuya Yamada; Akari Asai; Hannaneh Hajishirzi"
        },
        {
            "ref_id": "b35",
            "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc",
            "journal": "",
            "year": "",
            "authors": ""
        },
        {
            "ref_id": "b36",
            "title": "If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used",
            "journal": "",
            "year": "",
            "authors": " Nltk;  Spacy;  Rouge"
        },
        {
            "ref_id": "b37",
            "title": "crowdworkers) or research with human participants? 5.5",
            "journal": "",
            "year": "",
            "authors": ""
        },
        {
            "ref_id": "b38",
            "title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators",
            "journal": "",
            "year": "",
            "authors": " D1"
        },
        {
            "ref_id": "b39",
            "title": "crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? Not applicable",
            "journal": "",
            "year": "",
            "authors": ""
        },
        {
            "ref_id": "b40",
            "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?",
            "journal": "",
            "year": "",
            "authors": " D3"
        },
        {
            "ref_id": "b41",
            "title": "Was the data collection protocol approved (or determined exempt) by an ethics review board? Not applicable",
            "journal": "",
            "year": "",
            "authors": " D4"
        },
        {
            "ref_id": "b42",
            "title": "Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? Not applicable",
            "journal": "",
            "year": "",
            "authors": " D5"
        }
    ],
    "figures": [
        {
            "figure_label": "2",
            "figure_type": "figure",
            "figure_id": "fig_0",
            "figure_caption": "Figure 2 :2Figure 2: An overview of the ERRA framework.",
            "figure_data": ""
        },
        {
            "figure_label": "3",
            "figure_type": "figure",
            "figure_id": "fig_1",
            "figure_caption": "Figure 3 :3Figure 3: Ablation analysis of prediction and explanation tasks",
            "figure_data": ""
        },
        {
            "figure_label": "",
            "figure_type": "table",
            "figure_id": "tab_0",
            "figure_caption": "t 1 , . . . , t |t len | ]. After including the location [P 1 , P 2 , P 3 , . . . , P |s| ], where |s| is the length of the input, the final input becomes [H 1 , H 2 , H 3 , . . . , H |s| ].",
            "figure_data": ""
        },
        {
            "figure_label": "1",
            "figure_type": "table",
            "figure_id": "tab_1",
            "figure_caption": "Statistics of the datasets",
            "figure_data": "DatasetsYelpAmazon TripAdvisorNumber of users27,147157,2129,765Number of items20,26648,1866,280Number of reviews 1,293,247 1,128,437320,023Records per user47.647.1832.77Records per item63.8123.4150.96"
        },
        {
            "figure_label": "2",
            "figure_type": "table",
            "figure_id": "tab_2",
            "figure_caption": "Results of prediction",
            "figure_data": "YelpAmazonTripAdvisorPMFR \u2193 1.097 0.883 1.235 0.913 0.870 0.704 M \u2193 R \u2193 M \u2193 R \u2193 M \u2193SVD++ 1.022 0.793 1.196 0.871 0.811 0.623NARRE 1.028 0.791 1.176 0.865 0.796 0.612DAML1.014 0.784 1.173 0.858 0.793 0.617NRT1.016 0.796 1.188 0.853 0.797 0.611CAML1.026 0.798 1.191 0.878 0.818 0.622PETER 1.017 0.793 1.181 0.863 0.814 0.635ERRA1.008 0.781 1.158 0.832 0.787 0.603filtered out users with fewer than 5 comments andre-divided the dataset into three sub-datasets inthe ratio of 8:1:1. The details of the datasets areshown in Table 1. We use an aspects extraction tool"
        },
        {
            "figure_label": "3",
            "figure_type": "table",
            "figure_id": "tab_3",
            "figure_caption": "Results of explanation",
            "figure_data": "DatasetsMetricsBaselines NRT CAML ReXPlug PETER ERRA-A ERRA-R ERRA OursImprovementAmazon Yelp TripAdvisorBLEU1 13.37 11.19 BLEU4 1.44 1.12 R2-P 2.06 1.48 R2-R 2.08 1.23 R2-F 1.97 1.24 RL-P 12.52 9.32 RL-R 12.20 10.11 RL-F 10.77 8.11 BERT-S 75.4 74.9 BLEU1 10.5 9.91 BLEU4 0.67 0.56 R2-P 1.95 1.78 R2-R 1.29 1.05 R2-F 1.35 1.25 RL-P 15.88 14.25 RL-R 10.72 14.26 RL-F 9.53 9.16 BERT-S 83.6 83.2 BLEU1 15.78 14.43 BLEU4 0.85 0.86 R2-P 1.98 1.49 R2-R 1.92 1.91 R2-F 1.9 1.92 RL-P 14.85 13.36 RL-R 14.03 12.38 RL-F 12.25 12.39 BERT-S 82.7 84.810.8 1.29 2.17 1.12 1.22 9.20 10.58 8.73 75.3 8.59 0.57 1.49 1.07 1.11 13.32 9.56 8.70 82.2 12.64 0.71 1.61 1.49 1.61 11.38 10.22 9.97 83.213.78 1.68 2.21 2.02 1.97 12.62 12.06 11.07 76.2 10.29 0.69 1.91 1.31 1.43 16.07 10.14 10.26 83.3 15.33 0.89 1.92 2.01 1.94 13.54 14.75 12.61 86.414.07 1.76 2.67 2.86 2.34 15.85 14.11 12.49 78.1 10.62 0.71 1.95 1.34 1.46 16.45 10.83 10.62 84.7 15.93 1.02 2.03 2.1 2.02 15.3 14.93 13.08 87.613.28 1.64 2.37 2.33 2.18 13.49 12.67 11.97 77.3 10.59 0.71 1.90 1.29 1.41 15.95 10.21 10.14 83.1 15.43 0.95 1.97 1.98 1.99 14.84 14.77 12.79 86.914.38 1.88 2.71 2.93 2.57 16.13 14.41 13.87 79.8 10.71 0.73 2.03 1.36 1.48 16.60 11.23 10.82 85.2 16.13 1.06 2.09 2.15 2.05 15.40 15.02 13.17 88.14.17% 10.6% 14.8% 17.6% 21.2% 19.7% 16.3% 18.1% 4.5% 3.92% 5.43% 5.91% 3.6% 2.36% 3.19% 9.7% 5.1% 2.2% 5.9% 15.8% 8.1% 9.7% 5.3% 8.6% 1.81% 4.50% 1.96%"
        },
        {
            "figure_label": "2",
            "figure_type": "table",
            "figure_id": "tab_4",
            "figure_caption": "",
            "figure_data": ". As we can see, it shows that ourmethod consistently outperforms baseline methodsincluding PMF, NRT, and PETER in RMSE andMSE for all datasets. We mainly compare the per-formance of our model with the PETER model,which is a state-of-the-art method. Our modeldemonstrates a significant improvement over thebaseline methods on the TripAdvisor dataset. Weattribute this improvement to the way we modelusers. By taking aspects into consideration, ourmodel is capable of accurately modeling users.And this in turn can generate more accurate pre-dictions. As shown in Table 2, ERRA's predictiveindicator is the best result on each dataset."
        },
        {
            "figure_label": "4",
            "figure_type": "table",
            "figure_id": "tab_5",
            "figure_caption": "Explanations generated by ERRA and Baseline. PETER The hotel service is pretty good! looks very nice! ERRA The room environment is pretty comfortable! The traffic here is very convenient. Case 2 -Truth The screen of this phone is too small and his battery drains fast so I can't stand it. Aspects:(screen, too small) (battery, fast) NRT The phone is bad. PETER The phone is bad, It works poorly and I don't like it. ERRA I really hate this phone, the battery doesn't last long, the screen is faulty. Case 3 -Truth Delicious! The customer service is pretty good and the open all the way to 3 am in the morning. The prime foods are excellent! Aspects:(service, good) (foods, excellent) NRT The service is pretty good. PETER he tastes delicious! The service is pretty good! ERRA excellent! The service here is pretty good. The food here is very delicious! There are many unique foods in it and open till dawn.",
            "figure_data": "Case 1 -Truth NRTThe environment of this hotel is comfortable and the trans-portation is very convenient and the sound insulation effect is great. Aspects:(environment, comfortable) (hotel, insula-tion) The environment of this hotel is best!"
        },
        {
            "figure_label": "5",
            "figure_type": "table",
            "figure_id": "tab_6",
            "figure_caption": "Results of the fluency evaluation.",
            "figure_data": "Measures Fluency KappaNRT 2.73 (0.67)CAML ReXPlug ERRA 2.92 3.11 3.45 (0.63) (0.74) (0.79)"
        },
        {
            "figure_label": "",
            "figure_type": "table",
            "figure_id": "tab_7",
            "figure_caption": "Joint Conference on Natural Language Processing, pages 979-986. Yongfeng Zhang, Xu Chen, et al. 2020. Explainable recommendation: A survey and new perspectives. Foundations and Trends in Information Retrieval, 14(1):1-101. Yongfeng Zhang, Guokun Lai, and Shaoping Ma. 2014. Explicit factor models for explainable recommendation based on phrase-level sentiment analysis. In The 37th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 83-92.",
            "figure_data": ""
        }
    ],
    "formulas": [
        {
            "formula_id": "formula_0",
            "formula_text": "A i,h = softmax Q i,h K \u22a4 i,h \u221a d V i,h(1)",
            "formula_coordinates": [
                5.0,
                101.91,
                609.74,
                187.96,
                30.99
            ]
        },
        {
            "formula_id": "formula_1",
            "formula_text": "Q i,h = S i\u22121 W Q i,h , K i,h = S i\u22121 W K i,h ,(2)",
            "formula_coordinates": [
                5.0,
                96.51,
                656.07,
                193.36,
                16.8
            ]
        },
        {
            "formula_id": "formula_2",
            "formula_text": "V i,h = S i W V i,h(3)",
            "formula_coordinates": [
                5.0,
                145.71,
                681.51,
                144.16,
                15.25
            ]
        },
        {
            "formula_id": "formula_3",
            "formula_text": "W Q i,h , W K i,h , W V i,h \u2208 R d\u00d7 d",
            "formula_coordinates": [
                5.0,
                70.2,
                718.35,
                121.55,
                22.52
            ]
        },
        {
            "formula_id": "formula_4",
            "formula_text": "[U id , V id , A u 1 , A u 2 , s uv , s vu ,",
            "formula_coordinates": [
                5.0,
                306.14,
                356.27,
                126.07,
                14.32
            ]
        },
        {
            "formula_id": "formula_5",
            "formula_text": "L c = (u,v)\u2208T 1 |t len | |t len | t=1 \u2212 log H g ti v (4)",
            "formula_coordinates": [
                5.0,
                339.08,
                639.21,
                186.07,
                38.83
            ]
        },
        {
            "formula_id": "formula_6",
            "formula_text": "r u,v = ReLU ([H v , u id , v id ]W l,1 ) W l,2(5)",
            "formula_coordinates": [
                6.0,
                87.64,
                164.13,
                202.23,
                13.85
            ]
        },
        {
            "formula_id": "formula_7",
            "formula_text": "L r = 1 |T | (u,v)\u2208T (r u,v \u2212 ru,v ) 2 (6)",
            "formula_coordinates": [
                6.0,
                110.6,
                198.81,
                179.27,
                33.52
            ]
        },
        {
            "formula_id": "formula_8",
            "formula_text": "e t = softmax (W v H L,t + b v )(7)",
            "formula_coordinates": [
                6.0,
                111.72,
                440.67,
                178.14,
                14.16
            ]
        },
        {
            "formula_id": "formula_9",
            "formula_text": "L a = 1 |T | (u,v)\u2208T 1 |t len | |t len | t=1 (\u2212\u03c4 a log e t,a ) (8)",
            "formula_coordinates": [
                6.0,
                81.45,
                629.85,
                208.41,
                38.83
            ]
        },
        {
            "formula_id": "formula_10",
            "formula_text": "L g = 1 |T | (u,v)\u2208T 1 |t len | |t len | t=1 \u2212 log e gt 6+t (9)",
            "formula_coordinates": [
                6.0,
                329.78,
                384.18,
                195.37,
                38.83
            ]
        },
        {
            "formula_id": "formula_11",
            "formula_text": "L = plL r + \u03bb c L c + glL g + alL a + \u03bb l \u2225\u0398\u2225 2 2 (10)",
            "formula_coordinates": [
                6.0,
                311.6,
                577.76,
                213.56,
                20.96
            ]
        }
    ],
    "doi": ""
}