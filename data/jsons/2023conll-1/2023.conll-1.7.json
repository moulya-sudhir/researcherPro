{
    "title": "ArchBERT: Bi-Modal Understanding of Neural Architectures and Natural Languages",
    "authors": "Mohammad Akbari; Saeed Ranjbar Alvar; Behnam Kamranian; Amin Banitalebi-Dehkordi; Yong Zhang",
    "pub_date": "",
    "abstract": "Building multi-modal language models has been a trend in the recent years, where additional modalities such as image, video, speech, etc. are jointly learned along with natural languages (i.e., textual information). Despite the success of these multi-modal language models with different modalities, there is no existing solution for neural network architectures and natural languages. Providing neural architectural information as a new modality allows us to provide fast architecture-2-text and text-2-architecture retrieval/generation services on the cloud with a single inference. Such solution is valuable in terms of helping beginner and intermediate ML users to come up with better neural architectures or AutoML approaches with a simple text query. In this paper, we propose ArchBERT, a bi-modal model for joint learning and understanding of neural architectures and natural languages, which opens up new avenues for research in this area. We also introduce a pre-training strategy named Masked Architecture Modeling (MAM) for a more generalized joint learning. Moreover, we introduce and publicly release two new bi-modal datasets for training and validating our methods. The ArchBERT's performance is verified through a set of numerical experiments on different downstream tasks such as architecture-oriented reasoning, question answering, and captioning (summarization). Datasets, codes, and demos are available as supplementary materials 1 .",
    "sections": [
        {
            "heading": "Introduction",
            "text": [
                "Existing machine learning models are mostly based on uni-modal learning, where a single modality is learned for the desired tasks. Example scenarios include image classification with image-only data; or language translation with text-only data (Raffel Figure 1: Bi-modal understanding of neural architectures and natural languages with sample applications. et al., 2020;Akbari et al., 2022;Brown et al., 2020). Despite the success of existing uni-modal learning methods at traditional single-modal tasks, they are usually insufficient (Baltru\u0161aitis et al., 2018) to model the complete aspects of human's reasoning and understanding of the environment.",
                "The alternative solution for this problem is to use multi-modal learning, where a model can jointly learn from multiple modalities such as text, image, or video to yield more abstract and generalized representations. As a result, a better understanding of various senses in information can be achieved and many new challenges that concern multi-modality can be handled. Such solution also enables the possibility of supplying a missing modality based on the observed ones. As an example, in textbased image generation, we aim to generate photorealistic images which are semantically consistent with some given text description (Bao et al., 2022).",
                "One of the most popular multi-modal solutions is multi-modal language models (LMs), where an extra modality (e.g., image or video) is jointly used and learned along with the natural languages (i.e., textual information). Some of the recent multimodal LMs include ViLBERT for image+text (Lu et al., 2019), VideoBERT for video+text (Sun et al., 2019), CodeBERT for code+text (Feng et al., 2020), and also GPT-4 (OpenAI, 2023).",
                "Although many multi-modal LMs with different modalities have been introduced so far, there is no existing solution for joint learning of neural network architectures and natural languages. Providing neural architectural information as a new modality allows us to perform many architectureoriented tasks such as Architecture Search (AS), Architecture Reasoning (AR), Architectural Question Answering (AQA), and Architecture Captioning (AC) (Figure 1). The real-world applications of such solution include fast architecture-2-text and text-2-architecture retrieval/generation services on the cloud with a single inference. Such solution is valuable in terms of helping users to come up with better neural architectures or AutoML approaches with a simple text query especially for beginner and intermediate ML users. For instance, AC can be used for automatically generating descriptions or model card information on a model hub (i.e., machine learning models repository). Furthermore, AR is helpful when a model is uploaded to a repository or cloud along with some textual description provided by the user, where the relevancy of the user's description for the given model can be automatically verified. If not verified, alternative autogenerated descriptions by a architecture-2-text solution can be proposed to the user.",
                "In this paper, we propose ArchBERT as a bimodal solution for neural architecture and natural language understanding, where the semantics of both modalities and their relations can be jointly learned (Figure 1). To this end, we learn joint embeddings from the graph representations of architectures and their associated descriptions. Moreover, a pre-training strategy called Masked Architecture Modelling (MAM) for a more generalized and robust learning of architectures is proposed. We also introduce two new bi-modal datasets called TVHF and AutoNet for training and evaluating ArchBERT. To the best of our knowledge, ArchBERT is the first solution for joint learning of architecture-language modalities. In addition, ArchBERT can work with any natural languages and any type of neural network architectures designed for different machine learning tasks. The main contributions of this paper are as follows:",
                "\u2022 A novel bi-modal model for joint learning of neural architectures and natural languages",
                "\u2022 Two new bi-modal benchmark datasets for architecture-language learning and evaluation",
                "\u2022 A new pre-training technique called MAM",
                "\u2022 Introducing and benchmarking 6 architecturelanguage-related downstream applications"
            ],
            "publication_ref": [
                "b1",
                "b5",
                "b3",
                "b4",
                "b19",
                "b31",
                "b7"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Related Works",
            "text": [
                "Multi-modal models are used in many sub-fields in machine learning. For example, Michelsanti et al. (2021) and Schoneveld et al. (2021) introduced the audio-visual models trained on input acoustic speech signal and video frames of the speaker for speech enhancement, speech separation, and emotion recognition. Multi-modal models used in biomedical (Venugopalan et al., 2021;Vale-Silva and Rohr, 2021), remote-sensing (Hong et al., 2020;Maimaitijiang et al., 2020), and autonomous driving (Xiao et al., 2020) applications have also proven to provide more accurate prediction and detection than the unimodal models. Among different types of multi-modal LMs in the literature, transformer-based ones have shown significant performance, especially for vision-andlanguage tasks like visual question answering, image captioning, and visual reasoning. In Visual-BERT (Li et al., 2019), a stack of transformers is used to align the elements of text and image pairs. ViLBERT (Lu et al., 2019) extended BERT to a multi-modal double-stream model based on coattentional transformer layers. In LXMERT (Tan and Bansal, 2019), three encoders including language, object relation, and cross modality encoders are used. A single-stream vision-language model was introduced in VL-BEIT (Bao et al., 2022), where unpaired and paired image-text modalities were used for pre-training.",
                "Video is another modality that is used with language in multi-modal models. VideoBERT (Sun et al., 2019) is a single-stream video-language model, which learns a joint visual-linguistic representation from input video-text pairs. VIOLET (Fu et al., 2021) is another example that employs a video transformer to model the temporal dynamics of videos, and achieves SOTA results on video question answering and text-to-video retrieval. Programming language is also an emerging modality that has been used along with language. For example, CodeBERT (Feng et al., 2020) is a multistream model, which uses LMs in each stream, where the input code is regarded as a sequence of tokens. On the other hand, GraphCodeBERT (Guo et al., 2021) proposes a structure-aware pre-training technique to consider the inherent structure of the code by mapping it to a data flow graph.",
                "There are several prior works that combine more than two modalities. In Multimodal Transformer (MulT) (Tsai et al., 2019), cross-modal attention modules are added to the transformers to learn representations from unaligned multi-modal streams, including the language, the facial gestures, and the acoustic behaviors. VATT (Akbari et al., 2021) also used video, audio, and text transformers along with a self-supervised learning strategy to obtain multi-modal representations from unlabeled data.",
                "It is worth mentioning that ChatGPT (OpenAI, 2022) can be used for information retrieval, question answering, and also summarization over the textual descriptions of well-known neural architectures such AlexNet (Krizhevsky et al., 2017) or Faster-RCNN (Ren et al., 2015). However, unlike ArchBERT, it does not have a bi-modal understanding of both neural architectures (i.e., graphs) and natural languages, especially for newly proposed architectures and models."
            ],
            "publication_ref": [
                "b22",
                "b30",
                "b36",
                "b34",
                "b12",
                "b20",
                "b38",
                "b17",
                "b19",
                "b32",
                "b4",
                "b31",
                "b7",
                "b10",
                "b33",
                "b0",
                "b15",
                "b28"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Proposed Method: ArchBERT",
            "text": [
                "The overall ArchBERT framework is shown in Figure 2. The major components of ArchBERT include a text encoder, an architecture encoder, a cross encoder, and a pooling module.",
                "First, the input text represented by a sequence of n words W \" tw i |i P r1, nsu is tokenized to a sequence of n tokens T \" tt i |i P r1, nsu. Then, the text encoder E t is utilized to map them to some word/token embeddings denoted by M t P R pn\u02c6dq with the embedding size of d: M t \" E t pT q.",
                "On the other hand, the architecture encoder is responsible for encoding the input neural architecture. In this procedure, the computational graph of the input architecture is first extracted and represented with a directed acyclic graph G \" tV, A, Su where V \" tv i |i P r1, msu denotes a sequence of m nodes representing the operations and layers (e.g., convolutions, fully-connected layers, summations, etc.) and A P t0, 1u m\u02c6m denotes a binary adjacency matrix describing the edges and the connectivity between the nodes. In addition to the nodes and edges, we also extract the shape of the param-eters associated with each node (i.e., input/output channel dimensions and kernel sizes), denoted by S \" tps i P N 4 q|i P r1, msu.",
                "The nodes and the shapes are separately encoded using the node and shape embedders E v and E s , respectively. The adjacency matrix along with the summation of the resulting nodes and shapes embeddings are then given to a Graph Attention Network (GAT) (Veli\u010dkovi\u0107 et al., 2018) for computing the final architecture (graph) embeddings denoted by M g P R pm\u02c6dq with the embedding size of d:",
                "M g \" GAT pE v pV q `Es pSq, Aq(1)",
                "In general, GAT is designed to operate on graphstructured data in which a set of graph features (node+shape embeddings in our case) is transformed into higher-level features. Given the adjacency matrix, the GAT model also allows all nodes to attend over their neighborhoods' features based on a self-attention strategy.",
                "For joint learning of textual and architectural embeddings and share learning signals between both modalities, a cross transformer encoder, E c , is used to process both embeddings in parallel. These embeddings are then average-pooled to fixed-size 1D representations J t P R p1\u02c6dq and J g P R p1\u02c6dq :",
                "tJ t , J g u \" E c ptM t , M g uq (2)",
                "As in S-BERT (Reimers and Gurevych, 2019), we use the cosine similarity loss as a regression objective function to learn the similarity/dissimilarity between architectures and language embeddings. First, the cosine similarity between J t and J g are computed. Given a target soft score y P r0, 1s (i.e., 0: dissimilar, 1: similar), the following mean squared-error (MSE) loss is then employed:",
                "L SIM \" }y \u00b4Jt .J g maxp}J t } 2 .}J g } 2 , q } 2 ,(3)",
                "which minimizes the cosine distance between J t and J g pairs labeled as similar, while maximizes the distance for the dissimilar ones."
            ],
            "publication_ref": [
                "b35",
                "b27"
            ],
            "figure_ref": [
                "fig_0"
            ],
            "table_ref": []
        },
        {
            "heading": "Masked Architecture Modeling (MAM)",
            "text": [
                "In the literature, a well-known pre-training objective function called Masked Language Modeling (MLM) is widely used by BERT-based models for learning language representations (Devlin et al., 2019). Inspired by MLM, we introduce a new objective called Masked Architecture Modeling (MAM) to provide more generalized learning and understanding of the graph embeddings corresponding to the neural architectures by ArchBERT. Inspired by BERT (Devlin et al., 2019), we randomly mask 15% of the nodes with a special mask token and re-produce the masked nodes under the condition of the known ones. The MAM objective function is then defined as:",
                "L M AM \" \u00b4EV i \"V log ppV i | V q, (4",
                ")",
                "where V is the masked version of V . In other words, V includes the contextual unmasked tokens surrounding the masked token V i . In practice, the corresponding probability distribution is obtained by the MAM head H M . The MAM head defines the distribution by performing the softmax function on the logits F m P R pm\u02c6|E|q mapped from the graph embeddings J g as follows: F m \" H M pJ g q, where E is the entire vocabulary of nodes (or nodes corpus) set. Given L SIM and L M AM , the following weighted loss is then used for optimizing and pre-training the ArchBERT model:",
                "L \" L SIM `\u03b1L M AM .",
                "(5)"
            ],
            "publication_ref": [
                "b6",
                "b6"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Architectural Question Answering (AQA)",
            "text": [
                "The pre-trained ArchBERT can be utilized for the AQA task that is defined as the procedure of answering natural language questions about neural architectures. In other words, we can enable the Arch-BERT model to predict the answers to architecturerelated questions when the architecture and the question are matched.",
                "For this task, we can fine-tune ArchBERT as a fusion encoder to jointly encode the input neural architecture and the question. To this end, the question and the architecture are first encoded using the text and architecture encoders, respectively. Both embeddings are then cross-encoded and pooled in order to calculate the final joint embeddings J t and J g . The element-wise product is then computed to interactively catch similarity/dissimilarity and discrepancies between the embeddings. The resulting product is fed into AQA head for mapping to the logits F q P R |A| corresponding to |A| answers:",
                "F q \" H q pJ t .J g q (6)",
                "As in (Anderson et al., 2018), the AQA in our work is formulated as a multi-label classification task, which assigns a soft target score to each answer based on its relevancy to |A| answers. A binary cross-entropy loss (denoted by L AQA ) on the target scores is then used as objective function."
            ],
            "publication_ref": [
                "b2"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Language Decoder",
            "text": [
                "We can empower the pre-trained ArchBERT to learn from and then benefiting for neural architecture captioning (or summarization) task by attaching a transformer decoder (Lewis et al., 2020) to generate textual tokens one by one. In this regard, an auto-regressive decoding procedure is employed with the following loss function:",
                "L DEC \" \u00b4ET i \"T log ppT i |T \u0103i , T q, (7",
                ")",
                "where T is the masked version of the ground truth text T , and T i is the i-th token to be predicted. T \u0103i denotes the set of all the tokens decoded before T i . Similar to MAM, the probability distribution over the whole vocabulary is practically obtained by applying softmax on the decoded feature (or logits) F d P R pm\u02c6|C|q that is calculated by providing the graph embeddings J g to the decoder: F d \" D t pJ g q, where C denotes the entire vocabulary set."
            ],
            "publication_ref": [
                "b16"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Datasets",
            "text": [
                "For pre-training the ArchBERT model, a dataset of neural architectures labeled with some relevant descriptions is required. To the best of our knowledge, there is no such bi-modal dataset in the literature. In this paper, we introduce two datasets called TVHF and AutoNet for bi-modal learning of neural architectures and natural languages. The numerical and the statistical details of TVHF and AutoNet datasets are summarized in Table 1.",
                "Note that all the labels and descriptions in the proposed datasets have been manually checked and refined by human. There may be some minor noise in the dataset (i.e., an inevitable nature of any dataset, especially the very first versions), but in overall, the datasets are of sufficient quality for our proof-of-concept experiments."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": [
                "tab_0"
            ]
        },
        {
            "heading": "TVHF",
            "text": [
                "In order to create this dataset, we collected 538 unique neural architectures form TorchVision (TV) (Marcel and Rodriguez, 2010) and HuggingFace (HF) (Wolf et al., 2019) frameworks. The descriptions relevant to the architectures were extracted from TV and HF frameworks as well as other online resources such as papers and web pages (with the vocabulary size |C|=31,764). To increase the dataset size, the descriptions were split into individual sentences each assigned to the related architecture, which provided a collection of 2,224 positive samples, i.e., pairs of architecture with their relevant descriptions (details in the appendix).",
                "To assure the model learns both similarities and dissimilarities, we also generated negative samples by assigning irrelevant descriptions to the architectures (resulting in a total of 27,863 negative samples). We randomly split the dataset (in total 30,087 samples) into 80% for train and 20% for validation.",
                "For fine-tuning and evaluating ArchBERT on Architecture Clone Detection (ACD), we establish another dataset including pairs of architectures manually hard-labeled with a dissimilarity/similarity score (0 or 1). To this end, all combinations of two architectures from TVHF were collected (in total 82.8K samples) and split into train/val sets (80% and 20%). Details are provided in the appendix."
            ],
            "publication_ref": [
                "b21",
                "b37"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "AutoNet",
            "text": [
                "As described before, TVHF includes realistic human-designed architectures, which are manually labeled with real descriptions. On the other hand, we introduce the AutoNet dataset, which includes automatically generated architectures and descriptions. AutoNet is basically the modified and extended version of DeepNet1M (Knyazev et al., 2021), which is a standardized benchmark and dataset of randomly generated architectures for the parameter prediction tasks.",
                "In AutoNet, we extend the set of operations (layers) from 15 types (in DeepNet1M) to 85, which include most of the recent operations used in computer vision and natural language models. We followed the same procedure in DeepNet1M and ran- domly generated 10K and 1K architectures for train and validation sets, respectively.",
                "For automatic generation of textual descriptions related to each architecture, we created an extensive set of sentence templates, which were filled based on the information extracted from the structure, modules, and existing layers of the corresponding architecture. The same process was applied for generating negative samples, but with the textual information of the non-existing modules and layers in the architecture. For each architecture, 10-11 textual descriptions were created, which resulted in 103,306 and 10,338 architecture and text pairs for the train and validation sets (with the vocabulary size |C|=30,980), respectively. The details of this procedure are given in the appendix."
            ],
            "publication_ref": [
                "b14"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "AutoNet-AQA",
            "text": [
                "For fine-tuning and evaluating ArchBERT on AQA, another dataset including triplets of architectures, questions, and answers is needed. As in AutoNet, a set of question/answer templates were used to automatically generate the questions and answers. The same procedure of generating neural architectures as in AutoNet was employed. 10K and 1K architectures were respectively created for the train and validation sets. For each architecture, 35 unique questions were generated, and the answers were chosen from a list of |A| \" 51 unique answers. In total, the train and validation sets respectively include 350K and 35K samples.",
                "The visualization of two sample graphs generated for ResNet18 from TVHF and a random architecture from AutoNet is shown in Figure 3. More sample data along with the quality analysis of the datasets are given in the appendix."
            ],
            "publication_ref": [],
            "figure_ref": [
                "fig_1"
            ],
            "table_ref": []
        },
        {
            "heading": "Experimental Results",
            "text": [
                "In this section, the performance of ArchBERT on the following downstream tasks is evaluated and numerically analyzed.",
                "\u2022 Architectural Reasoning (AR): it is the task of determining if a statement regarding an architecture is correct or not. \u2022 Architecture Clone Detection (ACD): it includes the process of checking if two architectures are semantically/structurally similar or not. \u2022 Architectural Question Answering (AQA): as given in Section 3, it is the process of providing an answer to a question over a given architecture. \u2022 Architecture Captioning (AC): it is the task of generating descriptions for a given architecture.",
                "Since there is no related prior works, we compare our method with some uni-modal baselines for each of the above tasks. An ablation study over different components of ArchBERT is also presented.",
                "In this work, we employ the BERT-Base model (with 12 heads) as our ArchBERT's cross encoder. We pre-trained ArchBERT on both TVHF and Au-toNet datasets with a batch size of 80, embedding size of d=768, and the Adam optimizer with learning rate of 2e-5 for 6 hours. The training on TVHF and AutoNet was respectively done for 20 and 10 epochs. Since there is a large scale difference between the L SIM and L M AM loss values in the weighted loss in Equation 5, where L M AM \"L SIM , we set \u03b1=5e-2 to balance the total loss value (obtained experimentally). A batch size of 80 is used for all the tests with the pre-trained ArchBERT."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Uni-Modal Baselines",
            "text": [
                "For the AR baseline, we compare the architecture name with an input statement, which is considered as \"correct\" if the architecture name appears in the statement, otherwise it is \"incorrect\". Note that unlike this baseline, ArchBERT does not need the architecture name to infer about the statements.",
                "For the ACD uni-modal baseline (Figure 4-left), the architecture encoder is first used to separately map both input architectures, denoted by tG 1 , G 2 u, into the graph embeddings tM 1 g , M 2 g u (Equation 1). The cross encoder and pooling module are then applied to obtain the fixed-size joint representations tJ 1 g , J 2 g u (Equation 2). The cosine similarity loss in Equation 3 is finally performed on tJ 1 g , J 2 g u pairs along with a provided hard-label. For this baseline, we trained ArchBERT with architecture-only pairs (without text encoder) from TVHF-ACD train set.",
                "For the AQA uni-modal baseline (Figure 4middle), we train a text-only ArchBERT (without architecture encoder), where the context is obtained from the textual information and summary of the input architecture, e.g., layer names (i.e., using Pytorch model summary function). The extracted information is considered as the input context on which the question answering procedure is performed. The tokenized input question and context, denoted by tT q , T c u, are mapped into token embeddings tM q t , M c t u, which are then crossencoded and average-pooled to obtain the joint embeddings tJ q t , J c t u (Equation 2). As in Equation 6, the element-wise product of tJ q t , J c t u is given to the AQA head to obtain the logits required for the binary cross-entropy loss described in Section 3.2.",
                "For the AC uni-modal baseline (Figure 4-right), we trained ArchBERT (without text encoder) followed by the decoder from scratch (no bi-modal pre-training of ArchBERT). The detailed AC procedure is described in Section 3.3."
            ],
            "publication_ref": [],
            "figure_ref": [
                "fig_2",
                "fig_2",
                "fig_2"
            ],
            "table_ref": []
        },
        {
            "heading": "Architectural Reasoning (AR)",
            "text": [
                "For this task, the input text and the architecture are given to ArchBERT to create the pooled embeddings. The cosine similarity score between these embeddings is then computed. If the score is greater than some threshold \u03c4 (i.e., 0.5), the statement on the architecture is determined as \"correct\", otherwise \"incorrect\". We evaluate the performance of the pre-trained ArchBERT on this task over the TVHF validation set. As summarized in Table 2, an accuracy and F1 score of 96.13% and 71.86% were respectively achieved. F1 scores are reported to deal with the class imbalance.  "
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": [
                "tab_2"
            ]
        },
        {
            "heading": "As reported in",
            "text": "",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Architecture Clone Detection (ACD)",
            "text": [
                "To perform this task, both input architectures are given to ArchBERT's architecture encoder followed by the cross-encoder and pooling module to obtain the pooled embeddings. The cosine similarity of the embeddings is then computed. If the similarity score is greater than a threshold (i.e., 0.5), the two architectures are considered similar, otherwise dissimilar.",
                "We first evaluate the pre-trained ArchBERT's performance on the TVHF-ACD validation set. Although the pre-trained model has not specifically learned to detect similar/dissimilar architectures, it still achieves a good accuracy of 86.20% and F1 score 60.10% (Table 2). However, by fine-tuning the pre-trained ArchBERT with TVHF-ACD train set, significantly improved accuracy and F1 score of 96.78% and 85.98% are achieved.",
                "Two baselines including Jaccard similarity (Santisteban and Tejada-C\u00e1rcamo, 2015) and a unimodal version of ArchBERT are used to compare with our bi-modal ArchBERT on ACD task. For Jaccard, the similarity of the architecture pairs is computed by taking the average ratio of intersection over union of the nodes and edges (V and A). The pairs are considered as \"similar\" if the similarity score is greater than 0.5, otherwise \"dissimilar\". As shown in Table 2, the pre-trained and fine-tuned ArchBERT models respectively outperform this baseline with 14% and 40% higher F1 scores. The ACD uni-modal baseline also achieves F1 score of 84%, i.e., 2% lower than fine-tuned ArchBERT."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": [
                "tab_2",
                "tab_2"
            ]
        },
        {
            "heading": "Architectural Question Answering (AQA)",
            "text": [
                "For this, ArchBERT along with the attached AQA head (composed of a two layer MLP) is fine-tuned with the AutoNet-AQA dataset using a batch size of 140 over 10 epochs (for about 10 hours). We use the Adam optimizer with an initial learning rate of 2e-5. At the inference time, we simply take a sigmoid over the AQA head's logits (with the same batch size of 140). As given in Table 2, ArchBERT achieves an accuracy of 72.73% and F1 score of and 73.51% over the AutoNet-AQA validation set. For the AQA baseline, an F1 score of 61.84% was obtained on AutoNet-AQA, which is \u00ab12% lower than the proposed bi-modal ArchBERT."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": [
                "tab_2"
            ]
        },
        {
            "heading": "Architecture Captioning (AC)",
            "text": [
                "To analyze ArchBERT's performance on AC, the pre-trained ArchBERT (without text encoder) attached with a language decoder is fine-tuned on both TVHF and AutoNet with a batch size of 30 for Adam optimizer with an initial learning rate of 2e-5 was used. For the language decoder, a single-layer transformer decoder (with 12 heads and hidden size of d=768) followed by 2 linear layers is used. At the inference, the beam search (with the size of 10) was employed to auto-regressively generate the output tokens, which were then decoded back to their corresponding words. The same batch size of 30 was used for the evaluation. The results over the TVHF and AutoNet validation sets are summarized in Table 3, where Rouge-Lsum-Fmeasure (RL) (Lin, 2004) scores of 0.17 and 0.46 were respectively achieved. Unlike AutoNet, TVHF dataset includes more complicated neural architectures along with high-level human-written textual descriptions, which makes the architecture captioning more challenging. As a result, lower performance is achieved.",
                "The uni-modal AC baseline achieves an RL of 0.38 on AutoNet, which is 8% lower than the proposed bi-modal ArchBERT (i.e., pre-trained on both architectures and text, and fine-tuned for AC)."
            ],
            "publication_ref": [
                "b18"
            ],
            "figure_ref": [],
            "table_ref": [
                "tab_3"
            ]
        },
        {
            "heading": "Architecture Search (AS)",
            "text": [
                "ArchBERT is also applicable to Architecture Search (AS) downstream task. The task is to design a semantic search engine to receive a textual query from the user, search over a database of numerous neural architectures (or models), and return the best matching ones. As for any semantic search engine, an indexed database of all searched architecture embeddings is needed, within which the architecture search is performed. For the search procedure over such database using ArchBERT, the text query is encoded by the text encoder, and then is cross-encoded to make sure the previouslylearned architectural knowledge is also utilized for computing final text embeddings. The pooled text embeddings are then compared with all the architecture embeddings stored in the database to find the best matching (most similar) architectures. We did not report any numerical analysis for AS due to the lack of related validation set. However, qualitative demo is available in the supplementary materials."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Qualitative Results",
            "text": [
                "In Table 4, ArchBERT's predictions on AR and ACD tasks over some samples from TVHF validation set are given. In addition, we present the predictions on AC and AQA tasks over the right architecture in Figure 3 (i.e., a sample from AutoNet validation set). Sample cases for which ArchBERT makes wrong predictions are also given in the table (marked with *), e.g., AR's prediction for Vit_b_16 and ConvNext-tiny architectures."
            ],
            "publication_ref": [],
            "figure_ref": [
                "fig_1"
            ],
            "table_ref": [
                "tab_4"
            ]
        },
        {
            "heading": "Ablation Study",
            "text": [
                "We conduct ablation study to analyze the effect of ArchBERT's different modules such as MAM, Cross Encoder, and graph elements on the performance of AR, ACD, AQA, and AC tasks. The results are summarized in Tables 2 and 3. First, we remove the MAM head and its loss from the pre-training and fine-tuning stages. The performance of the pre-trained model without MAM is evaluated on AR and ACD with the TVHF dataset. As seen in Table 2, excluding MAM in pretraining results in a significant F1 drops by 7.59% and 10.51% on AR and ACD tasks, respectively. The effect of MAM on finetuend ArchBERT for AQA and AC downstream tasks is also evaluated and reported in in Tables 2 and 3. It is shown that using MAM provides F1 score improvements of 7.35% and 0.03% on AQA and AC, respectively.",
                "We also study the ArchBERT's performance when the Transformer cross encoder is not used for encoding the architectures. In this case, the embeddings obtained from the architecture encoder are directly used for training and evaluating the model by bypassing the cross encoder. The corresponding results on AR, ACD, and AQA tasks are given in Table 2. From the results, when the cross encoder is removed, the performance of both the pre-trained and fine-tuned models decreases. This reveals the importance of the cross encoder in joint encoding and learning of the text and architecture. As seen in the table, the F1 scores on AR, ACD, and AQA tasks are substantially reduced by 14.83%, 17.75%, and 10.18%, respectively, if the cross encoder is not utilized for architecture encoding.",
                "We also ran a set of ablations over different graph items. For AR, F1 scores of 71.86% (Arch-BERT), 69.16% (w/o shape), 68.98% (w/o edge), and 65.80% (w/o shape+edge) are achieved. For ACD, F1 scores of 60.10% (ArchBERT), 60.20% (w/o shape), 47.96% (w/o edge), and 56.45% (w/o shape+edge) are obtained. It is seen that using all graph items provides the best results. For ACD, the shape has no effect on F1 score, but excluding it gives \u00ab1% lower accuracy.",
                "The ArchBERT's performance on out-ofdistribution data will be presented in the appendix."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": [
                "tab_3",
                "tab_2",
                "tab_3",
                "tab_2"
            ]
        },
        {
            "heading": "Embeddings Visualization",
            "text": [
                "As discussed before, ArchBERT learns to minimize the cosine distance between relevant text and archi-tecture embeddings, while maximizing the distance for the irrelevant ones. To convey this concept, we visualize the joint embeddings of example relevant texts and architectures (i.e., J t and J g in Equation 2) form TVHF dataset in Figure 5. The points in the figure are obtained by projecting the embeddings to a 2D space via PCA (Jolliffe, 2005). As shown in Figure 5, the text embeddings are mapped to the points near by their relevant architectures. This implies that ArchBERT has learned to minimize the distance between the related pairs of texts and architectures (i.e., positive samples) and obtain similar embeddings for them. On the other hand, the points for the irrelevant descriptions and architectures are projected far from each other, which shows the success of ArchBERT in maximizing the distance between unrelated pairs."
            ],
            "publication_ref": [
                "b13"
            ],
            "figure_ref": [
                "fig_3",
                "fig_3"
            ],
            "table_ref": []
        },
        {
            "heading": "Conclusion",
            "text": [
                "In this paper, we proposed ArchBERT, a bi-modal solution for joint learning of neural architectures and natural languages. We also introduced a new pre-training technique called Masked Architecture Modeling (MAM) for a better generalization of ArchBERT. In addition, two new bi-modal benchmark datasets called TVHF and AutoNet were presented on which the proposed model was trained and evaluated for different downstream tasks. Five architecture-language-related tasks and applications were introduced in this work to verify the performance of ArchBERT. This work has opened up new avenues for research in the area of architecturelanguage joint understanding, particularly the proposed benchmarks. Potential research directions to this work include text-based neural architecture generation and bi-modal learning of languages and other graph-structured modalities such as knowledge graphs and social network graphs.",
                "TV-only, and HF-only datasets, and evaluate their performance on each other. The corresponding experimental results are summarized in Table 5.",
                "As observed in the table, the models trained on TV and HF subsets do not generalize to each other due to the difference in their data distributions, which results in poor performance. The distribution plots for TV and HF subsets are shown in Figure 8. As given in Table 5, the highest scores on each of TV and HF subsets are obtained by the model trained with the entire TVHF training dataset. In order to improve the performance of our model on OOD, some techniques such as zero-shot or fewshot learning can be employed, which is a potential research direction for this work."
            ],
            "publication_ref": [],
            "figure_ref": [
                "fig_6"
            ],
            "table_ref": [
                "tab_6",
                "tab_6"
            ]
        },
        {
            "heading": "A.3 Embeddings Visualization",
            "text": [
                "In Figure 5, an embedding visualization of some architecture-text pairs was illustrated. In Figure 7, the visualizations for two different architectures from TVHF dataset are individually presented. The points on the figures are obtained by projecting the final ArchBERT's embeddings onto a 2D space via PCA. As shown in the plots, unlike the relevant text embeddings (marked with `), the irrelevant ones (marked with \u02c6) are projected far from the corresponding architecture embedddings."
            ],
            "publication_ref": [],
            "figure_ref": [
                "fig_3",
                "fig_5"
            ],
            "table_ref": []
        },
        {
            "heading": "A.4 Data Generation",
            "text": [
                "The procedure of creating TVHF dataset along with negative samples are given in Algorithm 1. To generate the negative data samples, a pre-trained S-BERT model (Reimers and Gurevych, 2019) is used to calculate the similarity score between all possible pairs of unique descriptions. If the maximum similarity score between each unique sentence and all other sentences of each unique neural architecture is smaller than a threshold 0.5, that sen-    tence is chosen as an irrelevant description for that specific neural architecture. Note that 93% of the final TVHF train set contains negative samples. The above-mentioned procedure of generating many negative candidates per each positive sample was inspired by the multiple negatives sampling idea described by Henderson et al. (2017). Having multiple negatives was proved to be effective when used with dot-product and cosine similarity loss function (Equation 3 in the main paper).",
                "For TVHF-ACD dataset, all possible pairs of neural architectures were compared based on their structures. A hard score of 1 or 0 is then assigned to a similar or dissimilar pair of architectures, respectively. For TorchVision architectures with the same architectural base (e.g., ResNet family), a hard score of 1 is assigned to the pair. For Hugging-Face models, the configuration files were compared and in case of having similar specifications, a hard score of 1 has been assigned to those architectures. In overall, the TVHF-ACD dataset includes 11% of similar pairs of architectures.",
                "For AutoNet dataset, all unique layers of each architecture are first extracted. To do so, an algorithm is developed to take an architecture as input and recursively extracts all unique modules and their class path within that architecture. These unique layers are then used along with a list of various pre-defined templates to randomly generate meaningful descriptions with different words and sentence structures. The algorithm is then used with modules that are not included in the architecture to generate irrelevant descriptions that are considered as negative data samples. Each architecture has about 10-11 different descriptions about 30% of which are the positive ones. The same extracted layers and procedures are also used for automati- cally generating the question and answer pairs, but with a different set of templates for questions."
            ],
            "publication_ref": [
                "b27"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Algorithm 1 TVHF dataset generator",
            "text": "",
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "Input:",
            "text": [
                "Threshold \u03b2, architectures G, pos_samples T p Output: list of architectures plus their positive and negative descriptions for each unique neural architecture G j P G do for each unique description T p i P T p pG j q do if max(SBERT(T p i , T p \"i )) \u010f \u03b2 then Add T i to T n pG j q (list of neg_samples for jth architecture) end if end for end for return {G, (T p , T n )}"
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "A.5 Distribution Plots for TVHF and AutoNet",
            "text": [
                "Figure 9 shows the distribution plots of the TVHF, AutoNet, and AutoNet-AQA datasets. For each dataset, the plots of the training and validation distributions of the number of nodes, the number of edges, the number of textual tokens, and the sequence length of the descriptions are illustrated."
            ],
            "publication_ref": [],
            "figure_ref": [
                "fig_7"
            ],
            "table_ref": []
        },
        {
            "heading": "A.6 Sample Data from TVHF and AutoNet",
            "text": [
                "In Table 6, example positive architecturedescription pairs (for both computer vision and natural language processing problems) from TVHF dataset are given. Some sample pairs of architectures (with their corresponding \"similar\" or \"dissimilar\" ground truth labels) from TVHF-ACD dataset are also presented in Table 7.",
                "In Table 9, we also provide data samples for the BACD task, which includes quartets of two architectures, supporting description, and the similarity label. Note that the numerical analysis of ArchBERT over BACD is not provided because our BACD validation dataset is not finalized to be used for this matter.",
                "Table 8 also presents a few data samples from AutoNet dataset used for fine-tuning and evaluating ArchBERT on AC task. In Table 10, sample data from AutoNet-AQA including the automatically generated questions and ground truth answers for AQA downstream task are given.",
                "In Figures 10 and 11, the visualization of all graphs generated for the neural architectures listed in Tables 4, 8, and 10 are illustrated."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": [
                "tab_7",
                "tab_8",
                "tab_0",
                "tab_4"
            ]
        },
        {
            "heading": "A.7 Dataset Quality Analysis",
            "text": [
                "We provide dataset quality analysis based on four criteria: reliability and completeness, label/feature noise, feature representation, and minimizing skew (Google, 2022)."
            ],
            "publication_ref": [
                "b9"
            ],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "A.7.1 Reliability and Completeness",
            "text": [
                "The reliability of data refers to how trustable the data is, whether it has duplicated values and if it covers both positive and negative samples. As for dataset completeness, it refers to how much of the relevant information is included in the dataset for dealing with the desired problem.",
                "In our TVHF dataset, we have collected models and their relevant descriptions as related bi-modal data types for the ArchBERT model to learn neu- ral architectures along with their corresponding natural language descriptions. We considered the reliability and completeness of our dataset by collecting various models with different architectures designed for different tasks such as image and text classification, object detection, text summarization, etc. Also, the descriptions that have been assigned to each model were collected through blog posts, articles, papers, and documentations containing both high/low-level information related to that specific model. Due to the limited number of humandesigned models, to make our dataset large enough for training we used each architecture more than once, and each time we assigned a different unique description to it to avoid having duplicate architecture-description pairs in our dataset. Moreover, we generated negative samples by assigning irrelevant descriptions to the architectures, so that the model could learn both similarities and dissimilarities.",
                "As discussed in Section 4, some of the descriptions in TVHF dataset did not include relevant technical information to the corresponding models. We manually reviewed the descriptions and removed such samples. We will further enhance the descriptions associated with each model within the release of the next version of our dataset."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "A.7.2 Label/Feature Noise",
            "text": [
                "Label noise refers to an imperfect annotation of data that confounds the assessment of model performance when training machine learning models. Feature noise can be defined as the noise got into the dataset through various factors such as incorrect collection by humans or instruments. Inconsistencies in data formats, missing values, and outliers are examples of noise created by this process.",
                "If noise in a dataset is defined as a wrong description for a model, our dataset is a noise-free dataset because we annotated the samples manually.",
                "Since the description of building blocks in the AutoNet models are converted to textual descriptions and question samples automatically, all the generated samples are relevant and noise-free.",
                "For our ACD dataset, we manually hard-labeled the models based on their similarity with each another. Therefore, there is no missed or wrongly labeled example in the entire dataset."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "A.7.3 Feature Representation",
            "text": [
                "Mapping data to useful features while presenting them to the model is defined as feature representation. In this case, we consider how data is presented to the model and whether the numeric values need to be normalized.",
                "To show our data to the ArchBERT model, we have been consistent in the following way. For architectures, based on their computational graphs, we extracted nodes, shapes, and edges, which the major and sufficient items to represent an architecture in our work. We then normalized these items and passed them to the model. As for descriptions, we represented each textual description with tokens, normalized them, and used them as inputs to the model. We have collected our data and presented them to the model in the way that both training and validation stages receive the exact same set of features coming from the same distribution. This guarantees that our data is not skewed towards training or validation stages. "
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "A Appendix",
            "text": [
                "A.1 Code, Dataset, and Demo",
                "In order for the results to be reproducible, we share our test code (plus the pre-trained model files) with detailed instructions in the supplementary materials. The code also includes the scripts for generating both TVHF and AutoNet datasets.",
                "We also uploaded 6 video files demonstrating the performance of ArchBERT on the following downstream tasks: architecture search (AS), architectural reasoning (AR), architecture clone detection (ACD), bi-modal architecture clone detection (BACD), architectural question answering (AQA), and architecture captioning (AC).",
                "All the code and demo files are also available here 2 .",
                "BACD task is similar to ACD, except that a supporting text, which is considered as an extra criteria to refine the results, is also provided along with the two given architectures. The average similarity of the architectures' embeddings with the help of the text embeddings is evaluated to check if the architectures are similar or not.",
                "The video recordings were taken from a web application we built to demonstrate the real-world application of our method. Example screenshots of the AR and BACD demos are shown in Figure 6."
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        },
        {
            "heading": "A.2 ArchBERT's Performance on OOD Data",
            "text": [
                "In order to study the behaviour of ArchBERT on out-of-distribution (OOD) data, we establish another set of experiments on individual TV and HF datasets that have different distributions. In this regard, we pre-train ArchBERT on each of TVHF, "
            ],
            "publication_ref": [],
            "figure_ref": [],
            "table_ref": []
        }
    ],
    "references": [
        {
            "ref_id": "b0",
            "title": "VATT: Transformers for multimodal selfsupervised learning from raw video, audio and text",
            "journal": "Advances in Neural Information Processing Systems",
            "year": "2021",
            "authors": "Hassan Akbari; Liangzhe Yuan; Rui Qian; Wei-Hong Chuang; Shih-Fu Chang; Yin Cui; Boqing Gong"
        },
        {
            "ref_id": "b1",
            "title": "E-lang: Energy-based joint inferencing of super and swift language models",
            "journal": "Long Papers",
            "year": "2022",
            "authors": "Mohammad Akbari; Amin Banitalebi-Dehkordi; Yong Zhang"
        },
        {
            "ref_id": "b2",
            "title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "journal": "",
            "year": "2018",
            "authors": "Peter Anderson; Xiaodong He; Chris Buehler; Damien Teney; Mark Johnson; Stephen Gould; Lei Zhang"
        },
        {
            "ref_id": "b3",
            "title": "Multimodal machine learning: A survey and taxonomy. IEEE transactions on pattern analysis and machine intelligence",
            "journal": "",
            "year": "2018",
            "authors": "Tadas Baltru\u0161aitis; Chaitanya Ahuja; Louis-Philippe Morency"
        },
        {
            "ref_id": "b4",
            "title": "Vl-beit: Generative vision-language pretraining",
            "journal": "",
            "year": "2022",
            "authors": "Hangbo Bao; Wenhui Wang; Li Dong; Furu Wei"
        },
        {
            "ref_id": "b5",
            "title": "Language models are few-shot learners",
            "journal": "",
            "year": "2020",
            "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell"
        },
        {
            "ref_id": "b6",
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "journal": "",
            "year": "2019",
            "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"
        },
        {
            "ref_id": "b7",
            "title": "Codebert: A pre-trained model for programming and natural languages",
            "journal": "Association for Computational Linguistics",
            "year": "2020-11-20",
            "authors": "Zhangyin Feng; Daya Guo; Duyu Tang; Nan Duan; Xiaocheng Feng; Ming Gong; Linjun Shou; Bing Qin; Ting Liu; Daxin Jiang; Ming Zhou"
        },
        {
            "ref_id": "b8",
            "title": "Violet: End-to-end video-language transformers with masked visual-token modeling",
            "journal": "",
            "year": "2021",
            "authors": "Tsu-Jui Fu; Linjie Li; Zhe Gan; Kevin Lin; William Yang Wang; Lijuan Wang; Zicheng Liu"
        },
        {
            "ref_id": "b9",
            "title": "The size and quality of a data set",
            "journal": "",
            "year": "2022",
            "authors": " Google"
        },
        {
            "ref_id": "b10",
            "title": "Graphcodebert: Pre-training code representations with data flow",
            "journal": "",
            "year": "2021-05-03",
            "authors": "Daya Guo; Shuai Shuo Ren; Zhangyin Lu; Duyu Feng; Shujie Tang; Long Liu; Nan Zhou; Alexey Duan; Shengyu Svyatkovskiy; Michele Fu;  Tufano; Colin B Shao Kun Deng; Dawn Clement; Neel Drain; Jian Sundaresan; Daxin Yin; Ming Jiang;  Zhou"
        },
        {
            "ref_id": "b11",
            "title": "Balint Miklos, and Ray Kurzweil. 2017. Efficient natural language response suggestion for smart reply",
            "journal": "",
            "year": null,
            "authors": "Matthew Henderson; Rami Al-Rfou; Brian Strope; Yun-Hsuan Sung; L\u00e1szl\u00f3 Luk\u00e1cs; Ruiqi Guo"
        },
        {
            "ref_id": "b12",
            "title": "More diverse means better: Multimodal deep learning meets remote-sensing imagery classification",
            "journal": "IEEE Transactions on Geoscience and Remote Sensing",
            "year": "2020",
            "authors": "Danfeng Hong; Lianru Gao; Naoto Yokoya; Jing Yao; Jocelyn Chanussot; Qian Du; Bing Zhang"
        },
        {
            "ref_id": "b13",
            "title": "Principal component analysis. Encyclopedia of statistics in behavioral science",
            "journal": "",
            "year": "2005",
            "authors": "Ian Jolliffe"
        },
        {
            "ref_id": "b14",
            "title": "Parameter prediction for unseen deep architectures",
            "journal": "Advances in Neural Information Processing Systems",
            "year": "2021",
            "authors": "Boris Knyazev; Michal Drozdzal; W Graham; Adriana Romero Taylor;  Soriano"
        },
        {
            "ref_id": "b15",
            "title": "Imagenet classification with deep convolutional neural networks",
            "journal": "Communications of the ACM",
            "year": "2017",
            "authors": "Alex Krizhevsky; Ilya Sutskever; Geoffrey E Hin"
        },
        {
            "ref_id": "b16",
            "title": "Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
            "journal": "",
            "year": "2020",
            "authors": "Mike Lewis; Yinhan Liu; Naman Goyal ; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"
        },
        {
            "ref_id": "b17",
            "title": "VisualBERT: A simple and performant baseline for vision and language",
            "journal": "",
            "year": "2019",
            "authors": "Liunian Harold Li; Mark Yatskar; Da Yin; Cho-Jui Hsieh; Kai-Wei Chang"
        },
        {
            "ref_id": "b18",
            "title": "Rouge: A package for automatic evaluation of summaries",
            "journal": "",
            "year": "2004",
            "authors": "Chin-Yew Lin"
        },
        {
            "ref_id": "b19",
            "title": "ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
            "journal": "",
            "year": "2019",
            "authors": "Jiasen Lu; Dhruv Batra; Devi Parikh; Stefan Lee"
        },
        {
            "ref_id": "b20",
            "title": "Soybean yield prediction from uav using multimodal data fusion and deep learning. Remote sensing of environment",
            "journal": "",
            "year": "2020",
            "authors": "Maitiniyazi Maimaitijiang; Vasit Sagan; Paheding Sidike; Sean Hartling; Flavio Esposito; Felix B Fritschi"
        },
        {
            "ref_id": "b21",
            "title": "Torchvision the machine-vision package of torch",
            "journal": "Association for Computing Machinery",
            "year": "2010",
            "authors": "S\u00e9bastien Marcel; Yann Rodriguez"
        },
        {
            "ref_id": "b22",
            "title": "An overview of deep-learning-based audio-visual speech enhancement and separation",
            "journal": "",
            "year": "2021",
            "authors": "Daniel Michelsanti; Zheng-Hua Tan; Shi-Xiong Zhang; Yong Xu; Meng Yu; Dong Yu; Jesper Jensen"
        },
        {
            "ref_id": "b23",
            "title": "Speech, and Language Processing",
            "journal": "",
            "year": "",
            "authors": ""
        },
        {
            "ref_id": "b24",
            "title": "",
            "journal": "OpenAI. 2022. Introducing chatgpt",
            "year": "",
            "authors": ""
        },
        {
            "ref_id": "b25",
            "title": "",
            "journal": "OpenAI. 2023. Gpt-4 technical report",
            "year": "",
            "authors": ""
        },
        {
            "ref_id": "b26",
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "journal": "J. Mach. Learn. Res",
            "year": "2020",
            "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; J Peter;  Liu"
        },
        {
            "ref_id": "b27",
            "title": "Sentencebert: Sentence embeddings using siamese bertnetworks",
            "journal": "",
            "year": "2019",
            "authors": "Nils Reimers; Iryna Gurevych"
        },
        {
            "ref_id": "b28",
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "journal": "",
            "year": "2015",
            "authors": "Kaiming Shaoqing Ren; Ross He; Jian Girshick;  Sun"
        },
        {
            "ref_id": "b29",
            "title": "Unilateral jaccard similarity coefficient",
            "journal": "",
            "year": "2015",
            "authors": "Julio Santisteban; Javier Tejada-C\u00e1rcamo"
        },
        {
            "ref_id": "b30",
            "title": "Leveraging recent advances in deep learning for audio-visual emotion recognition",
            "journal": "",
            "year": "2021",
            "authors": "Liam Schoneveld; Alice Othmani; Hazem Abdelkawy"
        },
        {
            "ref_id": "b31",
            "title": "VideoBERT: A joint model for video and language representation learning",
            "journal": "",
            "year": "2019",
            "authors": "Chen Sun; Austin Myers; Carl Vondrick; Kevin Murphy; Cordelia Schmid"
        },
        {
            "ref_id": "b32",
            "title": "LXMERT: learning cross-modality encoder representations from transformers",
            "journal": "",
            "year": "2019-11-03",
            "authors": "Hao Tan; Mohit Bansal"
        },
        {
            "ref_id": "b33",
            "title": "Multimodal transformer for unaligned multimodal language sequences",
            "journal": "NIH Public Access",
            "year": "2019",
            "authors": "Yao-Hung Hubert Tsai; Shaojie Bai; Paul Pu Liang; Zico Kolter; Louis-Philippe Morency; Ruslan Salakhutdinov"
        },
        {
            "ref_id": "b34",
            "title": "Long-term cancer survival prediction using multimodal deep learning",
            "journal": "Scientific Reports",
            "year": "2021",
            "authors": "A Lu\u00eds; Karl Vale-Silva;  Rohr"
        },
        {
            "ref_id": "b35",
            "title": "Graph attention networks",
            "journal": "",
            "year": "2018",
            "authors": "Petar Veli\u010dkovi\u0107; Guillem Cucurull; Arantxa Casanova; Adriana Romero; Pietro Li\u00f2; Yoshua Bengio"
        },
        {
            "ref_id": "b36",
            "title": "Multimodal deep learning models for early detection of alzheimer's disease stage",
            "journal": "Scientific reports",
            "year": "2021",
            "authors": "Janani Venugopalan; Li Tong; Hamid Reza Hassanzadeh; May D Wang"
        },
        {
            "ref_id": "b37",
            "title": "Huggingface's transformers: State-of-the-art natural language processing",
            "journal": "",
            "year": "2019",
            "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz"
        },
        {
            "ref_id": "b38",
            "title": "Multimodal endto-end autonomous driving",
            "journal": "",
            "year": "2020",
            "authors": "Yi Xiao; Felipe Codevilla; Akhil Gurram; Onay Urfalioglu; Antonio M L\u00f3pez"
        },
        {
            "ref_id": "b39",
            "title": "",
            "journal": "",
            "year": "",
            "authors": " Bert-Mini"
        },
        {
            "ref_id": "b40",
            "title": "Graphs generated for the architectures listed in Table 4 (a) Architecture with layers: Conv2d, Batch-Norm2d, ReLU, Dil_conv2d, Sep_conv2d, Avg-Pool2d, AdaptiveAvgPool2d, Linear (b) Architecture with layers: Conv2d",
            "journal": "",
            "year": "",
            "authors": ""
        },
        {
            "ref_id": "b41",
            "title": "Graphs generated for the architectures listed in Tables 8 and 10",
            "journal": "",
            "year": "",
            "authors": ""
        }
    ],
    "figures": [
        {
            "figure_label": "2",
            "figure_type": "figure",
            "figure_id": "fig_0",
            "figure_caption": "Figure 2 :2Figure 2: Overall framework of ArchBERT.",
            "figure_data": ""
        },
        {
            "figure_label": "3",
            "figure_type": "figure",
            "figure_id": "fig_1",
            "figure_caption": "Figure 3 :3Figure 3: Sample graphs generated for ResNet18 (left) and a random architecture from AutoNet (right).",
            "figure_data": ""
        },
        {
            "figure_label": "4",
            "figure_type": "figure",
            "figure_id": "fig_2",
            "figure_caption": "Figure 4 :4Figure 4: Uni-Modal Baselines (left: ACD, middle: AQA, right: AC).",
            "figure_data": ""
        },
        {
            "figure_label": "5",
            "figure_type": "figure",
            "figure_id": "fig_3",
            "figure_caption": "Figure 5 :5Figure 5: Visualization of example relevant architecture and text embeddings in a 2D space (projected via PCA).",
            "figure_data": ""
        },
        {
            "figure_label": "6",
            "figure_type": "figure",
            "figure_id": "fig_4",
            "figure_caption": "Figure 6 :6Figure 6: Screenshots from the demos. Left: Architectural Reasoning (AR); Right: Bi-Modal Architecture Clone Detection (BACD).",
            "figure_data": ""
        },
        {
            "figure_label": "7",
            "figure_type": "figure",
            "figure_id": "fig_5",
            "figure_caption": "Figure 7 :7Figure 7: Visualization of example pairs of (ir)relevant architecture and text embeddings in a 2D space (projected via PCA).",
            "figure_data": ""
        },
        {
            "figure_label": "8",
            "figure_type": "figure",
            "figure_id": "fig_6",
            "figure_caption": "Figure 8 :8Figure 8: Distribution plots of TV and HF train and validation sub-datasets compared with each other.",
            "figure_data": ""
        },
        {
            "figure_label": "9",
            "figure_type": "figure",
            "figure_id": "fig_7",
            "figure_caption": "Figure 9 :9Figure 9: Distribution plots of TVHF, AutoNet, and AutoNet-AQA train/validation datasets.",
            "figure_data": ""
        },
        {
            "figure_label": "",
            "figure_type": "figure",
            "figure_id": "fig_8",
            "figure_caption": "One of the reasons that may cause getting different results for computed metrics at training vs. validation stages is training/validation skew. It usually happens when different features are presented to the model in training and validation stages.",
            "figure_data": ""
        },
        {
            "figure_label": "1",
            "figure_type": "table",
            "figure_id": "tab_0",
            "figure_caption": "Statistical details of TVHF and AutoNet datasets (*: AQA, \u00b5: mean, \u03c3: standard deviation, M : median).",
            "figure_data": "ArchitectureTextDatasetSplit #Samples#Unique#Unique#Nodes#Edges#Unique#TokensSequence LengthArchsNodes\u00b5\u03c3M\u00b5\u03c3MTokens\u00b5\u03c3M\u00b5\u03c3MTrain24069538501146.61 1162.38 705 1281 1302.90753350716.16 11.22 14 97.60 77.76 81TVHFVal6018538501146.61 1162.38 705 1281 1302.90753296516.21 11.59 14 97.88 80.33 81Train1033061000028371.50312.61 266 401322.9924176943.81 8.62 45 333.67 74.80 345AutoNetVal10338100028384.48343.31 266 419368.20 293.565243.92 8.66 45 334.01 74.92 345Train3500001000028373.33313.90 270 404325.452978610.78 1.89 11 62.76 12.48 62AutoNet*Val35000100028358.3301.98 261 390324.31 285.58610.79 1.89 11 62.76 12.45 62"
        },
        {
            "figure_label": "",
            "figure_type": "table",
            "figure_id": "tab_1",
            "figure_caption": "Table 2, a F1 score of 55.93% is achieved by the AR baseline, which is about 16% lower than ArchBERT.",
            "figure_data": ""
        },
        {
            "figure_label": "2",
            "figure_type": "table",
            "figure_id": "tab_2",
            "figure_caption": "The performance of ArchBERT and its components on different tasks and datasets (AR: Architectural Reasoning, ACD: Architecture Clone Detection, AQA: Architectural Question Answering, CR: Cross Encoder, MAM: Masked Architecture Modeling).",
            "figure_data": "Task Dataset ModelAcc(%) F1(%)ArchBERT96.13 71.86-w/o Shape95.44 69.16ARTVHF-w/o Edge -w/o Edge+Shape 95.12 65.80 95.52 68.98-w/o MAM95.18 64.27-w/o CR94.42 57.03Baseline89.03 55.93ArchBERT86.20 60.10-w/o Shape85.44 60.20-w/o Edge76.70 47.96-w/o Edge+Shape 82.90 56.45ACD TVHF-w/o MAM78.80 49.59-w/o CR69.89 42.35Jaccard80.22 45.96ArchBERT-ft96.78 85.98Baseline (uni)96.24 84.01ArchBERT72.73 73.51AQA AutoNet-w/o MAM -w/o CR66.08 66.16 60.32 63.33Baseline (uni)55.82 61.84"
        },
        {
            "figure_label": "3",
            "figure_type": "table",
            "figure_id": "tab_3",
            "figure_caption": "ArchBERT's performance on Architecture Captioning (AC) (CR: Cross Encoder, MAM: Masked Architecture Modeling, R1: Rouge1-Fmeasure, R2: Rouge2-Fmeasure, RL: Rouge-Lsum-Fmeasure).",
            "figure_data": "Dataset ModelR1R2RLArchBERT0.180.050.17TVHF-w/o MAM0.170.050.15Baseline (uni)0.180.070.17ArchBERT0.480.360.46AutoNet-w/o MAM0.450.340.43Baseline (uni)0.400.300.3810 epochs. The fine-tuning process for TVHF andAutoNet respectively took about 0.5 and 6 hours."
        },
        {
            "figure_label": "4",
            "figure_type": "table",
            "figure_id": "tab_4",
            "figure_caption": "Qualitative results on various tasks ( : Correct/Similar, : Incorrect/Dissimilar, *: wrong preds).",
            "figure_data": "ArchitectureTextARACDResNet18image classifier with residual layersFasterrcnntext classifier using(ResNet50)bert-based modelsBert-baseobject detection for photosRoBERTtext classifier using(small)bert-based modelsVit_b_16classification bert-like imageFasterrcnnobject detection for(mobilenet)photosConvNexta very large convnextBert-mini(tiny)architecturelanguage model withattention layersAC: \"this model separable convolutionFigure 3's right architecture (AutoNet)which divides a single convolution into two convolutions\" AQA: What type of pooling is used in this architecture?Prediction: 'MaxPool2d', 'AvgPool2d'"
        },
        {
            "figure_label": "5",
            "figure_type": "table",
            "figure_id": "tab_6",
            "figure_caption": "ArchBERT's performance on OOD data.",
            "figure_data": ""
        },
        {
            "figure_label": "6",
            "figure_type": "table",
            "figure_id": "tab_7",
            "figure_caption": "Positive data samples from the TVHF dataset (TV: TorchVision, HF: HuggingFace).",
            "figure_data": "ArchitectureDescriptionSourcevit_b_16adopted from BERTTVImproved version of DeepLab v2, with optimi-segmentation.deeplabv3_resnet101zation of ASPP layer hyper parameters andTVwithout a Dense CRF layer, for faster operation.Residual Networks, or ResNets, learnresnet101residual functions with reference to the layer inputs , instead of learning unreferencedTVfunctions.A DenseNet is a type of convolutionalneural network that utilises dense connectionsdensenet121between layers, through Dense Blocks,TVwhere we connect all layers (with matchingfeature-map sizes) directly with each otherResNeXt is a homogeneous neural networkresnext50_32x4dwhich reduces the number of hyper parametersTVrequired by conventional ResNet.detection.keypointrcnn_resnet50_fpn12 Million Parameters, 2 Billion FLOPs and File Size is 47.08 MB.TVDemangeJeremy/4-sentiments-with-flaubertThis model is a fine-tuned version of google/fnet-base on the GLUE WNLI datasetHFctoraman/RoBERTa-TR-medium-charModel architecture is similar to bert-medium (8 layers, 8 heads, and 512 hidden size)HFT5-Efficient-BASE-DM1000 is a variation ofgoogle/t5-efficient-base-dm1000Google's original T5 following the T5 modelHFarchitecture.a self-supervised Chinese-Japanese pre-trainedmicrosoft/unihanlm-basemasked language model (MLM) with a novelHFtwo-stage coarse-to-fine training approach.WMT 21 En-X is a 4.7B multilingualfacebook/wmt21-dense-24-wide-en-xencoder-decoder (seq-to-seq) model trainedHFfor one-to-many multilingual translation."
        },
        {
            "figure_label": "7",
            "figure_type": "table",
            "figure_id": "tab_8",
            "figure_caption": "Positive and negative data samples from TVHF-ACD validation set (TV: TorchVision, HF: HuggingFace, 0: dissimilar, 1: similar).",
            "figure_data": "Architecture 1Architecture 2Label Sourcevgg11vgg19_bn1TVmnasnet0_5mnasnet0_751TVinception_v3efficientnet_b30TVefficientnet_b1regnet_x_800mf0TVgoogle/t5-efficient-large-kv128 google/t5-efficient-small-kv161HFjweb/japanese-soseki-gpt2-1btartuNLP/gpt-4-est-large1HFhakurei/gpt-j-random-tinierminimaxir/magic-the-gathering0HFmwesner/bart-mlmtartuNLP/gpt-4-est-base0HF"
        }
    ],
    "formulas": [
        {
            "formula_id": "formula_0",
            "formula_text": "M g \" GAT pE v pV q `Es pSq, Aq(1)",
            "formula_coordinates": [
                3.0,
                341.31,
                371.87,
                183.1,
                10.63
            ]
        },
        {
            "formula_id": "formula_1",
            "formula_text": "tJ t , J g u \" E c ptM t , M g uq (2)",
            "formula_coordinates": [
                3.0,
                358.11,
                572.82,
                166.3,
                10.63
            ]
        },
        {
            "formula_id": "formula_2",
            "formula_text": "L SIM \" }y \u00b4Jt .J g maxp}J t } 2 .}J g } 2 , q } 2 ,(3)",
            "formula_coordinates": [
                3.0,
                322.94,
                703.29,
                201.47,
                25.56
            ]
        },
        {
            "formula_id": "formula_3",
            "formula_text": "L M AM \" \u00b4EV i \"V log ppV i | V q, (4",
            "formula_coordinates": [
                4.0,
                107.65,
                285.53,
                177.25,
                14.27
            ]
        },
        {
            "formula_id": "formula_4",
            "formula_text": ")",
            "formula_coordinates": [
                4.0,
                284.89,
                288.64,
                4.24,
                9.46
            ]
        },
        {
            "formula_id": "formula_5",
            "formula_text": "F q \" H q pJ t .J g q (6)",
            "formula_coordinates": [
                4.0,
                379.41,
                74.37,
                145.0,
                10.63
            ]
        },
        {
            "formula_id": "formula_6",
            "formula_text": "L DEC \" \u00b4ET i \"T log ppT i |T \u0103i , T q, (7",
            "formula_coordinates": [
                4.0,
                335.06,
                300.21,
                185.11,
                14.27
            ]
        },
        {
            "formula_id": "formula_7",
            "formula_text": ")",
            "formula_coordinates": [
                4.0,
                520.17,
                303.31,
                4.24,
                9.46
            ]
        }
    ],
    "doi": ""
}