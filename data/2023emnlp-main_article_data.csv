title,authors,abstract,pub_date,sections_list
IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions,Zhebin Zhang; Xinyu Zhang; Yuanhang Ren; Saijiang Shi; Meng Han; Yongkang Wu; Ruofei Lai; Zhao Cao,"Retrieval-Augmented Generation (RAG), by incorporating external knowledge with parametric memory of language models, has become the state-of-the-art architecture for opendomain QA tasks. However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions. In this paper, we propose an Induction-Augmented Generation (IAG) framework that utilizes inductive knowledge along with the retrieved documents for implicit reasoning. We leverage large language models (LLMs) for deriving such knowledge via a novel prompting method based on inductive reasoning patterns. On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student, respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for answer prediction, while IAG-Student gets rid of dependencies on GPT service at inference time by incorporating a student inductor model. The inductor is firstly trained via knowledge distillation and further optimized by back-propagating the generator feedback via differentiable beam scores. Experimental results show that IAG outperforms RAG baselines as well as ChatGPT on two Open-Domain QA tasks. Notably, our best models have won the first place in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA (since Jan 8, 2023).",,"['Introduction', 'Related Work', 'Overview', 'Knowledge Elicitation via Inductive Prompting', 'IAG Implementations', 'IAG-GPT', 'IAG-Student', 'Experimental Setup', 'Datasets', 'Models', 'Results', 'Main Results', 'Prompting Methods', 'Optimization of Inductor', 'Distillation Strategies', 'TAILBACK', 'Knowledge Fusion Mechanism', 'Knowledge Fusion v.s. Self-Consistency', 'Number of Knowledge Statements', 'Conclusion', 'Limitations', 'A Prompting Template', 'B Additional Experimental Results', 'B.1 Comparison between Information Retrieval and knowledge Induction', 'B.2 Comparison among Prompting Methods']"
Evaluating and Modeling Attribution for Cross-Lingual Question Answering,Benjamin Muller; John Wieting; Jonathan H Clark; Tom Kwiatkowski; Sebastian Ruder; Livio Baldini Soares; Roee Aharoni; Jonathan Herzig; Xinyi Wang; Inria Paris; Google Deepmind; Google Research,"Trustworthy answer content is abundant in many high-resource languages and is instantly accessible through question answering systems-yet this content can be hard to access for those that do not speak these languages. The leap forward in cross-lingual modeling quality offered by generative language models offers much promise, yet their raw generations often fall short in factuality. To improve trustworthiness in these systems, a promising direction is to attribute the answer to a retrieved source, possibly in a content-rich language different from the query. Our work is the first to study attribution for cross-lingual question answering. First, we introduce the XOR-AttriQA dataset to assess the attribution level of a state-of-theart cross-lingual question answering (QA) system in 5 languages. To our surprise, we find that a substantial portion of the answers is not attributable to any retrieved passages (up to 47% of answers exactly matching a gold reference) despite the system being able to attend directly to the retrieved text. Second, to address this poor attribution level, we experiment with a wide range of attribution detection techniques. We find that Natural Language Inference models and PaLM 2 fine-tuned on a very small amount of attribution data can accurately detect attribution. With these models, we improve the attribution level of a cross-lingual QA system. Overall, we show that current academic generative cross-lingual QA systems have substantial shortcomings in attribution and we build tooling to mitigate these issues. 1 1 The XOR-AttriQA dataset is available at https: //github.com/google-research/google-research/ tree/master/xor_attriqa. XOR-AttriQA includes approximately 10,000 annotated examples to foster research in the modeling and evaluation of attribution in cross-lingual settings. † Correspondence to {jwieting,jhclark}@google.com. ♠ Work done as an intern at Google Research.",,"['Introduction', 'Attribution of Generative Language Models', 'Attributed Question Answering', '2.3', 'Cross-Lingual QA Attribution Evaluation', 'The XOR-AttriQA Dataset', ""Raters' Demographic and Cultural Background"", 'Attribution Evaluation of CORA', 'Lack of Attribution of XORQA Predictions', ""Analysis of CORA's Attribution Level"", 'Attribution Detection for XORQA', 'Attribution Detection Models', 'STRING-MATCH', 'Results', 'NLI Model for Reranking', 'Discussion and Future Directions', 'Conclusion', 'Limitations', 'Contributions', 'General', 'Acknowledgements', '', 'A.1 Codebase', 'A.2 XOR-TyDiQA', 'A.3 Languages Distribution of MDPR', 'B Data Collection', 'B.2 AIS Score', 'C Examples of Attribution without Exact-Match', 'Query:']"
Absolute Position Embedding Learns Sinusoid-like Waves for Attention Based on Relative Position,Yuji Yamamoto; Takuya Matsuzaki,"Attention weight is a clue to interpret how a Transformer-based model makes an inference. In some attention heads, the attention focuses on the neighbors of each token. This allows the output vector of each token to depend on the surrounding tokens and contributes to make the inference context-dependent. We analyze the mechanism behind the concentration of attention on nearby tokens. We show that the phenomenon emerges as follows: (1) learned position embedding has sinusoid-like components, (2) such components are transmitted to the query and the key in the selfattention, (3) the attention head shifts the phases of the sinusoid-like components so that the attention concentrates on nearby tokens at specific relative positions. In other words, a certain type of Transformer-based model acquires the sinusoidal positional encoding to some extent on its own through Masked Language Modeling.",,"['Introduction', 'Background', 'Multi-Head Self-Attention', 'Position Embedding', 'Relative Position Dependence of Attention', 'Attention to Nearby Tokens', 'Learned Representation of Positions', 'APE Includes Sinusoid-like Waves', 'Dimensionality of Positional Representation', 'Positional Representation in', 'Rethinking About Query and Key', 'Spectral Analysis of Query and Key', 'Attention Based on Relative Position is due to the Phase Shift', 'Phase Shift Width is the Same even if', 'Attention to the Adjacent Tokens', 'Remark on the Learning Process of Position Embeddings', 'Related Works', 'Conclusion', 'Limitations', 'A Cumulative Principal Component', 'B Amplitude Spectra of Various Models', 'C Comparing Different Architectures', 'Figures for GPT-', 'Position embedding The position embeddings', 'E.2 Results', 'D Theorems About the Phase Shift', 'E How the Relative Position Dependence of Attention Emerges']"
Chinese Lexical Substitution: Dataset and Method,Jipeng Qiang; Kang Liu; Ying Li; Yun Li; Yi Zhu; Yunhao Yuan; Xiaocheng Hu; Xiaoye Ouyang,"Existing lexical substitution (LS) benchmarks were collected by asking human annotators to think of substitutes from memory, resulting in benchmarks with limited coverage and relatively small scales. To overcome this problem, we propose a novel annotation method to construct an LS dataset based on human and machine collaboration. Based on our annotation method, we construct the first Chinese LS dataset CHNLS which consists of 33,695 instances and 144,708 substitutes, covering three text genres (News, Novel, and Wikipedia). Specifically, we first combine four unsupervised LS methods as an ensemble method to generate the candidate substitutes, and then let human annotators judge these candidates or add new ones. This collaborative process combines the diversity of machine-generated substitutes with the expertise of human annotators. Experimental results that the ensemble method outperforms other LS methods. To our best knowledge, this is the first study for the Chinese LS task.",,"['Introduction', 'Related Work', 'Creating CHNLS', 'Data Preparation', 'Machine-generated Substitution', 'Manual Annotation', 'Experiments', 'Experimental Setup', 'Evaluation Results', 'Qualitative evaluation', 'Conclusions', 'Limitations', 'Ethics Statement', 'A.1 Selection of target words', 'A.2 Annotation Website', 'A.3 Annotation Manual', 'A.4 The Work of Annotators', 'B More Examples', 'Acknowledgement']"
Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting,Chenkai Sun; Jinning Li; Yi R Fung; Hou Pong Chan; Tarek Abdelzaher; Chengxiang Zhai; Heng Ji,"Automatic response forecasting for news media plays a crucial role in enabling content producers to efficiently predict the impact of news releases and prevent unexpected negative outcomes such as social conflict and moral injury. To effectively forecast responses, it is essential to develop measures that leverage the social dynamics and contextual information surrounding individuals, especially in cases where explicit profiles or historical actions of the users are limited (referred to as lurkers). As shown in a previous study, 97% of all tweets are produced by only the most active 25% of users. However, existing approaches have limited exploration of how to best process and utilize these important features. To address this gap, we propose a novel framework, named SOCIALSENSE, that leverages a large language model to induce a belief-centered graph on top of an existent social network, along with graph-based propagation to capture social dynamics. We hypothesize that the induced graph that bridges the gap between distant users who share similar beliefs allows the model to effectively capture the response patterns. Our method surpasses existing state-of-the-art in experimental evaluations for both zero-shot and supervised settings, demonstrating its effectiveness in response forecasting. Moreover, the analysis reveals the framework's capability to effectively handle unseen user and lurker scenarios, further highlighting its robustness and practical applicability.",,"['Introduction', 'Task Formulation', 'SOCIALSENSE', 'Attributes Operator', 'LLM-based Propagation', 'Network Construction', 'Unmasking Latent Persona with Large Language Model', 'Belief-Augmented Social Network', 'Information Propagation', 'Zero-Shot Prediction by Simulating Propagation with Social Prompts', 'Experiment', 'Data Construction', 'Experimental Setup', 'Results Discussion', 'Ablation Study', 'Zero-Shot Evaluation', 'Evaluation on Lurker and Unseen User Scenarios', 'Related Work', 'Limitations', 'Ethics Statements', 'A.2 Analysis of Belief Data', 'A.3 Prompts Templates', 'Acknowledgement', 'A Appendix', 'A.1 Implementation Details']"
Fine-grained Conversational Decoding via Isotropic and Proximal Search,Yuxuan Yao; Han Wu; Qiling Xu; Linqi Song,"General-purpose text decoding approaches are usually adopted for dialogue response generation. Although the quality of the generated responses can be improved with dialogue-specific encoding methods, conversational decoding methods are still under-explored. Inspired by Wu et al. ( 2023) that a good dialogue feature space should follow the rules of locality and isotropy, we present a fine-grained conversational decoding method, termed isotropic and proximal search (IPS). Our method is designed to generate the semantic-concentrated response, while still maintaining informativeness and discrimination against the context. Experiments show that our approach outperforms existing decoding strategies in the dialogue field across both automatic and human evaluation metrics. More in-depth analyses further confirm the effectiveness of our approach.",,"['Introduction', 'Methodology', 'Preliminary', 'Isotropic and Proximal Search', 'Experiments', 'Results and Discussion', 'Conclusion', 'Ackonwledgements', 'Limitations', 'Ethics Statement', 'A.1.2 Informativeness', 'A.1.3 Coherence', 'A.1.4 Semantic Coverage', 'A.2 More Details of the Task', 'A.2.1 Evaluation of G-EVAL Score', 'A.2.2 More Experimental Results', 'A.3 Surface-level Analysis', 'A.3.1 Score Distribution According to the Length of the Previous Context', 'A.3.2 Utterance Length Analysis', 'A.4.1 Instances Illustration', 'A.5 Cosine Similarity Heatmap', 'A.6 Examples of Generated Texts', '', 'A Appendix', 'A.1 Human Evaluation Instructions', 'A.1.1 Fluency']"
Holistic Inter-Annotator Agreement and Corpus Coherence Estimation in a Large-scale Multilingual Annotation Campaign,Nicolas Stefanovitch; Jakub Piskorski,"In this paper we report on the complexity of persuasion technique annotation in the context of a large multilingual annotation campaign involving 6 languages and approximately 40 annotators. We highlight the techniques that appear to be difficult for humans to annotate and elaborate on our findings on the causes of this phenomenon. We introduce Holistic IAA, a new word embedding-based annotator agreement metric and we report on various experiments using this metric and its correlation with the traditional Inter Annotator Agreement (IAA) metrics. However, given somewhat limited and loose interaction between annotators, i.e., only a few annotators annotate the same document subsets, we try to devise a way to assess the coherence of the entire dataset and strive to find a good proxy for IAA between annotators tasked to annotate different documents and in different languages, for which classical IAA metrics can not be applied.",,"['Introduction', 'Related Work', 'Persuasion Technique Annotation', 'Taxonomy', 'Annotation Process', 'Annotation Coherence & Complexity', 'Traditional IAA', 'Confusion matrix', ""Techniques' Annotation Complexity"", 'Disagreement sources', 'Holistic IAA', 'Validation: Methodology', 'Validation: Results', 'Validation: Error Analysis', 'Impact of the second curation step', 'Multilingual Dataset Coherence Estimation', 'Conclusions', 'Limitations', 'Ethics Statement', 'A Persuasion Techniques', 'B Dataset Statistics', 'C Annotation guidelines excerpt', 'ATTACK ON REPUTATION', 'JUSTIFICATION', 'DISTRACTION', 'SIMPLIFICATION', 'CALL', 'False Dilemma or No Choice:', '(b) Dictatorship', 'Consequential Oversimplification:', 'Slogans:', 'Obfuscation, Intentional Vagueness, Confusion:', 'D Confusion Matrix based on Holistic IAA', 'E Identifying the top and low groups of annotators', 'Acknowledgements', 'F Parameter search']"
PHD: Pixel-Based Language Modeling of Historical Documents,Nadav Borenstein; Phillip Rust; Desmond Elliott; Isabelle Augenstein,"The digitisation of historical documents has provided historians with unprecedented research opportunities. Yet, the conventional approach to analysing historical documents involves converting them from images to text using OCR, a process that overlooks the potential benefits of treating them as images and introduces high levels of noise. To bridge this gap, we take advantage of recent advancements in pixel-based language models trained to reconstruct masked patches of pixels instead of predicting token distributions. Due to the scarcity of real historical scans, we propose a novel method for generating synthetic scans to resemble real historical documents. We then pre-train our model, PHD, on a combination of synthetic scans and real historical newspapers from the 1700-1900 period. Through our experiments, we demonstrate that PHD exhibits high proficiency in reconstructing masked image patches and provide evidence of our model's noteworthy language understanding capabilities. Notably, we successfully apply our model to a historical QA task, highlighting its utility in this domain.",,"['Introduction', 'Background', 'NLP for Historical Texts', 'Pixel-based Models for NLU', 'Model', 'Training a Pixel-Based Historical LM', 'Artificially Generated Pretraining Data', 'Real Historical Scans', 'Pretraining Procedure', 'Pretraining Results', 'Training for Downstream NLU Tasks', 'Language Understanding', 'Historical Question Answering', 'Training Procedure', 'Results', 'Conclusion', 'Limitations', 'B Historical GLUE Baselines', 'C Additional Material', 'Acknowledgements', 'A Reproducibility', 'A.1 Training', 'Runaways Slaves in Britain', 'A.2 Dataset Generation']"
Primacy Effect of ChatGPT,Yiwei Wang; Yujun Cai; Muhao Chen; Yuxuan Liang; Bryan Hooi; Angeles ‡ Meta,"Instruction-tuned large language models (LLMs), such as ChatGPT, have led to promising zero-shot performance in discriminative natural language understanding (NLU) tasks. This involves querying the LLM using a prompt containing the question, and the candidate labels to choose from. The question-answering capabilities of ChatGPT arise from its pre-training on large amounts of human-written text, as well as its subsequent fine-tuning on human preferences, which motivates us to ask: Does ChatGPT also inherit humans' cognitive biases? In this paper, we study the primacy effect of ChatGPT: the tendency of selecting the labels at earlier positions as the answer. We have two main findings: i) ChatGPT's decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer. We hope that our experiments and analyses provide additional insights into building more reliable ChatGPT-based solutions. We release the source code at https: //github.com/wangywUST/PrimacyEffectGPT.",,"['Introduction', 'Primacy Effect of ChatGPT', 'Prompts for ChatGPT', 'Analysis with Label Shuffling', 'Prediction Comparison on an Instance', 'Statistics of Predicted Indices', 'Experiments', 'Experiment Setup', 'Consistency under Label Shuffling', 'Primacy Effect of ChatGPT', 'Evaluation on Fairness', 'Related Work', 'Conclusion', 'Limitation', 'Acknowledgement']"
Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension,Akira Kawabata; Saku Sugawara; Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel Ziegler; Jeffrey Wu; Clemens Winter; Chris Hesse; Mark Chen; Eric Sigler; Ma- Teusz Litwin; Scott Gray; Benjamin Chess; Jack Clark; Christopher Berner; Sam Mccandlish; Alec Radford; Ilya Sutskever; Dario 2020 Amodei; Dallas Card; Peter Henderson; Urvashi Khandelwal; Robin Jia; Kyle Mahowald; Dan 2020 Jurafsky; Wei-Lin Chiang; Zhuohan Li; Zi Lin; Ying Sheng; Zhanghao Wu; Hao Zhang; Lianmin Zheng; Siyuan Zhuang; Yonghao Zhuang; Joseph E Gonzalez; Ion Stoica; Eric P 2023 Xing;  Vicuna; Hyung Won; Le Hou; Shayne Longpre; Barret Zoph; Yi Tay; William Fedus; Yunxuan Li; Xuezhi Wang; Mostafa Dehghani; Siddhartha Brahma; Al- Bert Webson; Shane Shixiang; Zhuyun Gu; Mirac Dai; Xinyun Suzgun; Aakanksha Chen; Alex Chowdh- Ery; Marie Castro-Ros; Kevin Pellat; Dasha Robinson; Sharan Valter; Gaurav Narang; Adams Mishra; Vincent Yu; Yanping Zhao; Andrew Huang; Hongkun Dai; Slav Yu; Ed H Petrov; Jeff Chi; Ja- Cob Dean; Adam Devlin; Denny Roberts; Quoc V Zhou;  Le; Bhavana Dalvi; Peter Jansen; Oyvind Tafjord; Zhengnan Xie; Hannah Smith; Leighanna Pipatanangkura; Matt Gardner; Yoav Artzi; Victoria Basmov; Jonathan Berant; Ben Bogin; Sihao Chen; Pradeep Dasigi; Dheeru Dua; Yanai Elazar; Ananth Gottumukkala; Nitish Gupta; Hannaneh Hajishirzi; Gabriel Ilharco; Daniel Khashabi; Kevin Lin; Jiangming Liu; Nel- Son F Liu; Phoebe Mulcaire; Qiang Ning; Sameer Singh; Noah A Smith; Sanjay Subramanian; Reut Tsarfaty; Eric Wallace; Ally Zhang; Ben Zhou; Long Ouyang; Xu Jiang; Diogo Almeida; Carroll Wainwright; Pamela Mishkin; Chong Zhang; Katarina Slama; Alex Ray; John Schulman; Jacob Hilton; Fraser Kelton; Luke Miller; Maddie Simens; Peter Welinder; Paul F Christiano; Jan Leike; Ryan 2022 Lowe; Danilo Neves Ribeiro; Shen Wang; Xiaofei Ma; Swarnadeep Saha; Peter Hase; Nazneen Rajani; Prateek Yadav; Lisa Bauer; Mohit Bansal;  Explagraphs; Freda Shi; Xinyun Chen; Kanishka Misra; Nathan Scales; David Dohan; Ed H Chi; Nathanael Schärli; Denny Zhou;  Large; Vinh Q Tran; Xavier Garcia; Jason Wei; Won Chung; Dara Bahri; Tal Schuster; Steven Zheng; Neil Houlsby; Donald Metzler;  Ul2; Hugo Touvron; Thibaut Lavril; Gautier Izacard; Xavier Martinet; Marie-Anne Lachaux; Timothée Lacroix; Baptiste Rozière; Naman Goyal; Eric Hambro; Faisal Azhar; Aurelien Rodriguez; Armand Joulin; Louis Martin; Kevin Stone; Peter Al- Bert; Amjad Almahairi; Yasmine Babaei; Nikolay Bashlykov; Soumya Batra; Prajjwal Bhargava; Shruti Bhosale; Dan Bikel; Lukas Blecher; Cristian Canton Ferrer; Moya Chen; Guillem Cucurull; David Esiobu; Jude Fernandes; Jeremy Fu; Wenyin Fu; Brian Fuller; Cynthia Gao; Vedanuj Goswami; An- Thony Hartshorn; Saghar Hosseini; Rui Hou; Hakan Inan; Marcin Kardas; Viktor Kerkez; Madian Khabsa; Isabel Kloumann; Artem Korenev; Singh Koura; Jenya Lee; Di- Ana Liskovich; Yinghai Lu; Yuning Mao; Xavier Mar- Tinet; Todor Mihaylov; Pushkar Mishra; Igor Moly- Bog; Yixin Nie; Andrew Poulton; Jeremy Reizen- Stein; Rashi Rungta; Kalyan Saladi; Alan Schelten; Ruan Silva; Eric Michael Smith; Ranjan Subrama- Nian; Ellen Tan; Binh Tang; Ross Tay- Lor; Adina Williams; Jian Xiang Kuan; Puxin Xu; Zheng Yan; Iliyan Zarov; Yuchen Zhang; Angela Fan; Melanie Kambadur; Sharan Narang; Aurelien Ro- Driguez; Robert Stojnic; Sergey Edunov,"To precisely evaluate a language model's capability for logical reading comprehension, we present a dataset for testing the understanding of the rationale behind critical reasoning. For questions taken from an existing multiplechoice logical reading comprehension dataset, we crowdsource rationale texts that explain why we should select or eliminate answer options, resulting in 3,003 multiple-choice subquestions that are associated with 943 main questions. Experiments on our dataset show that recent large language models (e.g., InstructGPT) struggle to answer the subquestions even if they are able to answer the main questions correctly. We find that the models perform particularly poorly in answering subquestions written for the incorrect options of the main questions, implying that the models have a limited capability for explaining why incorrect alternatives should be eliminated. These results suggest that our dataset encourages further investigation into the critical reasoning ability of language models while focusing on the elimination process of relevant alternatives. Mana Ashida and Saku Sugawara. 2022. Possible stories: Evaluating situated commonsense reasoning under multiple possible scenarios. In",,"['Introduction', 'Passage', 'Main Question', 'Sub Question (on the rationale of eliminating Option A)', '🤖 🧔', 'Related Works', 'Option D', 'RationaleD', 'RULE Data Collection', 'Design Choices', 'Rationale Collection', 'Collecting Rationales', 'Rationale Writing', 'Rationale Validation', 'Subquestion Construction', 'Human Validation', 'Dataset Statistics', 'Baseline Performance on RULE', 'Models and Settings', 'Few-and', 'Results', 'Analysis', 'Conclusion', 'Ethical Consideration', 'Limitations', 'B Question Generation Prompt', 'C Crowdsourcing Details', 'D Chain-of-Thought Prompt', 'E Test Split Setting', 'F MainQ-wise SubQ Results of InstructGPT', 'G Complementary Few-Shot and Zero-Shot Results', 'H Main Results of the Subquestions for the Correctly-Answered Main Questions', 'I Relationship between Question and Option Length and Model Performance', 'J Rationale Alignment Task', 'K Reasoning Type Annotation', 'L ReClor Reasoning Types and Subquestion Accuracy', 'M Context-Ablation Analysis', 'N Similarity of Rationale with MainQ Option', 'Rationale1:', 'Question:', 'Rationale0:', 'Test Instance', 'Question:', 'Rationale2:', 'Direct External', 'Indirect Contextual', 'Indirect External', 'MainQ', 'Selective SubQ', 'Eliminative SubQ', 'Question Type Option Rationale', 'Eliminative', 'Answer:', 'Test Instance', 'Solve reading comprehension questions!', 'Question', 'Acknowledgments', 'Evaluate explanation for reading comprehension Instructions', 'Question']"
